--- gcc-3.3.1/gcc/ada/5rtpopsp.adb.hammer-branch	2003-01-29 18:40:47.000000000 +0100
+++ gcc-3.3.1/gcc/ada/5rtpopsp.adb	2003-08-05 18:22:46.000000000 +0200
@@ -7,7 +7,7 @@
 --                                                                          --
 --                                  B o d y                                 --
 --                                                                          --
---                             $Revision: 1.1.4.1 $
+--                             $Revision: 1.2.2.1 $
 --                                                                          --
 --            Copyright (C) 1991-1999, Florida State University             --
 --                                                                          --
--- gcc-3.3.1/gcc/ada/Make-lang.in.hammer-branch	2003-07-04 21:53:53.000000000 +0200
+++ gcc-3.3.1/gcc/ada/Make-lang.in	2003-08-05 18:22:46.000000000 +0200
@@ -802,6 +802,12 @@ ada.stage3: stage3-start
 ada.stage4: stage4-start
 	-$(MV) ada/*$(objext) ada/*.ali ada/b_*.c stage4/ada
 	-$(MV) ada/stamp-* stage4/ada
+ada.stageprofile: stageprofile-start
+	-$(MV) ada/*$(objext) ada/*.ali ada/b_*.c stageprofile/ada
+	-$(MV) ada/stamp-* stageprofile/ada
+ada.stagefeedback: stagefeedback-start
+	-$(MV) ada/*$(objext) ada/*.ali ada/b_*.c stagefeedback/ada
+	-$(MV) ada/stamp-* stagefeedback/ada
 
 check-ada:
 
--- gcc-3.3.1/gcc/config/alpha/alpha.c.hammer-branch	2003-03-24 18:59:37.000000000 +0100
+++ gcc-3.3.1/gcc/config/alpha/alpha.c	2003-08-05 18:22:46.000000000 +0200
@@ -134,6 +134,8 @@ static int some_small_symbolic_operand_1
   PARAMS ((rtx *, void *));
 static int split_small_symbolic_operand_1
   PARAMS ((rtx *, void *));
+static bool alpha_cannot_copy_insn_p
+  PARAMS ((rtx));
 static void alpha_set_memflags_1
   PARAMS ((rtx, int, int, int));
 static rtx alpha_emit_set_const_1
@@ -303,6 +305,8 @@ static void unicosmk_unique_section PARA
 #undef TARGET_ASM_CAN_OUTPUT_MI_THUNK
 #define TARGET_ASM_CAN_OUTPUT_MI_THUNK hook_bool_tree_hwi_hwi_tree_true
 #endif
+#undef TARGET_CANNOT_COPY_INSN_P
+#define TARGET_CANNOT_COPY_INSN_P alpha_cannot_copy_insn_p
 
 struct gcc_target targetm = TARGET_INITIALIZER;
 
@@ -2335,6 +2339,49 @@ split_small_symbolic_operand_1 (px, data
   return 0;
 }
 
+/* Indicate that INSN cannot be duplicated.  This is true for any insn
+   that we've marked with gpdisp relocs, since those have to stay in
+   1-1 correspondence with one another.
+
+   Techinically we could copy them if we could set up a mapping from one
+   sequence number to another, across the set of insns to be duplicated.
+   This seems overly complicated and error-prone since interblock motion
+   from sched-ebb could move one of the pair of insns to a different block.  */
+
+static bool
+alpha_cannot_copy_insn_p (insn)
+     rtx insn;
+{
+  rtx pat;
+
+  if (!reload_completed || !TARGET_EXPLICIT_RELOCS)
+    return false;
+
+  if (GET_CODE (insn) != INSN)
+    return false;
+  if (asm_noperands (insn) >= 0)
+    return false;
+
+  pat = PATTERN (insn);
+  if (GET_CODE (pat) != SET)
+    return false;
+  pat = SET_SRC (pat);
+  if (GET_CODE (pat) == UNSPEC_VOLATILE)
+    {
+      if (XINT (pat, 1) == UNSPECV_LDGP1
+	  || XINT (pat, 1) == UNSPECV_PLDGP2)
+	return true;
+    }
+  else if (GET_CODE (pat) == UNSPEC)
+    {
+      if (XINT (pat, 1) == UNSPEC_LDGP2)
+	return true;
+    }
+
+  return false;
+}
+
+  
 /* Try a machine-dependent way of reloading an illegitimate address
    operand.  If we find one, push the reload and return the new rtx.  */
    
--- gcc-3.3.1/gcc/config/i386/athlon.md.hammer-branch	2002-11-16 02:09:18.000000000 +0100
+++ gcc-3.3.1/gcc/config/i386/athlon.md	2003-08-05 18:22:46.000000000 +0200
@@ -1,34 +1,5 @@
 ;; AMD Athlon Scheduling
-;; Copyright (C) 2002 Free Software Foundation, Inc.
 ;;
-;; This file is part of GNU CC.
-;;
-;; GNU CC is free software; you can redistribute it and/or modify
-;; it under the terms of the GNU General Public License as published by
-;; the Free Software Foundation; either version 2, or (at your option)
-;; any later version.
-;;
-;; GNU CC is distributed in the hope that it will be useful,
-;; but WITHOUT ANY WARRANTY; without even the implied warranty of
-;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-;; GNU General Public License for more details.
-;;
-;; You should have received a copy of the GNU General Public License
-;; along with GNU CC; see the file COPYING.  If not, write to
-;; the Free Software Foundation, 59 Temple Place - Suite 330,
-;; Boston, MA 02111-1307, USA.  */
-(define_attr "athlon_decode" "direct,vector"
-  (cond [(eq_attr "type" "call,imul,idiv,other,multi,fcmov,fpspc,str,pop,cld,fcmov")
-	   (const_string "vector")
-         (and (eq_attr "type" "push")
-              (match_operand 1 "memory_operand" ""))
-	   (const_string "vector")
-         (and (eq_attr "type" "fmov")
-	      (and (eq_attr "memory" "load,store")
-		   (eq_attr "mode" "XF")))
-	   (const_string "vector")]
-	(const_string "direct")))
-
 ;; The Athlon does contain three pipelined FP units, three integer units and
 ;; three address generation units. 
 ;;
@@ -46,161 +17,649 @@
 ;; The load/store queue unit is not attached to the schedulers but
 ;; communicates with all the execution units separately instead.
 
-(define_function_unit "athlon_vectordec" 1 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "athlon_decode" "vector"))
-  1 1)
-
-(define_function_unit "athlon_directdec" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "athlon_decode" "direct"))
-  1 1)
-
-(define_function_unit "athlon_vectordec" 1 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "athlon_decode" "direct"))
-  1 1 [(eq_attr "athlon_decode" "vector")])
-
-(define_function_unit "athlon_ieu" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "type" "alu1,negnot,alu,icmp,test,imov,imovx,lea,incdec,ishift,ishift1,rotate,rotate1,ibr,call,callv,icmov,cld,pop,setcc,push,pop"))
-  1 1)
-
-(define_function_unit "athlon_ieu" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "type" "str"))
-  15 15)
-
-(define_function_unit "athlon_ieu" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "type" "imul"))
-  5 0)
-
-(define_function_unit "athlon_ieu" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "type" "idiv"))
-  42 0)
-
-(define_function_unit "athlon_muldiv" 1 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "type" "imul"))
-  5 0)
-
-(define_function_unit "athlon_muldiv" 1 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "type" "idiv"))
-  42 42)
-
-(define_attr "athlon_fpunits" "none,store,mul,add,muladd,any"
-  (cond [(eq_attr "type" "fop,fcmp,fistp")
-	   (const_string "add")
-         (eq_attr "type" "fmul,fdiv,fpspc,fsgn,fcmov")
-	   (const_string "mul")
-	 (and (eq_attr "type" "fmov") (eq_attr "memory" "store,both"))
-	   (const_string "store")
-	 (and (eq_attr "type" "fmov") (eq_attr "memory" "load"))
-	   (const_string "any")
+(define_attr "athlon_decode" "direct,vector,double"
+  (cond [(eq_attr "type" "call,imul,idiv,other,multi,fcmov,fpspc,str,pop,cld,leave")
+	   (const_string "vector")
+         (and (eq_attr "type" "push")
+              (match_operand 1 "memory_operand" ""))
+	   (const_string "vector")
          (and (eq_attr "type" "fmov")
-              (ior (match_operand:SI 1 "register_operand" "")
-                   (match_operand 1 "immediate_operand" "")))
-	   (const_string "store")
-         (eq_attr "type" "fmov")
-	   (const_string "muladd")]
-	(const_string "none")))
-
-;; We use latencies 1 for definitions.  This is OK to model colisions
-;; in execution units.  The real latencies are modeled in the "fp" pipeline.
-
-;; fsin, fcos: 96-192
-;; fsincos: 107-211
-;; fsqrt: 19 for SFmode, 27 for DFmode, 35 for XFmode.
-(define_function_unit "athlon_fp" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "type" "fpspc"))
-  100 1)
-
-;; 16 cycles for SFmode, 20 for DFmode and 24 for XFmode.
-(define_function_unit "athlon_fp" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "type" "fdiv"))
-  24 1)
-
-(define_function_unit "athlon_fp" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "type" "fop,fmul,fistp"))
-  4 1)
-
-;; XFmode loads are slow.
-;; XFmode store is slow too (8 cycles), but we don't need to model it, because
-;; there are no dependent instructions.
-
-(define_function_unit "athlon_fp" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (and (eq_attr "type" "fmov")
-	    (and (eq_attr "memory" "load")
-		 (eq_attr "mode" "XF"))))
-  10 1)
-
-(define_function_unit "athlon_fp" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "type" "fmov,fsgn"))
-  2 1)
-
-;; fcmp and ftst instructions
-(define_function_unit "athlon_fp" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (and (eq_attr "type" "fcmp")
-	    (eq_attr "athlon_decode" "direct")))
-  3 1)
-
-;; fcmpi instructions.
-(define_function_unit "athlon_fp" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (and (eq_attr "type" "fcmp")
-	    (eq_attr "athlon_decode" "vector")))
-  3 1)
-
-(define_function_unit "athlon_fp" 3 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "type" "fcmov"))
-  7 1)
-
-(define_function_unit "athlon_fp_mul" 1 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "athlon_fpunits" "mul"))
-  1 1)
-
-(define_function_unit "athlon_fp_add" 1 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "athlon_fpunits" "add"))
-  1 1)
-
-(define_function_unit "athlon_fp_muladd" 2 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "athlon_fpunits" "muladd,mul,add"))
-  1 1)
-
-(define_function_unit "athlon_fp_store" 1 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "athlon_fpunits" "store"))
-  1 1)
-
-;; We don't need to model the Address Generation Unit, since we don't model
-;; the re-order buffer yet and thus we never schedule more than three operations
-;; at time.  Later we may want to experiment with MD_SCHED macros modeling the
-;; decoders independently on the functional units.
-
-;(define_function_unit "athlon_agu" 3 0
-;  (and (eq_attr "cpu" "athlon")
-;       (and (eq_attr "memory" "!none")
-;            (eq_attr "athlon_fpunits" "none")))
-;  1 1)
-
-;; Model load unit to avoid too long sequences of loads.  We don't need to
-;; model store queue, since it is hardly going to be bottleneck.
-
-(define_function_unit "athlon_load" 2 0
-  (and (eq_attr "cpu" "athlon")
-       (eq_attr "memory" "load,both"))
-  1 1)
+	      (and (eq_attr "memory" "load,store")
+		   (eq_attr "mode" "XF")))
+	   (const_string "vector")]
+	(const_string "direct")))
 
+;;
+;;           decode0 decode1 decode2
+;;                 \    |   /
+;;    instruction control unit (72 entry scheduler)
+;;                |                        |
+;;      integer scheduler (18)         stack map
+;;     /  |    |    |    |   \        stack rename
+;;  ieu0 agu0 ieu1 agu1 ieu2 agu2      scheduler
+;;    |  agu0  |   agu1      agu2    register file
+;;    |      \ |    |       /         |     |     |
+;;     \      /\    |     /         fadd  fmul  fstore
+;;       \  /    \  |   /           fadd  fmul  fstore
+;;       imul  load/store (2x)      fadd  fmul  fstore
+
+(define_automaton "athlon,athlon_load,athlon_mult,athlon_fp")
+(define_cpu_unit "athlon-decode0" "athlon")
+(define_cpu_unit "athlon-decode1" "athlon")
+(define_cpu_unit "athlon-decode2" "athlon")
+(define_cpu_unit "athlon-decodev" "athlon")
+;; Model the fact that double decoded instruction may take 2 cycles
+;; to decode when decoder2 and decoder0 in next cycle
+;; is used (this is needed to allow troughput of 1.5 double decoded
+;; instructions per cycle).
+;;
+;; In order to avoid dependence between reservation of decoder
+;; and other units, we model decoder as two stage fully pipelined unit
+;; and only double decoded instruction may occupy unit in the first cycle.
+;; With this scheme however two double instructions can be issued cycle0.
+;;
+;; Avoid this by using presence set requiring decoder0 to be allocated
+;; too. Vector decoded instructions then can't be issued when
+;; modeled as consuming decoder0+decoder1+decoder2.
+;; We solve that by specialized vector decoder unit and exclusion set.
+(presence_set "athlon-decode2" "athlon-decode0")
+(exclusion_set "athlon-decodev" "athlon-decode0,athlon-decode1,athlon-decode2")
+(define_reservation "athlon-vector" "nothing,athlon-decodev")
+(define_reservation "athlon-direct0" "nothing,athlon-decode0")
+(define_reservation "athlon-direct" "nothing,
+				     (athlon-decode0 | athlon-decode1
+				     | athlon-decode2)")
+;; Double instructions behaves like two direct instructions.
+(define_reservation "athlon-double" "((athlon-decode2, athlon-decode0)
+				     | (nothing,(athlon-decode0 + athlon-decode1))
+				     | (nothing,(athlon-decode1 + athlon-decode2)))")
+
+;; Agu and ieu unit results in extremely large automatons and
+;; in our approximation they are hardly filled in.  Only ieu
+;; unit can, as issue rate is 3 and agu unit is always used
+;; first in the insn reservations.  Skip the models.
+
+;(define_cpu_unit "athlon-ieu0" "athlon_ieu")
+;(define_cpu_unit "athlon-ieu1" "athlon_ieu")
+;(define_cpu_unit "athlon-ieu2" "athlon_ieu")
+;(define_reservation "athlon-ieu" "(athlon-ieu0 | athlon-ieu1 | athlon-ieu2)")
+(define_reservation "athlon-ieu" "nothing")
+(define_cpu_unit "athlon-ieu0" "athlon")
+;(define_cpu_unit "athlon-agu0" "athlon_agu")
+;(define_cpu_unit "athlon-agu1" "athlon_agu")
+;(define_cpu_unit "athlon-agu2" "athlon_agu")
+;(define_reservation "athlon-agu" "(athlon-agu0 | athlon-agu1 | athlon-agu2)")
+(define_reservation "athlon-agu" "nothing,nothing")
+
+(define_cpu_unit "athlon-mult" "athlon_mult")
+
+(define_cpu_unit "athlon-load0" "athlon_load")
+(define_cpu_unit "athlon-load1" "athlon_load")
+(define_reservation "athlon-load" "athlon-agu,
+				   (athlon-load0 | athlon-load1)")
+(define_reservation "athlon-store" "nothing")
+
+;; The three fp units are fully pipelined with latency of 3
+(define_cpu_unit "athlon-fadd" "athlon_fp")
+(define_cpu_unit "athlon-fmul" "athlon_fp")
+(define_cpu_unit "athlon-fstore" "athlon_fp")
+(define_reservation "athlon-fany" "(athlon-fadd | athlon-fmul | athlon-fstore)")
+(define_reservation "athlon-faddmul" "(athlon-fadd | athlon-fmul)")
+
+
+;; Jump instructions are executed in the branch unit completely transparent to us
+(define_insn_reservation "athlon_branch" 0
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "ibr"))
+			 "athlon-direct")
+(define_insn_reservation "athlon_call" 0
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "call,callv"))
+			 "athlon-vector")
+
+;; Latency of push operation is 3 cycles, but ESP value is available
+;; earlier
+(define_insn_reservation "athlon_push" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "push"))
+			 "athlon-direct,nothing,athlon-store")
+(define_insn_reservation "athlon_pop" 4
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "pop"))
+			 "athlon-vector,athlon-ieu,athlon-load")
+(define_insn_reservation "athlon_pop_k8" 3
+			 (and (eq_attr "cpu" "k8")
+			      (eq_attr "type" "pop"))
+			 "athlon-double,athlon-ieu,athlon-load")
+(define_insn_reservation "athlon_leave" 3
+			 (and (eq_attr "cpu" "athlon")
+			      (eq_attr "type" "leave"))
+			 "athlon-vector,athlon-load")
+(define_insn_reservation "athlon_leave_k8" 3
+			 (and (eq_attr "cpu" "k8")
+			      (eq_attr "type" "leave"))
+			 "athlon-double,athlon-load")
+
+;; Lea executes in AGU unit with 2 cycles latency.
+(define_insn_reservation "athlon_lea" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "lea"))
+			 "athlon-direct,athlon-agu")
+
+;; Mul executes in special multiplier unit attached to IEU0
+(define_insn_reservation "athlon_imul" 5
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "imul")
+				   (eq_attr "memory" "none,unknown")))
+			 "athlon-vector,athlon-ieu0,athlon-mult,nothing,nothing,athlon-ieu0")
+;; ??? Widening multiply is vector or double.
+(define_insn_reservation "athlon_imul_k8_DI" 4
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "imul")
+				   (and (eq_attr "mode" "DI")
+					(eq_attr "memory" "none,unknown"))))
+			 "athlon-direct0,athlon-ieu0,athlon-mult,nothing,athlon-ieu0")
+(define_insn_reservation "athlon_imul_k8" 3
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "imul")
+				   (eq_attr "memory" "none,unknown")))
+			 "athlon-direct0,athlon-ieu0,athlon-mult,athlon-ieu0")
+(define_insn_reservation "athlon_imul_mem" 8
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "imul")
+				   (eq_attr "memory" "load,both")))
+			 "athlon-vector,athlon-load,athlon-ieu,athlon-mult,nothing,nothing,athlon-ieu")
+(define_insn_reservation "athlon_imul_mem_k8_DI" 7
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "imul")
+				   (and (eq_attr "mode" "DI")
+					(eq_attr "memory" "load,both"))))
+			 "athlon-vector,athlon-load,athlon-ieu,athlon-mult,nothing,athlon-ieu")
+(define_insn_reservation "athlon_imul_mem_k8" 6
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "imul")
+				   (eq_attr "memory" "load,both")))
+			 "athlon-vector,athlon-load,athlon-ieu,athlon-mult,athlon-ieu")
+(define_insn_reservation "athlon_idiv" 42
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "idiv")
+				   (eq_attr "memory" "none,unknown")))
+			 "athlon-vector,athlon-ieu*42")
+(define_insn_reservation "athlon_idiv_mem" 45
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "idiv")
+				   (eq_attr "memory" "load,both")))
+			 "athlon-vector,athlon-load,athlon-ieu*42")
+(define_insn_reservation "athlon_str" 15
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "str")
+				   (eq_attr "memory" "load,both,store")))
+			 "athlon-vector,athlon-load,athlon-ieu*10")
+
+(define_insn_reservation "athlon_idirect" 1
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "athlon_decode" "direct")
+				   (and (eq_attr "unit" "integer,unknown")
+					(eq_attr "memory" "none,unknown"))))
+			 "athlon-direct,athlon-ieu")
+(define_insn_reservation "athlon_ivector" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "athlon_decode" "vector")
+				   (and (eq_attr "unit" "integer,unknown")
+					(eq_attr "memory" "none,unknown"))))
+			 "athlon-vector,athlon-ieu,athlon-ieu")
+(define_insn_reservation "athlon_idirect_loadmov" 3
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "imov")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-load")
+(define_insn_reservation "athlon_idirect_load" 4
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "athlon_decode" "direct")
+				   (and (eq_attr "unit" "integer,unknown")
+					(eq_attr "memory" "load"))))
+			 "athlon-direct,athlon-load,athlon-ieu")
+(define_insn_reservation "athlon_ivector_load" 6
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "athlon_decode" "vector")
+				   (and (eq_attr "unit" "integer,unknown")
+					(eq_attr "memory" "load"))))
+			 "athlon-vector,athlon-load,athlon-ieu,athlon-ieu")
+(define_insn_reservation "athlon_idirect_movstore" 1
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "imov")
+				   (eq_attr "memory" "store")))
+			 "athlon-direct,athlon-agu,athlon-store")
+(define_insn_reservation "athlon_idirect_both" 4
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "athlon_decode" "direct")
+				   (and (eq_attr "unit" "integer,unknown")
+					(eq_attr "memory" "both"))))
+			 "athlon-direct,athlon-load,athlon-ieu,
+			  athlon-store")
+(define_insn_reservation "athlon_ivector_both" 6
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "athlon_decode" "vector")
+				   (and (eq_attr "unit" "integer,unknown")
+					(eq_attr "memory" "both"))))
+			 "athlon-vector,athlon-load,athlon-ieu,athlon-ieu,
+			  athlon-store")
+(define_insn_reservation "athlon_idirect_store" 1
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "athlon_decode" "direct")
+				   (and (eq_attr "unit" "integer,unknown")
+					(eq_attr "memory" "store"))))
+			 "athlon-direct,athlon-ieu,
+			  athlon-store")
+(define_insn_reservation "athlon_ivector_store" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "athlon_decode" "vector")
+				   (and (eq_attr "unit" "integer,unknown")
+					(eq_attr "memory" "store"))))
+			 "athlon-vector,athlon-ieu,athlon-ieu,
+			  athlon-store")
+
+;; Athlon floatin point unit
+(define_insn_reservation "athlon_fldxf" 12
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "fmov")
+				   (and (eq_attr "memory" "load")
+					(eq_attr "mode" "XF"))))
+			 "athlon-vector,athlon-fany")
+(define_insn_reservation "athlon_fldxf_k8" 13
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "fmov")
+				   (and (eq_attr "memory" "load")
+					(eq_attr "mode" "XF"))))
+			 "athlon-vector,athlon-fany")
+(define_insn_reservation "athlon_fld" 6
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "fmov")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-fany,nothing,athlon-load")
+(define_insn_reservation "athlon_fld_k8" 4
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "fmov")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-fany,athlon-load")
+(define_insn_reservation "athlon_fstxf" 10
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "fmov")
+				   (and (eq_attr "memory" "store,both")
+					(eq_attr "mode" "XF"))))
+			 "athlon-vector,athlon-fstore")
+(define_insn_reservation "athlon_fstxf_k8" 8
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "fmov")
+				   (and (eq_attr "memory" "store,both")
+					(eq_attr "mode" "XF"))))
+			 "athlon-vector,athlon-fstore")
+(define_insn_reservation "athlon_fst" 4
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "fmov")
+				   (eq_attr "memory" "store,both")))
+			 "athlon-direct,athlon-fstore,nothing,athlon-store")
+(define_insn_reservation "athlon_fst_k8" 2
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "fmov")
+				   (eq_attr "memory" "store,both")))
+			 "athlon-direct,athlon-fstore,athlon-store")
+(define_insn_reservation "athlon_fist" 4
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "fistp"))
+			 "athlon-direct,athlon-fstore,nothing")
+(define_insn_reservation "athlon_fmov" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "fmov"))
+			 "athlon-direct,athlon-faddmul")
+(define_insn_reservation "athlon_fadd_load" 7
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "fop")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_fadd_load_k8" 6
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "fop")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_fadd" 4
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "fop"))
+			 "athlon-direct,athlon-fadd")
+(define_insn_reservation "athlon_fmul_load" 7
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "fmul")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_fmul_load_k8" 6
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "fmul")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_fmul" 4
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "fmul"))
+			 "athlon-direct,athlon-fmul")
+(define_insn_reservation "athlon_fsgn" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "fsgn"))
+			 "athlon-direct,athlon-fmul")
+(define_insn_reservation "athlon_fdiv_load" 24
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "fdiv")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_fdiv_load_k8" 13
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "fdiv")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_fdiv" 24
+			 (and (eq_attr "cpu" "athlon")
+			      (eq_attr "type" "fdiv"))
+			 "athlon-direct,athlon-fmul")
+(define_insn_reservation "athlon_fdiv_k8" 11
+			 (and (eq_attr "cpu" "k8")
+			      (eq_attr "type" "fdiv"))
+			 "athlon-direct,athlon-fmul")
+(define_insn_reservation "athlon_fpspc_load" 103
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "fpspc")
+				   (eq_attr "memory" "load")))
+			 "athlon-vector,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_fpspc" 100
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "fpspc"))
+			 "athlon-vector,athlon-fmul")
+(define_insn_reservation "athlon_fcmov_load" 10
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "fcmov")
+				   (eq_attr "memory" "load")))
+			 "athlon-vector,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_fcmov" 7
+			 (and (eq_attr "cpu" "athlon")
+			      (eq_attr "type" "fcmov"))
+			 "athlon-vector,athlon-fmul")
+(define_insn_reservation "athlon_fcmov_load_k8" 17
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "fcmov")
+				   (eq_attr "memory" "load")))
+			 "athlon-vector,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_fcmov_k8" 15
+			 (and (eq_attr "cpu" "k8")
+			      (eq_attr "type" "fcmov"))
+			 "athlon-vector,athlon-fmul")
+(define_insn_reservation "athlon_fcomi_load" 6
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "fcmp")
+				   (and (eq_attr "athlon_decode" "vector")
+				        (eq_attr "memory" "load"))))
+			 "athlon-vector,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_fcomi" 3
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "athlon_decode" "vector")
+				   (eq_attr "type" "fcmp")))
+			 "athlon-vector,athlon-fadd")
+(define_insn_reservation "athlon_fcom_load" 5
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "fcmp")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_fcom" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "fcmp"))
+			 "athlon-direct,athlon-fadd")
+(define_insn_reservation "athlon_fxch" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "fxch"))
+			 "athlon-direct,athlon-fany")
+;; Athlon handle MMX operations in the FPU unit with shorter latencies
+(define_insn_reservation "athlon_movlpd_load" 4
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "ssemov")
+				   (match_operand:DF 1 "memory_operand" "")))
+			 "athlon-direct,athlon-load")
+(define_insn_reservation "athlon_movaps_load" 4
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "ssemov")
+				   (and (eq_attr "mode" "V4SF,V2DF,TI")
+					(eq_attr "memory" "load"))))
+			 "athlon-double,athlon-load")
+(define_insn_reservation "athlon_movss_load" 3
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "ssemov")
+				   (and (eq_attr "mode" "SF,DI")
+					(eq_attr "memory" "load"))))
+			 "athlon-double,athlon-load")
+(define_insn_reservation "athlon_mmxsseld" 4
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "mmxmov,ssemov")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-fany,athlon-load")
+(define_insn_reservation "athlon_mmxssest" 3
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "mmxmov,ssemov")
+				   (and (eq_attr "mode" "V4SF,V2DF,TI")
+					(eq_attr "memory" "store,both"))))
+			 "athlon-double,athlon-store")
+(define_insn_reservation "athlon_mmxssest_k8" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "mmxmov,ssemov")
+				   (eq_attr "memory" "store,both")))
+			 "athlon-direct,athlon-store")
+(define_insn_reservation "athlon_movaps" 2
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "ssemov")
+				   (eq_attr "mode" "V4SF,V2DF")))
+			 "athlon-double,athlon-faddmul,athlon-faddmul")
+(define_insn_reservation "athlon_mmxssemov" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "mmxmov,ssemov"))
+			 "athlon-direct,athlon-faddmul")
+(define_insn_reservation "athlon_mmxmul_load" 6
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "mmxmul")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_mmxmul" 3
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "type" "mmxmul"))
+			 "athlon-direct,athlon-fmul")
+(define_insn_reservation "athlon_mmx_load" 5
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "unit" "mmx")
+				   (eq_attr "memory" "load")))
+			 "athlon-direct,athlon-load,athlon-faddmul")
+(define_insn_reservation "athlon_mmx" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (eq_attr "unit" "mmx"))
+			 "athlon-direct,athlon-faddmul")
+;; SSE operations are handled by the i387 unit as well.  The latency
+;; is same as for i387 operations for scalar operations
+(define_insn_reservation "athlon_sselog_load" 6
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "sselog")
+				   (eq_attr "memory" "load")))
+			 "athlon-vector,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_sselog_load_k8" 5
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "sselog")
+				   (eq_attr "memory" "load")))
+			 "athlon-double,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_sselog" 3
+			 (and (eq_attr "cpu" "athlon")
+			      (eq_attr "type" "sselog"))
+			 "athlon-vector,athlon-fmul")
+(define_insn_reservation "athlon_sselog_k8" 3
+			 (and (eq_attr "cpu" "k8")
+			      (eq_attr "type" "sselog"))
+			 "athlon-double,athlon-fmul")
+(define_insn_reservation "athlon_ssecmp_load" 5
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "ssecmp,ssecomi")
+				   (and (eq_attr "mode" "SF,DF")
+					(eq_attr "memory" "load"))))
+			 "athlon-vector,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_ssecmp" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "ssecmp,ssecomi")
+				   (eq_attr "mode" "SF,DF")))
+			 "athlon-direct,athlon-fadd")
+(define_insn_reservation "athlon_ssecmpvector_load" 6
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "ssecmp,ssecomi")
+				   (eq_attr "memory" "load")))
+			 "athlon-vector,athlon-fadd")
+(define_insn_reservation "athlon_ssecmpvector_load_k8" 5
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "ssecmp,ssecomi")
+				   (eq_attr "memory" "load")))
+			 "athlon-double,athlon-fadd")
+(define_insn_reservation "athlon_ssecmpvector" 3
+			 (and (eq_attr "cpu" "athlon")
+			      (eq_attr "type" "ssecmp,ssecomi"))
+			 "athlon-vector,athlon-fadd")
+(define_insn_reservation "athlon_ssecmpvector_k8" 3
+			 (and (eq_attr "cpu" "k8")
+			      (eq_attr "type" "ssecmp,ssecomi"))
+			 "athlon-double,athlon-fadd")
+(define_insn_reservation "athlon_sseadd_load" 7
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "sseadd")
+				   (and (eq_attr "mode" "SF,DF")
+					(eq_attr "memory" "load"))))
+			 "athlon-direct,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_sseadd_load_k8" 6
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "sseadd")
+				   (and (eq_attr "mode" "SF,DF")
+					(eq_attr "memory" "load"))))
+			 "athlon-direct,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_sseadd" 4
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "sseadd")
+				   (eq_attr "mode" "SF,DF")))
+			 "athlon-direct,athlon-fadd")
+(define_insn_reservation "athlon_sseaddvector_load" 8
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "sseadd")
+				   (eq_attr "memory" "load")))
+			 "athlon-vector,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_sseaddvector_load_k8" 7
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "sseadd")
+				   (eq_attr "memory" "load")))
+			 "athlon-vector,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_sseaddvector" 5
+			 (and (eq_attr "cpu" "athlon")
+			      (eq_attr "type" "sseadd"))
+			 "athlon-vector,athlon-fadd")
+(define_insn_reservation "athlon_sseaddvector_k8" 4
+			 (and (eq_attr "cpu" "k8")
+			      (eq_attr "type" "sseadd"))
+			 "athlon-vector,athlon-fadd")
+(define_insn_reservation "athlon_ssecvt_load" 5
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "ssecvt")
+				   (and (eq_attr "mode" "SF,DF")
+					(eq_attr "memory" "load"))))
+			 "athlon-direct,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_ssecvt_load_k8" 4
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "ssecvt")
+				   (and (eq_attr "mode" "SF,DF")
+					(eq_attr "memory" "load"))))
+			 "athlon-direct,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_ssecvt" 2
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "ssecvt")
+				   (eq_attr "mode" "SF,DF")))
+			 "athlon-direct,athlon-fadd")
+(define_insn_reservation "athlon_ssecvtvector_load" 6
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "ssecvt")
+				   (eq_attr "memory" "load")))
+			 "athlon-vector,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_ssecvtvector_load_k8" 5
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "ssecvt")
+				   (eq_attr "memory" "load")))
+			 "athlon-vector,athlon-load,athlon-fadd")
+(define_insn_reservation "athlon_ssecvtvector" 5
+			 (and (eq_attr "cpu" "athlon")
+			      (eq_attr "type" "ssecvt"))
+			 "athlon-vector,athlon-fadd")
+(define_insn_reservation "athlon_ssecvtvector_k8" 3
+			 (and (eq_attr "cpu" "k8")
+			      (eq_attr "type" "ssecvt"))
+			 "athlon-vector,athlon-fadd")
+(define_insn_reservation "athlon_ssemul_load" 7
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "ssemul")
+				   (and (eq_attr "mode" "SF,DF")
+					(eq_attr "memory" "load"))))
+			 "athlon-direct,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_ssemul_load_k8" 6
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "ssemul")
+				   (and (eq_attr "mode" "SF,DF")
+					(eq_attr "memory" "load"))))
+			 "athlon-direct,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_ssemul" 4
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "ssemul")
+				   (eq_attr "mode" "SF,DF")))
+			 "athlon-direct,athlon-fmul")
+(define_insn_reservation "athlon_ssemulvector_load" 8
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "ssemul")
+				   (eq_attr "memory" "load")))
+			 "athlon-vector,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_ssemulvector_load_k8" 7
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "ssemul")
+				   (eq_attr "memory" "load")))
+			 "athlon-double,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_ssemulvector" 5
+			 (and (eq_attr "cpu" "athlon")
+			      (eq_attr "type" "ssemul"))
+			 "athlon-vector,athlon-fmul")
+(define_insn_reservation "athlon_ssemulvector_k8" 5
+			 (and (eq_attr "cpu" "k8")
+			      (eq_attr "type" "ssemul"))
+			 "athlon-double,athlon-fmul")
+(define_insn_reservation "athlon_ssediv_load" 19
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "ssediv")
+				   (and (eq_attr "mode" "SF,DF")
+					(eq_attr "memory" "load"))))
+			 "athlon-direct,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_ssediv_load_k8" 18
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "ssediv")
+				   (and (eq_attr "mode" "SF,DF")
+					(eq_attr "memory" "load"))))
+			 "athlon-direct,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_ssediv" 16
+			 (and (eq_attr "cpu" "athlon,k8")
+			      (and (eq_attr "type" "ssediv")
+				   (eq_attr "mode" "SF,DF")))
+			 "athlon-direct,athlon-fmul")
+(define_insn_reservation "athlon_ssedivvector_load" 32
+			 (and (eq_attr "cpu" "athlon")
+			      (and (eq_attr "type" "ssediv")
+				   (eq_attr "memory" "load")))
+			 "athlon-vector,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_ssedivvector_load_k8" 35
+			 (and (eq_attr "cpu" "k8")
+			      (and (eq_attr "type" "ssediv")
+				   (eq_attr "memory" "load")))
+			 "athlon-vector,athlon-load,athlon-fmul")
+(define_insn_reservation "athlon_ssedivvector" 29
+			 (and (eq_attr "cpu" "athlon")
+			      (eq_attr "type" "ssediv"))
+			 "athlon-vector,athlon-fmul")
+(define_insn_reservation "athlon_ssedivvector_k8" 33
+			 (and (eq_attr "cpu" "k8")
+			      (eq_attr "type" "ssediv"))
+			 "athlon-vector,athlon-fmul")
--- gcc-3.3.1/gcc/config/i386/i386-protos.h.hammer-branch	2003-08-05 18:22:45.000000000 +0200
+++ gcc-3.3.1/gcc/config/i386/i386-protos.h	2003-08-05 18:22:46.000000000 +0200
@@ -1,6 +1,6 @@
 /* Definitions of target machine for GNU compiler for IA-32.
    Copyright (C) 1988, 1992, 1994, 1995, 1996, 1996, 1997, 1998, 1999,
-   2000, 2001 Free Software Foundation, Inc.
+   2000, 2001, 2002, 2003 Free Software Foundation, Inc.
 
 This file is part of GNU CC.
 
@@ -39,8 +39,13 @@ extern void ix86_output_addr_diff_elt PA
 extern int ix86_aligned_p PARAMS ((rtx));
 
 extern int standard_80387_constant_p PARAMS ((rtx));
+extern const char *standard_80387_constant_opcode PARAMS ((rtx));
+extern rtx standard_80387_constant_rtx PARAMS ((int));
 extern int standard_sse_constant_p PARAMS ((rtx));
 extern int symbolic_reference_mentioned_p PARAMS ((rtx));
+extern bool extended_reg_mentioned_p PARAMS ((rtx));
+extern bool x86_extended_QIreg_mentioned_p PARAMS ((rtx));
+extern bool x86_extended_reg_mentioned_p PARAMS ((rtx));
 
 extern int any_fp_register_operand PARAMS ((rtx, enum machine_mode));
 extern int register_and_not_any_fp_reg_operand PARAMS ((rtx, enum machine_mode));
@@ -137,6 +142,7 @@ extern void ix86_expand_branch PARAMS ((
 extern int ix86_expand_setcc PARAMS ((enum rtx_code, rtx));
 extern int ix86_expand_int_movcc PARAMS ((rtx[]));
 extern int ix86_expand_fp_movcc PARAMS ((rtx[]));
+extern int ix86_expand_int_addcc PARAMS ((rtx[]));
 extern void ix86_expand_call PARAMS ((rtx, rtx, rtx, rtx, rtx));
 extern void x86_initialize_trampoline PARAMS ((rtx, rtx, rtx));
 extern rtx ix86_zero_extend_to_Pmode PARAMS ((rtx));
@@ -185,6 +191,7 @@ extern void emit_i387_cw_initialization 
 extern bool ix86_fp_jump_nontrivial_p PARAMS ((enum rtx_code));
 extern void x86_order_regs_for_local_alloc PARAMS ((void));
 extern void x86_function_profiler PARAMS ((FILE *, int));
+extern void x86_emit_floatuns PARAMS ((rtx [2]));
 
 
 #ifdef TREE_CODE
--- gcc-3.3.1/gcc/config/i386/i386.c.hammer-branch	2003-08-05 18:22:45.000000000 +0200
+++ gcc-3.3.1/gcc/config/i386/i386.c	2003-08-05 18:22:46.000000000 +0200
@@ -55,9 +55,9 @@ struct processor_costs size_cost = {	/* 
   3,					/* cost of a lea instruction */
   2,					/* variable shift costs */
   3,					/* constant shift costs */
-  3,					/* cost of starting a multiply */
+  {3, 3, 3, 3, 5},			/* cost of starting a multiply */
   0,					/* cost of multiply per each bit set */
-  3,					/* cost of a divide/mod */
+  {3, 3, 3, 3, 5},			/* cost of a divide/mod */
   3,					/* cost of movsx */
   3,					/* cost of movzx */
   0,					/* "large" insn */
@@ -84,6 +84,7 @@ struct processor_costs size_cost = {	/* 
   3,					/* MMX or SSE register to integer */
   0,					/* size of prefetch block */
   0,					/* number of parallel prefetches */
+  1,					/* Branch cost */
   2,					/* cost of FADD and FSUB insns.  */
   2,					/* cost of FMUL instruction.  */
   2,					/* cost of FDIV instruction.  */
@@ -99,9 +100,9 @@ struct processor_costs i386_cost = {	/* 
   1,					/* cost of a lea instruction */
   3,					/* variable shift costs */
   2,					/* constant shift costs */
-  6,					/* cost of starting a multiply */
+  {6, 6, 6, 6, 6},			/* cost of starting a multiply */
   1,					/* cost of multiply per each bit set */
-  23,					/* cost of a divide/mod */
+  {23, 23, 23, 23, 23},			/* cost of a divide/mod */
   3,					/* cost of movsx */
   2,					/* cost of movzx */
   15,					/* "large" insn */
@@ -128,6 +129,7 @@ struct processor_costs i386_cost = {	/* 
   3,					/* MMX or SSE register to integer */
   0,					/* size of prefetch block */
   0,					/* number of parallel prefetches */
+  1,					/* Branch cost */
   23,					/* cost of FADD and FSUB insns.  */
   27,					/* cost of FMUL instruction.  */
   88,					/* cost of FDIV instruction.  */
@@ -142,9 +144,9 @@ struct processor_costs i486_cost = {	/* 
   1,					/* cost of a lea instruction */
   3,					/* variable shift costs */
   2,					/* constant shift costs */
-  12,					/* cost of starting a multiply */
+  {12, 12, 12, 12, 12},			/* cost of starting a multiply */
   1,					/* cost of multiply per each bit set */
-  40,					/* cost of a divide/mod */
+  {40, 40, 40, 40, 40},			/* cost of a divide/mod */
   3,					/* cost of movsx */
   2,					/* cost of movzx */
   15,					/* "large" insn */
@@ -171,6 +173,7 @@ struct processor_costs i486_cost = {	/* 
   3,					/* MMX or SSE register to integer */
   0,					/* size of prefetch block */
   0,					/* number of parallel prefetches */
+  1,					/* Branch cost */
   8,					/* cost of FADD and FSUB insns.  */
   16,					/* cost of FMUL instruction.  */
   73,					/* cost of FDIV instruction.  */
@@ -185,9 +188,9 @@ struct processor_costs pentium_cost = {
   1,					/* cost of a lea instruction */
   4,					/* variable shift costs */
   1,					/* constant shift costs */
-  11,					/* cost of starting a multiply */
+  {11, 11, 11, 11, 11},			/* cost of starting a multiply */
   0,					/* cost of multiply per each bit set */
-  25,					/* cost of a divide/mod */
+  {25, 25, 25, 25, 25},			/* cost of a divide/mod */
   3,					/* cost of movsx */
   2,					/* cost of movzx */
   8,					/* "large" insn */
@@ -214,6 +217,7 @@ struct processor_costs pentium_cost = {
   3,					/* MMX or SSE register to integer */
   0,					/* size of prefetch block */
   0,					/* number of parallel prefetches */
+  2,					/* Branch cost */
   3,					/* cost of FADD and FSUB insns.  */
   3,					/* cost of FMUL instruction.  */
   39,					/* cost of FDIV instruction.  */
@@ -228,9 +232,9 @@ struct processor_costs pentiumpro_cost =
   1,					/* cost of a lea instruction */
   1,					/* variable shift costs */
   1,					/* constant shift costs */
-  4,					/* cost of starting a multiply */
+  {4, 4, 4, 4, 4},			/* cost of starting a multiply */
   0,					/* cost of multiply per each bit set */
-  17,					/* cost of a divide/mod */
+  {17, 17, 17, 17, 17},			/* cost of a divide/mod */
   1,					/* cost of movsx */
   1,					/* cost of movzx */
   8,					/* "large" insn */
@@ -257,6 +261,7 @@ struct processor_costs pentiumpro_cost =
   3,					/* MMX or SSE register to integer */
   32,					/* size of prefetch block */
   6,					/* number of parallel prefetches */
+  2,					/* Branch cost */
   3,					/* cost of FADD and FSUB insns.  */
   5,					/* cost of FMUL instruction.  */
   56,					/* cost of FDIV instruction.  */
@@ -271,9 +276,9 @@ struct processor_costs k6_cost = {
   2,					/* cost of a lea instruction */
   1,					/* variable shift costs */
   1,					/* constant shift costs */
-  3,					/* cost of starting a multiply */
+  {3, 3, 3, 3, 3},			/* cost of starting a multiply */
   0,					/* cost of multiply per each bit set */
-  18,					/* cost of a divide/mod */
+  {18, 18, 18, 18, 18},			/* cost of a divide/mod */
   2,					/* cost of movsx */
   2,					/* cost of movzx */
   8,					/* "large" insn */
@@ -300,6 +305,7 @@ struct processor_costs k6_cost = {
   6,					/* MMX or SSE register to integer */
   32,					/* size of prefetch block */
   1,					/* number of parallel prefetches */
+  1,					/* Branch cost */
   2,					/* cost of FADD and FSUB insns.  */
   2,					/* cost of FMUL instruction.  */
   56,					/* cost of FDIV instruction.  */
@@ -314,9 +320,9 @@ struct processor_costs athlon_cost = {
   2,					/* cost of a lea instruction */
   1,					/* variable shift costs */
   1,					/* constant shift costs */
-  5,					/* cost of starting a multiply */
+  {5, 5, 5, 5, 5},			/* cost of starting a multiply */
   0,					/* cost of multiply per each bit set */
-  42,					/* cost of a divide/mod */
+  {18, 26, 42, 74, 74},			/* cost of a divide/mod */
   1,					/* cost of movsx */
   1,					/* cost of movzx */
   8,					/* "large" insn */
@@ -343,6 +349,7 @@ struct processor_costs athlon_cost = {
   5,					/* MMX or SSE register to integer */
   64,					/* size of prefetch block */
   6,					/* number of parallel prefetches */
+  2,					/* Branch cost */
   4,					/* cost of FADD and FSUB insns.  */
   4,					/* cost of FMUL instruction.  */
   24,					/* cost of FDIV instruction.  */
@@ -352,14 +359,58 @@ struct processor_costs athlon_cost = {
 };
 
 static const
+struct processor_costs k8_cost = {
+  1,					/* cost of an add instruction */
+  2,					/* cost of a lea instruction */
+  1,					/* variable shift costs */
+  1,					/* constant shift costs */
+  {3, 4, 3, 4, 5},			/* cost of starting a multiply */
+  0,					/* cost of multiply per each bit set */
+  {18, 26, 42, 74, 74},			/* cost of a divide/mod */
+  1,					/* cost of movsx */
+  1,					/* cost of movzx */
+  8,					/* "large" insn */
+  9,					/* MOVE_RATIO */
+  4,					/* cost for loading QImode using movzbl */
+  {3, 4, 3},				/* cost of loading integer registers
+					   in QImode, HImode and SImode.
+					   Relative to reg-reg move (2).  */
+  {3, 4, 3},				/* cost of storing integer registers */
+  4,					/* cost of reg,reg fld/fst */
+  {4, 4, 12},				/* cost of loading fp registers
+					   in SFmode, DFmode and XFmode */
+  {6, 6, 8},				/* cost of loading integer registers */
+  2,					/* cost of moving MMX register */
+  {3, 3},				/* cost of loading MMX registers
+					   in SImode and DImode */
+  {4, 4},				/* cost of storing MMX registers
+					   in SImode and DImode */
+  2,					/* cost of moving SSE register */
+  {4, 3, 6},				/* cost of loading SSE registers
+					   in SImode, DImode and TImode */
+  {4, 4, 5},				/* cost of storing SSE registers
+					   in SImode, DImode and TImode */
+  5,					/* MMX or SSE register to integer */
+  64,					/* size of prefetch block */
+  6,					/* number of parallel prefetches */
+  2,					/* Branch cost */
+  4,					/* cost of FADD and FSUB insns.  */
+  4,					/* cost of FMUL instruction.  */
+  19,					/* cost of FDIV instruction.  */
+  2,					/* cost of FABS instruction.  */
+  2,					/* cost of FCHS instruction.  */
+  35,					/* cost of FSQRT instruction.  */
+};
+
+static const
 struct processor_costs pentium4_cost = {
   1,					/* cost of an add instruction */
   1,					/* cost of a lea instruction */
-  8,					/* variable shift costs */
-  8,					/* constant shift costs */
-  30,					/* cost of starting a multiply */
+  4,					/* variable shift costs */
+  4,					/* constant shift costs */
+  {15, 15, 15, 15, 15},			/* cost of starting a multiply */
   0,					/* cost of multiply per each bit set */
-  112,					/* cost of a divide/mod */
+  {56, 56, 56, 56, 56},			/* cost of a divide/mod */
   1,					/* cost of movsx */
   1,					/* cost of movzx */
   16,					/* "large" insn */
@@ -386,6 +437,7 @@ struct processor_costs pentium4_cost = {
   10,					/* MMX or SSE register to integer */
   64,					/* size of prefetch block */
   6,					/* number of parallel prefetches */
+  2,					/* Branch cost */
   5,					/* cost of FADD and FSUB insns.  */
   7,					/* cost of FMUL instruction.  */
   43,					/* cost of FDIV instruction.  */
@@ -404,52 +456,68 @@ const struct processor_costs *ix86_cost 
 #define m_K6  (1<<PROCESSOR_K6)
 #define m_ATHLON  (1<<PROCESSOR_ATHLON)
 #define m_PENT4  (1<<PROCESSOR_PENTIUM4)
+#define m_K8  (1<<PROCESSOR_K8)
+#define m_ATHLON_K8  (m_K8 | m_ATHLON)
 
-const int x86_use_leave = m_386 | m_K6 | m_ATHLON;
-const int x86_push_memory = m_386 | m_K6 | m_ATHLON | m_PENT4;
+const int x86_use_leave = m_386 | m_K6 | m_ATHLON_K8;
+const int x86_push_memory = m_386 | m_K6 | m_ATHLON_K8 | m_PENT4;
 const int x86_zero_extend_with_and = m_486 | m_PENT;
-const int x86_movx = m_ATHLON | m_PPRO | m_PENT4 /* m_386 | m_K6 */;
+const int x86_movx = m_ATHLON_K8 | m_PPRO | m_PENT4 /* m_386 | m_K6 */;
 const int x86_double_with_add = ~m_386;
 const int x86_use_bit_test = m_386;
-const int x86_unroll_strlen = m_486 | m_PENT | m_PPRO | m_ATHLON | m_K6;
-const int x86_cmove = m_PPRO | m_ATHLON | m_PENT4;
-const int x86_3dnow_a = m_ATHLON;
-const int x86_deep_branch = m_PPRO | m_K6 | m_ATHLON | m_PENT4;
+const int x86_unroll_strlen = m_486 | m_PENT | m_PPRO | m_ATHLON_K8 | m_K6;
+const int x86_cmove = m_PPRO | m_ATHLON_K8 | m_PENT4;
+const int x86_3dnow_a = m_ATHLON_K8;
+const int x86_deep_branch = m_PPRO | m_K6 | m_ATHLON_K8 | m_PENT4;
 const int x86_branch_hints = m_PENT4;
 const int x86_use_sahf = m_PPRO | m_K6 | m_PENT4;
 const int x86_partial_reg_stall = m_PPRO;
 const int x86_use_loop = m_K6;
-const int x86_use_fiop = ~(m_PPRO | m_ATHLON | m_PENT);
+const int x86_use_fiop = ~(m_PPRO | m_ATHLON_K8 | m_PENT);
 const int x86_use_mov0 = m_K6;
 const int x86_use_cltd = ~(m_PENT | m_K6);
 const int x86_read_modify_write = ~m_PENT;
 const int x86_read_modify = ~(m_PENT | m_PPRO);
 const int x86_split_long_moves = m_PPRO;
-const int x86_promote_QImode = m_K6 | m_PENT | m_386 | m_486 | m_ATHLON;
+const int x86_promote_QImode = m_K6 | m_PENT | m_386 | m_486 | m_ATHLON_K8;
 const int x86_fast_prefix = ~(m_PENT | m_486 | m_386);
 const int x86_single_stringop = m_386 | m_PENT4;
 const int x86_qimode_math = ~(0);
 const int x86_promote_qi_regs = 0;
 const int x86_himode_math = ~(m_PPRO);
 const int x86_promote_hi_regs = m_PPRO;
-const int x86_sub_esp_4 = m_ATHLON | m_PPRO | m_PENT4;
-const int x86_sub_esp_8 = m_ATHLON | m_PPRO | m_386 | m_486 | m_PENT4;
-const int x86_add_esp_4 = m_ATHLON | m_K6 | m_PENT4;
-const int x86_add_esp_8 = m_ATHLON | m_PPRO | m_K6 | m_386 | m_486 | m_PENT4;
-const int x86_integer_DFmode_moves = ~(m_ATHLON | m_PENT4 | m_PPRO);
-const int x86_partial_reg_dependency = m_ATHLON | m_PENT4;
-const int x86_memory_mismatch_stall = m_ATHLON | m_PENT4;
-const int x86_accumulate_outgoing_args = m_ATHLON | m_PENT4 | m_PPRO;
-const int x86_prologue_using_move = m_ATHLON | m_PENT4 | m_PPRO;
-const int x86_epilogue_using_move = m_ATHLON | m_PENT4 | m_PPRO;
+const int x86_sub_esp_4 = m_ATHLON_K8 | m_PPRO | m_PENT4;
+const int x86_sub_esp_8 = m_ATHLON_K8 | m_PPRO | m_386 | m_486 | m_PENT4;
+const int x86_add_esp_4 = m_ATHLON_K8 | m_K6 | m_PENT4;
+const int x86_add_esp_8 = m_ATHLON_K8 | m_PPRO | m_K6 | m_386 | m_486 | m_PENT4;
+const int x86_integer_DFmode_moves = ~(m_ATHLON_K8 | m_PENT4 | m_PPRO);
+const int x86_partial_reg_dependency = m_ATHLON_K8 | m_PENT4;
+const int x86_memory_mismatch_stall = m_ATHLON_K8 | m_PENT4;
+const int x86_accumulate_outgoing_args = m_ATHLON_K8 | m_PENT4 | m_PPRO;
+const int x86_prologue_using_move = m_ATHLON_K8 | m_PENT4 | m_PPRO;
+const int x86_epilogue_using_move = m_ATHLON_K8 | m_PENT4 | m_PPRO;
 const int x86_decompose_lea = m_PENT4;
 const int x86_shift1 = ~m_486;
-const int x86_arch_always_fancy_math_387 = m_PENT | m_PPRO | m_ATHLON | m_PENT4;
+const int x86_arch_always_fancy_math_387 = m_PENT | m_PPRO | m_ATHLON_K8 | m_PENT4;
+const int x86_sse_partial_reg_dependency = m_PENT4 | m_PPRO;
+/* Set for machines where the type and dependencies are resolved on SSE register
+   parts instead of whole registers, so we may maintain just lower part of
+   scalar values in proper format leaving the upper part undefined.  */
+const int x86_sse_partial_regs = m_ATHLON_K8;
+/* Athlon optimizes partial-register FPS special case, thus avoiding the
+   need for extra instructions beforehand  */
+const int x86_sse_partial_regs_for_cvtsd2ss = 0;
+const int x86_sse_typeless_stores = m_ATHLON_K8;
+const int x86_sse_load0_by_pxor = m_PPRO | m_PENT4;
+const int x86_use_ffreep = m_ATHLON_K8;
+const int x86_rep_movl_optimal = m_386 | m_PENT | m_PPRO | m_K6 | m_ATHLON_K8;
+const int x86_inter_unit_moves = ~(m_ATHLON_K8);
+const int x86_ext_80387_constants = m_K6 | m_ATHLON | m_PENT4 | m_PPRO;
 
-/* In case the avreage insn count for single function invocation is
+/* In case the average insn count for single function invocation is
    lower than this constant, emit fast (but longer) prologue and
    epilogue code.  */
-#define FAST_PROLOGUE_INSN_COUNT 30
+#define FAST_PROLOGUE_INSN_COUNT 20
 
 /* Set by prologue expander and used by epilogue expander to determine
    the style used.  */
@@ -583,8 +651,8 @@ int const svr4_dbx_register_map[FIRST_PS
   -1, 9, -1, -1, -1,			/* arg, flags, fpsr, dir, frame */
   21, 22, 23, 24, 25, 26, 27, 28,	/* SSE registers */
   29, 30, 31, 32, 33, 34, 35, 36,	/* MMX registers */
-  -1, -1, -1, -1, -1, -1, -1, -1,	/* extemded integer registers */
-  -1, -1, -1, -1, -1, -1, -1, -1,	/* extemded SSE registers */
+  -1, -1, -1, -1, -1, -1, -1, -1,	/* extended integer registers */
+  -1, -1, -1, -1, -1, -1, -1, -1,	/* extended SSE registers */
 };
 
 /* Test and compare insns in i386.md store the information needed to
@@ -755,6 +823,7 @@ static void x86_output_mi_thunk PARAMS (
 					 HOST_WIDE_INT, tree));
 static bool x86_can_output_mi_thunk PARAMS ((tree, HOST_WIDE_INT,
 					     HOST_WIDE_INT, tree));
+bool ix86_expand_carry_flag_compare PARAMS ((enum rtx_code, rtx, rtx, rtx*));
 
 struct ix86_address
 {
@@ -799,7 +868,10 @@ const struct attribute_spec ix86_attribu
 static tree ix86_handle_cdecl_attribute PARAMS ((tree *, tree, tree, int, bool *));
 static tree ix86_handle_regparm_attribute PARAMS ((tree *, tree, tree, int, bool *));
 static int ix86_value_regno PARAMS ((enum machine_mode));
+static int extended_reg_mentioned_1 PARAMS ((rtx *, void *));
 static bool contains_128bit_aligned_vector_p PARAMS ((tree));
+static int min_insn_size PARAMS ((rtx));
+static void k8_avoid_jump_misspredicts PARAMS ((rtx));
 
 #if defined (DO_GLOBAL_CTORS_BODY) && defined (HAS_INIT_SECTION)
 static void ix86_svr3_asm_out_constructor PARAMS ((rtx, int));
@@ -808,9 +880,9 @@ static void ix86_svr3_asm_out_constructo
 /* Register class used for passing given 64bit part of the argument.
    These represent classes as documented by the PS ABI, with the exception
    of SSESF, SSEDF classes, that are basically SSE class, just gcc will
-   use SF or DFmode move instead of DImode to avoid reformating penalties.
+   use SF or DFmode move instead of DImode to avoid reformatting penalties.
 
-   Similary we play games with INTEGERSI_CLASS to use cheaper SImode moves
+   Similarly we play games with INTEGERSI_CLASS to use cheaper SImode moves
    whenever possible (upper half does contain padding).
  */
 enum x86_64_reg_class
@@ -839,6 +911,12 @@ static rtx construct_container PARAMS ((
 					const int *, int));
 static enum x86_64_reg_class merge_classes PARAMS ((enum x86_64_reg_class,
 						    enum x86_64_reg_class));
+static void x86_optimize_local_function PARAMS ((tree));
+
+/* Table of constants used by fldpi, fldln2, etc...  */
+static REAL_VALUE_TYPE ext_80387_constants_table [5];
+static bool ext_80387_constants_init = 0;
+static void init_ext_80387_constants PARAMS ((void));
 
 /* Initialize the GCC target structure.  */
 #undef TARGET_ATTRIBUTE_TABLE
@@ -897,6 +975,9 @@ static enum x86_64_reg_class merge_class
 #undef TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD
 #define TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD \
   ia32_multipass_dfa_lookahead
+#undef TARGET_CGRAPH_OPTIMIZE_LOCAL_FUNCTION
+#define TARGET_CGRAPH_OPTIMIZE_LOCAL_FUNCTION x86_optimize_local_function
+
 
 #ifdef HAVE_AS_TLS
 #undef TARGET_HAVE_TLS
@@ -944,17 +1025,17 @@ override_options ()
       const int align_jump;
       const int align_jump_max_skip;
       const int align_func;
-      const int branch_cost;
     }
   const processor_target_table[PROCESSOR_max] =
     {
-      {&i386_cost, 0, 0, 4, 3, 4, 3, 4, 1},
-      {&i486_cost, 0, 0, 16, 15, 16, 15, 16, 1},
-      {&pentium_cost, 0, 0, 16, 7, 16, 7, 16, 1},
-      {&pentiumpro_cost, 0, 0, 16, 15, 16, 7, 16, 1},
-      {&k6_cost, 0, 0, 32, 7, 32, 7, 32, 1},
-      {&athlon_cost, 0, 0, 16, 7, 64, 7, 16, 1},
-      {&pentium4_cost, 0, 0, 0, 0, 0, 0, 0, 1}
+      {&i386_cost, 0, 0, 4, 3, 4, 3, 4},
+      {&i486_cost, 0, 0, 16, 15, 16, 15, 16},
+      {&pentium_cost, 0, 0, 16, 7, 16, 7, 16},
+      {&pentiumpro_cost, 0, 0, 16, 15, 16, 7, 16},
+      {&k6_cost, 0, 0, 32, 7, 32, 7, 32},
+      {&athlon_cost, 0, 0, 16, 7, 16, 7, 16},
+      {&pentium4_cost, 0, 0, 0, 0, 0, 0, 0},
+      {&k8_cost, 0, 0, 16, 7, 16, 7, 16}
     };
 
   static const char * const cpu_names[] = TARGET_CPU_DEFAULT_NAMES;
@@ -969,7 +1050,8 @@ override_options ()
 	  PTA_MMX = 4,
 	  PTA_PREFETCH_SSE = 8,
 	  PTA_3DNOW = 16,
-	  PTA_3DNOW_A = 64
+	  PTA_3DNOW_A = 64,
+	  PTA_64BIT = 128
 	} flags;
     }
   const processor_alias_table[] =
@@ -1001,6 +1083,8 @@ override_options ()
 				      | PTA_3DNOW_A | PTA_SSE},
       {"athlon-mp", PROCESSOR_ATHLON, PTA_MMX | PTA_PREFETCH_SSE | PTA_3DNOW
 				      | PTA_3DNOW_A | PTA_SSE},
+      {"k8", PROCESSOR_K8, PTA_MMX | PTA_PREFETCH_SSE | PTA_3DNOW | PTA_64BIT
+				      | PTA_3DNOW_A | PTA_SSE | PTA_SSE2},
     };
 
   int const pta_size = ARRAY_SIZE (processor_alias_table);
@@ -1011,7 +1095,7 @@ override_options ()
   real_format_for_mode[TFmode - QFmode] = &ieee_extended_intel_128_format;
 
   /* Set the default values for switches whose default depends on TARGET_64BIT
-     in case they weren't overwriten by command line options.  */
+     in case they weren't overwritten by command line options.  */
   if (TARGET_64BIT)
     {
       if (flag_omit_frame_pointer == 2)
@@ -1040,7 +1124,7 @@ override_options ()
   if (!ix86_cpu_string)
     ix86_cpu_string = cpu_names [TARGET_CPU_DEFAULT];
   if (!ix86_arch_string)
-    ix86_arch_string = TARGET_64BIT ? "athlon-4" : "i386";
+    ix86_arch_string = TARGET_64BIT ? "k8" : "i386";
 
   if (ix86_cmodel_string != 0)
     {
@@ -1106,6 +1190,8 @@ override_options ()
 	  target_flags |= MASK_SSE2;
 	if (processor_alias_table[i].flags & PTA_PREFETCH_SSE)
 	  x86_prefetch_sse = true;
+	if (TARGET_64BIT && !(processor_alias_table[i].flags & PTA_64BIT))
+	  error ("CPU you selected does not support x86-64 instruction set");
 	break;
       }
 
@@ -1116,6 +1202,8 @@ override_options ()
     if (! strcmp (ix86_cpu_string, processor_alias_table[i].name))
       {
 	ix86_cpu = processor_alias_table[i].processor;
+	if (TARGET_64BIT && !(processor_alias_table[i].flags & PTA_64BIT))
+	  error ("CPU you selected does not support x86-64 instruction set");
 	break;
       }
   if (processor_alias_table[i].flags & PTA_PREFETCH_SSE)
@@ -1222,7 +1310,7 @@ override_options ()
     }
 
   /* Validate -mbranch-cost= value, or provide default.  */
-  ix86_branch_cost = processor_target_table[ix86_cpu].branch_cost;
+  ix86_branch_cost = processor_target_table[ix86_cpu].cost->branch_cost;
   if (ix86_branch_cost_string)
     {
       i = atoi (ix86_branch_cost_string);
@@ -1316,7 +1404,7 @@ override_options ()
   if (TARGET_3DNOW)
     {
       target_flags |= MASK_MMX;
-      /* If we are targetting the Athlon architecture, enable the 3Dnow/MMX
+      /* If we are targeting the Athlon architecture, enable the 3Dnow/MMX
 	 extensions it adds.  */
       if (x86_3dnow_a & (1 << ix86_arch))
 	target_flags |= MASK_3DNOW_A;
@@ -1629,7 +1717,7 @@ init_cumulative_args (cum, fntype, libna
   return;
 }
 
-/* x86-64 register passing impleemntation.  See x86-64 ABI for details.  Goal
+/* x86-64 register passing implementation.  See x86-64 ABI for details.  Goal
    of this code is to classify each 8bytes of incoming argument by the register
    class and assign registers accordingly.  */
 
@@ -2093,7 +2181,7 @@ construct_container (mode, type, in_retu
 	    break;
 	  case X86_64_INTEGER_CLASS:
 	  case X86_64_INTEGERSI_CLASS:
-	    /* Merge TImodes on aligned occassions here too.  */
+	    /* Merge TImodes on aligned occasions here too.  */
 	    if (i * 8 + 8 > bytes)
 	      tmpmode = mode_for_size ((bytes - i * 8) * BITS_PER_UNIT, MODE_INT, 0);
 	    else if (class[i] == X86_64_INTEGERSI_CLASS)
@@ -2233,7 +2321,7 @@ function_arg (cum, mode, type, named)
     (mode == BLKmode) ? int_size_in_bytes (type) : (int) GET_MODE_SIZE (mode);
   int words = (bytes + UNITS_PER_WORD - 1) / UNITS_PER_WORD;
 
-  /* Handle an hidden AL argument containing number of registers for varargs
+  /* Handle a hidden AL argument containing number of registers for varargs
      x86-64 functions.  For i386 ABI just return constm1_rtx to avoid
      any AL settings.  */
   if (mode == VOIDmode)
@@ -2447,8 +2535,8 @@ ix86_function_value (valtype)
       rtx ret = construct_container (TYPE_MODE (valtype), valtype, 1,
 				     REGPARM_MAX, SSE_REGPARM_MAX,
 				     x86_64_int_return_registers, 0);
-      /* For zero sized structures, construct_continer return NULL, but we need
-         to keep rest of compiler happy by returning meaningfull value.  */
+      /* For zero sized structures, construct_container return NULL, but we need
+         to keep rest of compiler happy by returning meaningful value.  */
       if (!ret)
 	ret = gen_rtx_REG (TYPE_MODE (valtype), 0);
       return ret;
@@ -2632,7 +2720,7 @@ ix86_setup_incoming_varargs (cum, mode, 
   if (next_cum.sse_nregs)
     {
       /* Now emit code to save SSE registers.  The AX parameter contains number
-	 of SSE parameter regsiters used to call this function.  We use
+	 of SSE parameter registers used to call this function.  We use
 	 sse_prologue_save insn template that produces computed jump across
 	 SSE saves.  We need some preparation work to get this working.  */
 
@@ -2806,11 +2894,11 @@ ix86_va_arg (valist, type)
       need_temp = ((needed_intregs && TYPE_ALIGN (type) > 64)
 		   || TYPE_ALIGN (type) > 128);
 
-      /* In case we are passing structure, verify that it is consetuctive block
+      /* In case we are passing structure, verify that it is consecutive block
          on the register save area.  If not we need to do moves.  */
       if (!need_temp && !REG_P (container))
 	{
-	  /* Verify that all registers are strictly consetuctive  */
+	  /* Verify that all registers are strictly consecutive  */
 	  if (SSE_REGNO_P (REGNO (XEXP (XVECEXP (container, 0, 0), 0))))
 	    {
 	      int i;
@@ -3009,7 +3097,7 @@ register_and_not_any_fp_reg_operand (op,
   return register_operand (op, mode) && !ANY_FP_REG_P (op);
 }
 
-/* Return nonzero of OP is a register operand other than an
+/* Return nonzero if OP is a register operand other than an
    i387 fp register.  */
 int
 register_and_not_fp_reg_operand (op, mode)
@@ -3062,7 +3150,9 @@ x86_64_nonmemory_operand (op, mode)
   return x86_64_sign_extended_value (op);
 }
 
-/* Return nonzero if OP is nonmemory operand acceptable by movabs patterns.  */
+/* Return nonzero if OP is nonmemory operand acceptable by movabs patterns.  
+   The predicate must accept only constants as otherwise reload may in-place
+   modify shared memory references in movabs* patterns.  */
 
 int
 x86_64_movabs_operand (op, mode)
@@ -3070,9 +3160,7 @@ x86_64_movabs_operand (op, mode)
      enum machine_mode mode;
 {
   if (!TARGET_64BIT || !flag_pic)
-    return nonmemory_operand (op, mode);
-  if (register_operand (op, mode) || x86_64_sign_extended_value (op))
-    return 1;
+    return immediate_operand (op, mode);
   if (CONSTANT_P (op) && !symbolic_reference_mentioned_p (op))
     return 1;
   return 0;
@@ -3383,7 +3471,7 @@ const248_operand (op, mode)
 	  && (INTVAL (op) == 2 || INTVAL (op) == 4 || INTVAL (op) == 8));
 }
 
-/* True if this is a constant appropriate for an increment or decremenmt.  */
+/* True if this is a constant appropriate for an increment or decrement.  */
 
 int
 incdec_operand (op, mode)
@@ -3520,6 +3608,18 @@ q_regs_operand (op, mode)
   return ANY_QI_REG_P (op);
 }
 
+/* Return true if op is an flags register.  */
+
+int
+flags_reg_operand (op, mode)
+     register rtx op;
+     enum machine_mode mode;
+{
+  if (mode != VOIDmode && GET_MODE (op) != mode)
+    return 0;
+  return REG_P (op) && REGNO (op) == FLAGS_REG && GET_MODE (op) != VOIDmode;
+}
+
 /* Return true if op is a NON_Q_REGS class register.  */
 
 int
@@ -3534,6 +3634,31 @@ non_q_regs_operand (op, mode)
   return NON_QI_REG_P (op);
 }
 
+int
+zero_extended_scalar_load_operand (op, mode)
+     rtx op;
+     enum machine_mode mode ATTRIBUTE_UNUSED;
+{
+  unsigned n_elts;
+  if (GET_CODE (op) != MEM)
+    return 0;
+  op = maybe_get_pool_constant (op);
+  if (!op)
+    return 0;
+  if (GET_CODE (op) != CONST_VECTOR)
+    return 0;
+  n_elts =
+    (GET_MODE_SIZE (GET_MODE (op)) /
+     GET_MODE_SIZE (GET_MODE_INNER (GET_MODE (op))));
+  for (n_elts--; n_elts > 0; n_elts--)
+    {
+      rtx elt = CONST_VECTOR_ELT (op, n_elts);
+      if (elt != CONST0_RTX (GET_MODE_INNER (GET_MODE (op))))
+	return 0;
+    }
+  return 1;
+}
+
 /*  Return 1 when OP is operand acceptable for standard SSE move.  */
 int
 vector_move_operand (op, mode)
@@ -3621,6 +3746,40 @@ ix86_comparison_operator (op, mode)
     }
 }
 
+/* Return 1 if OP is a valid comparison operator testing carry flag
+   to be set.  */
+int
+ix86_carry_flag_operator (op, mode)
+     register rtx op;
+     enum machine_mode mode;
+{
+  enum machine_mode inmode;
+  enum rtx_code code = GET_CODE (op);
+
+  if (mode != VOIDmode && GET_MODE (op) != mode)
+    return 0;
+  if (GET_RTX_CLASS (code) != '<')
+    return 0;
+  inmode = GET_MODE (XEXP (op, 0));
+  if (GET_CODE (XEXP (op, 0)) != REG
+      || REGNO (XEXP (op, 0)) != 17
+      || XEXP (op, 1) != const0_rtx)
+    return 0;
+
+  if (inmode == CCFPmode || inmode == CCFPUmode)
+    {
+      enum rtx_code second_code, bypass_code;
+
+      ix86_fp_comparison_codes (code, &bypass_code, &code, &second_code);
+      if (bypass_code != NIL || second_code != NIL)
+	return 0;
+      code = ix86_fp_compare_code_to_integer (code);
+    }
+  else if (inmode != CCmode)
+    return 0;
+  return code == LTU;
+}
+
 /* Return 1 if OP is a comparison operator that can be issued by fcmov.  */
 
 int
@@ -3630,6 +3789,7 @@ fcmov_comparison_operator (op, mode)
 {
   enum machine_mode inmode;
   enum rtx_code code = GET_CODE (op);
+
   if (mode != VOIDmode && GET_MODE (op) != mode)
     return 0;
   if (GET_RTX_CLASS (code) != '<')
@@ -3638,6 +3798,7 @@ fcmov_comparison_operator (op, mode)
   if (inmode == CCFPmode || inmode == CCFPUmode)
     {
       enum rtx_code second_code, bypass_code;
+
       ix86_fp_comparison_codes (code, &bypass_code, &code, &second_code);
       if (bypass_code != NIL || second_code != NIL)
 	return 0;
@@ -3713,7 +3874,7 @@ ext_register_operand (op, mode)
   if (!register_operand (op, VOIDmode))
     return 0;
 
-  /* Be curefull to accept only registers having upper parts.  */
+  /* Be careful to accept only registers having upper parts.  */
   regno = REG_P (op) ? REGNO (op) : REGNO (SUBREG_REG (op));
   return (regno > LAST_VIRTUAL_REGISTER || regno < 4);
 }
@@ -3886,9 +4047,34 @@ aligned_operand (op, mode)
   return 1;
 }
 
+/* Initialize the table of extra 80387 mathematical constants.  */
+
+static void
+init_ext_80387_constants ()
+{
+  static const char * cst[5] =
+  {
+    "0.3010299956639811952256464283594894482",  /* 0: fldlg2  */
+    "0.6931471805599453094286904741849753009",  /* 1: fldln2  */
+    "1.4426950408889634073876517827983434472",  /* 2: fldl2e  */
+    "3.3219280948873623478083405569094566090",  /* 3: fldl2t  */
+    "3.1415926535897932385128089594061862044",  /* 4: fldpi   */
+  };
+  int i;
+
+  for (i = 0; i < 5; i++)
+    {
+      real_from_string (&ext_80387_constants_table[i], cst[i]);
+      /* Ensure each constant is rounded to XFmode precision.  */
+      real_convert (&ext_80387_constants_table[i], XFmode,
+		    &ext_80387_constants_table[i]);
+    }
+
+  ext_80387_constants_init = 1;
+}
+
 /* Return true if the constant is something that can be loaded with
-   a special instruction.  Only handle 0.0 and 1.0; others are less
-   worthwhile.  */
+   a special instruction.  */
 
 int
 standard_80387_constant_p (x)
@@ -3896,16 +4082,89 @@ standard_80387_constant_p (x)
 {
   if (GET_CODE (x) != CONST_DOUBLE || !FLOAT_MODE_P (GET_MODE (x)))
     return -1;
-  /* Note that on the 80387, other constants, such as pi, that we should support
-     too.  On some machines, these are much slower to load as standard constant,
-     than to load from doubles in memory.  */
+
   if (x == CONST0_RTX (GET_MODE (x)))
     return 1;
   if (x == CONST1_RTX (GET_MODE (x)))
     return 2;
+
+  /* For XFmode constants, try to find a special 80387 instruction on
+     those CPUs that benefit from them.  */
+  if (GET_MODE (x) == XFmode
+      && x86_ext_80387_constants & CPUMASK)
+    {
+      REAL_VALUE_TYPE r;
+      int i;
+
+      if (! ext_80387_constants_init)
+	init_ext_80387_constants ();
+
+      REAL_VALUE_FROM_CONST_DOUBLE (r, x);
+      for (i = 0; i < 5; i++)
+        if (real_identical (&r, &ext_80387_constants_table[i]))
+	  return i + 3;
+    }
+
   return 0;
 }
 
+/* Return the opcode of the special instruction to be used to load
+   the constant X.  */
+
+const char *
+standard_80387_constant_opcode (x)
+     rtx x;
+{
+  switch (standard_80387_constant_p (x))
+    {
+    case 1: 
+      return "fldz";
+    case 2:
+      return "fld1";
+    case 3: 
+      return "fldlg2";
+    case 4:
+      return "fldln2";
+    case 5: 
+      return "fldl2e";
+    case 6:
+      return "fldl2t";
+    case 7: 
+      return "fldpi";
+    }
+  abort ();
+}
+
+/* Return the CONST_DOUBLE representing the 80387 constant that is
+   loaded by the specified special instruction.  The argument IDX
+   matches the return value from standard_80387_constant_p.  */
+
+rtx
+standard_80387_constant_rtx (idx)
+     int idx;
+{
+  int i;
+
+  if (! ext_80387_constants_init)
+    init_ext_80387_constants ();
+
+  switch (idx)
+    {
+    case 3:
+    case 4:
+    case 5:
+    case 6:
+    case 7:
+      i = idx - 3;
+      break;
+
+    default:
+      abort ();
+    }
+
+  return CONST_DOUBLE_FROM_REAL_VALUE (ext_80387_constants_table[i], XFmode);
+}
+
 /* Return 1 if X is FP constant we can load to SSE register w/o using memory.
  */
 int
@@ -4603,14 +4862,32 @@ ix86_expand_prologue ()
   int use_mov = 0;
   HOST_WIDE_INT allocate;
 
+  ix86_compute_frame_layout (&frame);
   if (!optimize_size)
     {
-      use_fast_prologue_epilogue
-	 = !expensive_function_p (FAST_PROLOGUE_INSN_COUNT);
+      int count = frame.nregs;
+
+      /* The fast prologue uses move instead of push to save registers.  This
+         is significantly longer, but also executes faster as modern hardware
+         can execute the moves in parallel, but can't do that for push/pop.
+	 
+	 Be careful about choosing what prologue to emit:  When function takes
+	 many instructions to execute we may use slow version as well as in
+	 case function is known to be outside hot spot (this is known with
+	 feedback only).  Weight the size of function by number of registers
+	 to save as it is cheap to use one or two push instructions but very
+	 slow to use many of them.  */
+      if (count)
+	count = (count - 1) * FAST_PROLOGUE_INSN_COUNT;
+      if (cfun->function_frequency < FUNCTION_FREQUENCY_NORMAL
+	  || (flag_branch_probabilities
+	      && cfun->function_frequency < FUNCTION_FREQUENCY_HOT))
+	use_fast_prologue_epilogue = 0;
+      else
+        use_fast_prologue_epilogue = !expensive_function_p (count);
       if (TARGET_PROLOGUE_USING_MOVE)
         use_mov = use_fast_prologue_epilogue;
     }
-  ix86_compute_frame_layout (&frame);
 
   /* Note: AT&T enter does NOT have reversed args.  Enter is probably
      slower on all targets.  Also sdb doesn't like it.  */
@@ -4701,7 +4978,7 @@ ix86_expand_prologue ()
       /* Even with accurate pre-reload life analysis, we can wind up
 	 deleting all references to the pic register after reload.
 	 Consider if cross-jumping unifies two sides of a branch
-	 controled by a comparison vs the only read from a global.
+	 controlled by a comparison vs the only read from a global.
 	 In which case, allow the set_got to be deleted, though we're
 	 too late to do anything about the ebx save in the prologue.  */
       REG_NOTES (insn) = gen_rtx_EXPR_LIST (REG_MAYBE_DEAD, const0_rtx, NULL);
@@ -4763,7 +5040,7 @@ ix86_expand_epilogue (style)
      while this code results in LEAVE instruction (or discrete equivalent),
      so it is profitable in some other cases as well.  Especially when there
      are no registers to restore.  We also use this code when TARGET_USE_LEAVE
-     and there is exactly one register to pop. This heruistic may need some
+     and there is exactly one register to pop. This heuristic may need some
      tuning in future.  */
   if ((!sp_valid && frame.nregs <= 1)
       || (TARGET_EPILOGUE_USING_MOVE
@@ -5185,6 +5462,13 @@ legitimate_constant_p (x)
 	  && tls_symbolic_operand (XEXP (inner, 0), Pmode))
 	return false;
 
+      if (GET_CODE (inner) == PLUS)
+	{
+	  if (GET_CODE (XEXP (inner, 1)) != CONST_INT)
+	    return false;
+	  inner = XEXP (inner, 0);
+	}
+
       /* Only some unspecs are valid as "constants".  */
       if (GET_CODE (inner) == UNSPEC)
 	switch (XINT (inner, 1))
@@ -5221,26 +5505,7 @@ bool
 constant_address_p (x)
      rtx x;
 {
-  switch (GET_CODE (x))
-    {
-    case LABEL_REF:
-    case CONST_INT:
-      return true;
-
-    case CONST_DOUBLE:
-      return TARGET_64BIT;
-
-    case CONST:
-      /* For Mach-O, really believe the CONST.  */
-      if (TARGET_MACHO)
-	return true;
-      /* Otherwise fall through.  */
-    case SYMBOL_REF:
-      return !flag_pic && legitimate_constant_p (x);
-
-    default:
-      return false;
-    }
+  return CONSTANT_P (x) && legitimate_address_p (Pmode, x, 1);
 }
 
 /* Nonzero if the constant value X is a legitimate general operand
@@ -5363,7 +5628,10 @@ legitimate_pic_address_disp_p (disp)
 	return false;
       return GET_CODE (XVECEXP (disp, 0, 0)) == SYMBOL_REF;
     case UNSPEC_GOTOFF:
-      return local_symbolic_operand (XVECEXP (disp, 0, 0), Pmode);
+      if (GET_CODE (XVECEXP (disp, 0, 0)) == SYMBOL_REF
+	  || GET_CODE (XVECEXP (disp, 0, 0)) == LABEL_REF)
+        return local_symbolic_operand (XVECEXP (disp, 0, 0), Pmode);
+      return false;
     case UNSPEC_GOTTPOFF:
     case UNSPEC_GOTNTPOFF:
     case UNSPEC_INDNTPOFF:
@@ -5591,7 +5859,12 @@ legitimate_address_p (mode, addr, strict
 	     that never results in lea, this seems to be easier and
 	     correct fix for crash to disable this test.  */
 	}
-      else if (!CONSTANT_ADDRESS_P (disp))
+      else if (GET_CODE (disp) != LABEL_REF
+	       && GET_CODE (disp) != CONST_INT
+	       && (GET_CODE (disp) != CONST
+		   || !legitimate_constant_p (disp))
+	       && (GET_CODE (disp) != SYMBOL_REF
+		   || !legitimate_constant_p (disp)))
 	{
 	  reason = "displacement is not constant";
 	  goto report_error;
@@ -5601,11 +5874,6 @@ legitimate_address_p (mode, addr, strict
 	  reason = "displacement is out of range";
 	  goto report_error;
 	}
-      else if (!TARGET_64BIT && GET_CODE (disp) == CONST_DOUBLE)
-	{
-	  reason = "displacement is a const_double";
-	  goto report_error;
-	}
     }
 
   /* Everything looks valid.  */
@@ -5676,7 +5944,15 @@ legitimize_pic_address (orig, reg)
 
       if (reload_in_progress)
 	regs_ever_live[PIC_OFFSET_TABLE_REGNUM] = 1;
-      new = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, addr), UNSPEC_GOTOFF);
+      if (GET_CODE (addr) == CONST)
+	addr = XEXP (addr, 0);
+      if (GET_CODE (addr) == PLUS)
+	  {
+            new = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, XEXP (addr, 0)), UNSPEC_GOTOFF);
+	    new = gen_rtx_PLUS (Pmode, new, XEXP (addr, 1));
+	  }
+	else
+          new = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, addr), UNSPEC_GOTOFF);
       new = gen_rtx_CONST (Pmode, new);
       new = gen_rtx_PLUS (Pmode, pic_offset_table_rtx, new);
 
@@ -6947,7 +7223,7 @@ print_operand (file, x, code)
 		    int cputaken = final_forward_branch_p (current_output_insn) == 0;
 
 		    /* Emit hints only in the case default branch prediction
-		       heruistics would fail.  */
+		       heuristics would fail.  */
 		    if (taken != cputaken)
 		      {
 			/* We use 3e (DS) prefix for taken branches and
@@ -7002,10 +7278,8 @@ print_operand (file, x, code)
 	}
 
       x = XEXP (x, 0);
-      if (flag_pic && CONSTANT_ADDRESS_P (x))
-	output_pic_addr_const (file, x, code);
       /* Avoid (%rip) for call operands.  */
-      else if (CONSTANT_ADDRESS_P (x) && code == 'P'
+      if (CONSTANT_ADDRESS_P (x) && code == 'P'
 	       && GET_CODE (x) != CONST_INT)
 	output_addr_const (file, x);
       else if (this_is_asm_operands && ! address_operand (x, VOIDmode))
@@ -8296,7 +8570,7 @@ ix86_cc_mode (code, op0, op1)
 	return CCGCmode;
       /* Codes doable only with sign flag when comparing
          against zero, but we miss jump instruction for it
-         so we need to use relational tests agains overflow
+         so we need to use relational tests against overflow
          that thus needs to be zero.  */
     case GT:			/* ZF=0 & SF=OF */
     case LE:			/* ZF=1 | SF<>OF */
@@ -8327,7 +8601,7 @@ ix86_use_fcomi_compare (code)
 
 /* Swap, force into registers, or otherwise massage the two operands
    to a fp comparison.  The operands are updated in place; the new
-   comparsion code is returned.  */
+   comparison code is returned.  */
 
 static enum rtx_code
 ix86_prepare_fp_compare_args (code, pop0, pop1)
@@ -8498,7 +8772,7 @@ ix86_fp_comparison_codes (code, bypass_c
 }
 
 /* Return cost of comparison done fcom + arithmetics operations on AX.
-   All following functions do use number of instructions as an cost metrics.
+   All following functions do use number of instructions as a cost metrics.
    In future this should be tweaked to compute bytes for optimize_size and
    take into account performance of various instructions on various CPUs.  */
 static int
@@ -8542,7 +8816,7 @@ ix86_fp_comparison_fcomi_cost (code)
      enum rtx_code code;
 {
   enum rtx_code bypass_code, first_code, second_code;
-  /* Return arbitarily high cost when instruction is not supported - this
+  /* Return arbitrarily high cost when instruction is not supported - this
      prevents gcc from using it.  */
   if (!TARGET_CMOVE)
     return 1024;
@@ -8557,7 +8831,7 @@ ix86_fp_comparison_sahf_cost (code)
      enum rtx_code code;
 {
   enum rtx_code bypass_code, first_code, second_code;
-  /* Return arbitarily high cost when instruction is not preferred - this
+  /* Return arbitrarily high cost when instruction is not preferred - this
      avoids gcc from using it.  */
   if (!TARGET_USE_SAHF && !optimize_size)
     return 1024;
@@ -9123,6 +9397,128 @@ ix86_expand_setcc (code, dest)
   return 1; /* DONE */
 }
 
+/* Expand comparison setting or clearing carry flag.  Return true when successful
+   and set pop for the operation.  */
+bool
+ix86_expand_carry_flag_compare (code, op0, op1, pop)
+     rtx op0, op1, *pop;
+     enum rtx_code code;
+{
+  enum machine_mode mode =
+    GET_MODE (op0) != VOIDmode ? GET_MODE (op0) : GET_MODE (op1);
+
+  /* Do not handle DImode compares that go trought special path.  Also we can't
+     deal with FP compares yet.  This is possible to add.   */
+  if ((mode == DImode && !TARGET_64BIT))
+    return false;
+  if (FLOAT_MODE_P (mode))
+    {
+      rtx second_test = NULL, bypass_test = NULL;
+      rtx compare_op, compare_seq;
+
+      /* Shortcut:  following common codes never translate into carry flag compares.  */
+      if (code == EQ || code == NE || code == UNEQ || code == LTGT
+	  || code == ORDERED || code == UNORDERED)
+	return false;
+
+      /* These comparisons require zero flag; swap operands so they won't.  */
+      if ((code == GT || code == UNLE || code == LE || code == UNGT)
+	  && !TARGET_IEEE_FP)
+	{
+	  rtx tmp = op0;
+	  op0 = op1;
+	  op1 = tmp;
+	  code = swap_condition (code);
+	}
+
+      /* Try to expand the comparsion and verify that we end up with carry flag
+	 based comparsion.  This is fails to be true only when we decide to expand
+	 comparsion using arithmetic that is not too common scenario.  */
+      start_sequence ();
+      compare_op = ix86_expand_fp_compare (code, op0, op1, NULL_RTX,
+					   &second_test, &bypass_test);
+      compare_seq = get_insns ();
+      end_sequence ();
+
+      if (second_test || bypass_test)
+	return false;
+      if (GET_MODE (XEXP (compare_op, 0)) == CCFPmode
+	  || GET_MODE (XEXP (compare_op, 0)) == CCFPUmode)
+        code = ix86_fp_compare_code_to_integer (GET_CODE (compare_op));
+      else
+	code = GET_CODE (compare_op);
+      if (code != LTU && code != GEU)
+	return false;
+      emit_insn (compare_seq);
+      *pop = compare_op;
+      return true;
+    }
+  if (!INTEGRAL_MODE_P (mode))
+    return false;
+  switch (code)
+    {
+    case LTU:
+    case GEU:
+      break;
+
+    /* Convert a==0 into (unsigned)a<1.  */
+    case EQ:
+    case NE:
+      if (op1 != const0_rtx)
+	return false;
+      op1 = const1_rtx;
+      code = (code == EQ ? LTU : GEU);
+      break;
+
+    /* Convert a>b into b<a or a>=b-1.  */
+    case GTU:
+    case LEU:
+      if (GET_CODE (op1) == CONST_INT)
+	{
+	  op1 = gen_int_mode (INTVAL (op1) + 1, GET_MODE (op0));
+	  /* Bail out on overflow.  We still can swap operands but that
+	     would force loading of the constant into register. */
+	  if (op1 == const0_rtx
+	      || !x86_64_immediate_operand (op1, GET_MODE (op1)))
+	    return false;
+	  code = (code == GTU ? GEU : LTU);
+	}
+      else
+	{
+	  rtx tmp = op1;
+	  op1 = op0;
+	  op0 = tmp;
+	  code = (code == GTU ? LTU : GEU);
+	}
+      break;
+
+    /* Convert a>0 into (unsigned)a<0x7fffffff.  */
+    case LT:
+    case GE:
+      if (mode == DImode || op1 != const0_rtx)
+	return false;
+      op1 = gen_int_mode (~(1 << (GET_MODE_BITSIZE (mode) - 1)), mode);
+      code = (code == LT ? GEU : LTU);
+      break;
+    case LE:
+    case GT:
+      if (mode == DImode || op1 != constm1_rtx)
+	return false;
+      op1 = gen_int_mode (~(1 << (GET_MODE_BITSIZE (mode) - 1)), mode);
+      code = (code == LE ? GEU : LTU);
+      break;
+
+    default:
+      return false;
+    }
+  ix86_compare_op0 = op0;
+  ix86_compare_op1 = op1;
+  *pop = ix86_expand_compare (code, NULL, NULL);
+  if (GET_CODE (*pop) != LTU && GET_CODE (*pop) != GEU)
+    abort ();
+  return true;
+}
+
 int
 ix86_expand_int_movcc (operands)
      rtx operands[];
@@ -9131,30 +9527,7 @@ ix86_expand_int_movcc (operands)
   rtx compare_seq, compare_op;
   rtx second_test, bypass_test;
   enum machine_mode mode = GET_MODE (operands[0]);
-
-  /* When the compare code is not LTU or GEU, we can not use sbbl case.
-     In case comparsion is done with immediate, we can convert it to LTU or
-     GEU by altering the integer.  */
-
-  if ((code == LEU || code == GTU)
-      && GET_CODE (ix86_compare_op1) == CONST_INT
-      && mode != HImode
-      && INTVAL (ix86_compare_op1) != -1
-      /* For x86-64, the immediate field in the instruction is 32-bit
-	 signed, so we can't increment a DImode value above 0x7fffffff.  */
-      && (!TARGET_64BIT
-	  || GET_MODE (ix86_compare_op0) != DImode
-	  || INTVAL (ix86_compare_op1) != 0x7fffffff)
-      && GET_CODE (operands[2]) == CONST_INT
-      && GET_CODE (operands[3]) == CONST_INT)
-    {
-      if (code == LEU)
-	code = LTU;
-      else
-	code = GEU;
-      ix86_compare_op1 = gen_int_mode (INTVAL (ix86_compare_op1) + 1,
-				       GET_MODE (ix86_compare_op0));
-    }
+  bool sign_bit_compare_p = false;;
 
   start_sequence ();
   compare_op = ix86_expand_compare (code, &second_test, &bypass_test);
@@ -9163,10 +9536,14 @@ ix86_expand_int_movcc (operands)
 
   compare_code = GET_CODE (compare_op);
 
+  if ((ix86_compare_op1 == const0_rtx && (code == GE || code == LT))
+      || (ix86_compare_op1 == constm1_rtx && (code == GT || code == LE)))
+    sign_bit_compare_p = true;
+
   /* Don't attempt mode expansion here -- if we had to expand 5 or 6
      HImode insns, we'd be swallowed in word prefix ops.  */
 
-  if (mode != HImode
+  if ((mode != HImode || TARGET_FAST_PREFIX)
       && (mode != DImode || TARGET_64BIT)
       && GET_CODE (operands[2]) == CONST_INT
       && GET_CODE (operands[3]) == CONST_INT)
@@ -9176,32 +9553,72 @@ ix86_expand_int_movcc (operands)
       HOST_WIDE_INT cf = INTVAL (operands[3]);
       HOST_WIDE_INT diff;
 
-      if ((compare_code == LTU || compare_code == GEU)
-	  && !second_test && !bypass_test)
+      diff = ct - cf;
+      /*  Sign bit compares are better done using shifts than we do by using
+ 	  sbb.  */
+      if (sign_bit_compare_p
+	  || ix86_expand_carry_flag_compare (code, ix86_compare_op0,
+					     ix86_compare_op1, &compare_op))
 	{
 	  /* Detect overlap between destination and compare sources.  */
 	  rtx tmp = out;
 
-	  /* To simplify rest of code, restrict to the GEU case.  */
-	  if (compare_code == LTU)
+          if (!sign_bit_compare_p)
 	    {
-	      HOST_WIDE_INT tmp = ct;
-	      ct = cf;
-	      cf = tmp;
-	      compare_code = reverse_condition (compare_code);
-	      code = reverse_condition (code);
-	    }
-	  diff = ct - cf;
+	      bool fpcmp = false;
+
+	      compare_code = GET_CODE (compare_op);
+
+	      if (GET_MODE (XEXP (compare_op, 0)) == CCFPmode
+		  || GET_MODE (XEXP (compare_op, 0)) == CCFPUmode)
+		{
+		  fpcmp = true;
+		  compare_code = ix86_fp_compare_code_to_integer (compare_code);
+		}
+
+	      /* To simplify rest of code, restrict to the GEU case.  */
+	      if (compare_code == LTU)
+		{
+		  HOST_WIDE_INT tmp = ct;
+		  ct = cf;
+		  cf = tmp;
+		  compare_code = reverse_condition (compare_code);
+		  code = reverse_condition (code);
+		}
+	      else
+		{
+		  if (fpcmp)
+		    PUT_CODE (compare_op,
+			      reverse_condition_maybe_unordered
+			        (GET_CODE (compare_op)));
+		  else
+		    PUT_CODE (compare_op, reverse_condition (GET_CODE (compare_op)));
+		}
+	      diff = ct - cf;
 
-	  if (reg_overlap_mentioned_p (out, ix86_compare_op0)
-	      || reg_overlap_mentioned_p (out, ix86_compare_op1))
-	    tmp = gen_reg_rtx (mode);
+	      if (reg_overlap_mentioned_p (out, ix86_compare_op0)
+		  || reg_overlap_mentioned_p (out, ix86_compare_op1))
+		tmp = gen_reg_rtx (mode);
 
-	  emit_insn (compare_seq);
-	  if (mode == DImode)
-	    emit_insn (gen_x86_movdicc_0_m1_rex64 (tmp));
+	      if (mode == DImode)
+		emit_insn (gen_x86_movdicc_0_m1_rex64 (tmp, compare_op));
+	      else
+		emit_insn (gen_x86_movsicc_0_m1 (gen_lowpart (SImode, tmp), compare_op));
+	    }
 	  else
-	    emit_insn (gen_x86_movsicc_0_m1 (tmp));
+	    {
+	      if (code == GT || code == GE)
+		code = reverse_condition (code);
+	      else
+		{
+		  HOST_WIDE_INT tmp = ct;
+		  ct = cf;
+		  cf = tmp;
+		  diff = ct - cf;
+		}
+	      tmp = emit_store_flag (tmp, code, ix86_compare_op0,
+				     ix86_compare_op1, VOIDmode, 0, -1);
+	    }
 
 	  if (diff == 1)
 	    {
@@ -9215,7 +9632,7 @@ ix86_expand_int_movcc (operands)
 	      if (ct)
 	       	tmp = expand_simple_binop (mode, PLUS,
 					   tmp, GEN_INT (ct),
-					   tmp, 1, OPTAB_DIRECT);
+					   copy_rtx (tmp), 1, OPTAB_DIRECT);
 	    }
 	  else if (cf == -1)
 	    {
@@ -9228,7 +9645,7 @@ ix86_expand_int_movcc (operands)
 	       */
 	      tmp = expand_simple_binop (mode, IOR,
 					 tmp, GEN_INT (ct),
-					 tmp, 1, OPTAB_DIRECT);
+					 copy_rtx (tmp), 1, OPTAB_DIRECT);
 	    }
 	  else if (diff == -1 && ct)
 	    {
@@ -9240,11 +9657,11 @@ ix86_expand_int_movcc (operands)
 	       *
 	       * Size 8 - 11.
 	       */
-	      tmp = expand_simple_unop (mode, NOT, tmp, tmp, 1);
+	      tmp = expand_simple_unop (mode, NOT, tmp, copy_rtx (tmp), 1);
 	      if (cf)
 	       	tmp = expand_simple_binop (mode, PLUS,
-					   tmp, GEN_INT (cf),
-					   tmp, 1, OPTAB_DIRECT);
+					   copy_rtx (tmp), GEN_INT (cf),
+					   copy_rtx (tmp), 1, OPTAB_DIRECT);
 	    }
 	  else
 	    {
@@ -9262,26 +9679,25 @@ ix86_expand_int_movcc (operands)
 		{
 		  cf = ct;
 		  ct = 0;
-		  tmp = expand_simple_unop (mode, NOT, tmp, tmp, 1);
+		  tmp = expand_simple_unop (mode, NOT, tmp, copy_rtx (tmp), 1);
 		}
 
 	      tmp = expand_simple_binop (mode, AND,
-					 tmp,
+					 copy_rtx (tmp),
 					 gen_int_mode (cf - ct, mode),
-					 tmp, 1, OPTAB_DIRECT);
+					 copy_rtx (tmp), 1, OPTAB_DIRECT);
 	      if (ct)
 	       	tmp = expand_simple_binop (mode, PLUS,
-					   tmp, GEN_INT (ct),
-					   tmp, 1, OPTAB_DIRECT);
+					   copy_rtx (tmp), GEN_INT (ct),
+					   copy_rtx (tmp), 1, OPTAB_DIRECT);
 	    }
 
-	  if (tmp != out)
-	    emit_move_insn (out, tmp);
+	  if (!rtx_equal_p (tmp, out))
+	    emit_move_insn (copy_rtx (out), copy_rtx (tmp));
 
 	  return 1; /* DONE */
 	}
 
-      diff = ct - cf;
       if (diff < 0)
 	{
 	  HOST_WIDE_INT tmp;
@@ -9357,8 +9773,10 @@ ix86_expand_int_movcc (operands)
 	    }
 	}
 
+
       if ((diff == 1 || diff == 2 || diff == 4 || diff == 8
 	   || diff == 3 || diff == 5 || diff == 9)
+	  && ((mode != QImode && mode != HImode) || !TARGET_PARTIAL_REG_STALL)
 	  && (mode != DImode || x86_64_sign_extended_value (GEN_INT (cf))))
 	{
 	  /*
@@ -9400,15 +9818,14 @@ ix86_expand_int_movcc (operands)
 	      tmp = gen_rtx_PLUS (mode, tmp, GEN_INT (cf));
 	      nops++;
 	    }
-	  if (tmp != out
-	      && (GET_CODE (tmp) != SUBREG || SUBREG_REG (tmp) != out))
+	  if (!rtx_equal_p (tmp, out))
 	    {
 	      if (nops == 1)
 		out = force_operand (tmp, copy_rtx (out));
 	      else
 		emit_insn (gen_rtx_SET (VOIDmode, copy_rtx (out), copy_rtx (tmp)));
 	    }
-	  if (out != operands[0])
+	  if (!rtx_equal_p (out, operands[0]))
 	    emit_move_insn (operands[0], copy_rtx (out));
 
 	  return 1; /* DONE */
@@ -9428,12 +9845,10 @@ ix86_expand_int_movcc (operands)
        * This is reasonably steep, but branch mispredict costs are
        * high on modern cpus, so consider failing only if optimizing
        * for space.
-       *
-       * %%% Parameterize branch_cost on the tuning architecture, then
-       * use that.  The 80386 couldn't care less about mispredicts.
        */
 
-      if (!optimize_size && !TARGET_CMOVE)
+      if ((!TARGET_CMOVE || (mode == QImode && TARGET_PARTIAL_REG_STALL))
+	  && BRANCH_COST >= 2)
 	{
 	  if (cf == 0)
 	    {
@@ -9487,31 +9902,31 @@ ix86_expand_int_movcc (operands)
 	      out = emit_store_flag (out, code, ix86_compare_op0,
 				     ix86_compare_op1, VOIDmode, 0, 1);
 
-	      out = expand_simple_binop (mode, PLUS, out, constm1_rtx,
-					 out, 1, OPTAB_DIRECT);
+	      out = expand_simple_binop (mode, PLUS, copy_rtx (out), constm1_rtx,
+					 copy_rtx (out), 1, OPTAB_DIRECT);
 	    }
 
-	  out = expand_simple_binop (mode, AND, out,
+	  out = expand_simple_binop (mode, AND, copy_rtx (out),
 				     gen_int_mode (cf - ct, mode),
-				     out, 1, OPTAB_DIRECT);
+				     copy_rtx (out), 1, OPTAB_DIRECT);
 	  if (ct)
-	    out = expand_simple_binop (mode, PLUS, out, GEN_INT (ct),
-				       out, 1, OPTAB_DIRECT);
-	  if (out != operands[0])
-	    emit_move_insn (operands[0], out);
+	    out = expand_simple_binop (mode, PLUS, copy_rtx (out), GEN_INT (ct),
+				       copy_rtx (out), 1, OPTAB_DIRECT);
+	  if (!rtx_equal_p (out, operands[0]))
+	    emit_move_insn (operands[0], copy_rtx (out));
 
 	  return 1; /* DONE */
 	}
     }
 
-  if (!TARGET_CMOVE)
+  if (!TARGET_CMOVE || (mode == QImode && TARGET_PARTIAL_REG_STALL))
     {
       /* Try a few things more with specific constants and a variable.  */
 
       optab op;
       rtx var, orig_out, out, tmp;
 
-      if (optimize_size)
+      if (BRANCH_COST <= 2)
 	return 0; /* FAIL */
 
       /* If one of the two operands is an interesting constant, load a
@@ -9520,9 +9935,9 @@ ix86_expand_int_movcc (operands)
       if (GET_CODE (operands[2]) == CONST_INT)
 	{
 	  var = operands[3];
-	  if (INTVAL (operands[2]) == 0)
+	  if (INTVAL (operands[2]) == 0 && operands[3] != constm1_rtx)
 	    operands[3] = constm1_rtx, op = and_optab;
-	  else if (INTVAL (operands[2]) == -1)
+	  else if (INTVAL (operands[2]) == -1 && operands[3] != const0_rtx)
 	    operands[3] = const0_rtx, op = ior_optab;
 	  else
 	    return 0; /* FAIL */
@@ -9530,9 +9945,9 @@ ix86_expand_int_movcc (operands)
       else if (GET_CODE (operands[3]) == CONST_INT)
 	{
 	  var = operands[2];
-	  if (INTVAL (operands[3]) == 0)
+	  if (INTVAL (operands[3]) == 0 && operands[2] != constm1_rtx)
 	    operands[2] = constm1_rtx, op = and_optab;
-	  else if (INTVAL (operands[3]) == -1)
+	  else if (INTVAL (operands[3]) == -1 && operands[3] != const0_rtx)
 	    operands[2] = const0_rtx, op = ior_optab;
 	  else
 	    return 0; /* FAIL */
@@ -9551,8 +9966,8 @@ ix86_expand_int_movcc (operands)
       /* Mask in the interesting variable.  */
       out = expand_binop (mode, op, var, tmp, orig_out, 0,
 			  OPTAB_WIDEN);
-      if (out != orig_out)
-	emit_move_insn (orig_out, out);
+      if (!rtx_equal_p (out, orig_out))
+	emit_move_insn (copy_rtx (orig_out), copy_rtx (out));
 
       return 1; /* DONE */
     }
@@ -9585,27 +10000,33 @@ ix86_expand_int_movcc (operands)
       emit_move_insn (tmp, operands[2]);
       operands[2] = tmp;
     }
+
   if (! register_operand (operands[2], VOIDmode)
-      && ! register_operand (operands[3], VOIDmode))
+      && (mode == QImode 
+          || ! register_operand (operands[3], VOIDmode)))
     operands[2] = force_reg (mode, operands[2]);
 
+  if (mode == QImode
+      && ! register_operand (operands[3], VOIDmode))
+    operands[3] = force_reg (mode, operands[3]);
+
   emit_insn (compare_seq);
   emit_insn (gen_rtx_SET (VOIDmode, operands[0],
 			  gen_rtx_IF_THEN_ELSE (mode,
 						compare_op, operands[2],
 						operands[3])));
   if (bypass_test)
-    emit_insn (gen_rtx_SET (VOIDmode, operands[0],
+    emit_insn (gen_rtx_SET (VOIDmode, copy_rtx (operands[0]),
 			    gen_rtx_IF_THEN_ELSE (mode,
 				  bypass_test,
-				  operands[3],
-				  operands[0])));
+				  copy_rtx (operands[3]),
+				  copy_rtx (operands[0]))));
   if (second_test)
-    emit_insn (gen_rtx_SET (VOIDmode, operands[0],
+    emit_insn (gen_rtx_SET (VOIDmode, copy_rtx (operands[0]),
 			    gen_rtx_IF_THEN_ELSE (mode,
 				  second_test,
-				  operands[2],
-				  operands[0])));
+				  copy_rtx (operands[2]),
+				  copy_rtx (operands[0]))));
 
   return 1; /* DONE */
 }
@@ -9646,8 +10067,14 @@ ix86_expand_fp_movcc (operands)
       if (rtx_equal_p (operands[2], op0) && rtx_equal_p (operands[3], op1))
 	{
 	  /* Check for min operation.  */
-	  if (code == LT)
+	  if (code == LT || code == UNLE)
 	    {
+	       if (code == UNLE)
+		{
+		  rtx tmp = op0;
+		  op0 = op1;
+		  op1 = tmp;
+		}
 	       operands[0] = force_reg (GET_MODE (operands[0]), operands[0]);
 	       if (memory_operand (op0, VOIDmode))
 		 op0 = force_reg (GET_MODE (operands[0]), op0);
@@ -9658,8 +10085,14 @@ ix86_expand_fp_movcc (operands)
 	       return 1;
 	    }
 	  /* Check for max operation.  */
-	  if (code == GT)
+	  if (code == GT || code == UNGE)
 	    {
+	       if (code == UNGE)
+		{
+		  rtx tmp = op0;
+		  op0 = op1;
+		  op1 = tmp;
+		}
 	       operands[0] = force_reg (GET_MODE (operands[0]), operands[0]);
 	       if (memory_operand (op0, VOIDmode))
 		 op0 = force_reg (GET_MODE (operands[0]), op0);
@@ -9684,7 +10117,7 @@ ix86_expand_fp_movcc (operands)
 					VOIDmode, ix86_compare_op0,
 					ix86_compare_op1);
 	}
-      /* Similary try to manage result to be first operand of conditional
+      /* Similarly try to manage result to be first operand of conditional
 	 move. We also don't support the NE comparison on SSE, so try to
 	 avoid it.  */
       if ((rtx_equal_p (operands[0], operands[3])
@@ -9764,6 +10197,91 @@ ix86_expand_fp_movcc (operands)
   return 1;
 }
 
+/* Expand conditional increment or decrement using adb/sbb instructions.
+   The default case using setcc followed by the conditional move can be
+   done by generic code.  */
+int
+ix86_expand_int_addcc (operands)
+     rtx operands[];
+{
+  enum rtx_code code = GET_CODE (operands[1]);
+  rtx compare_op;
+  rtx val = const0_rtx;
+  bool fpcmp = false;
+  enum machine_mode mode = GET_MODE (operands[0]);
+
+  if (operands[3] != const1_rtx
+      && operands[3] != constm1_rtx)
+    return 0;
+  if (!ix86_expand_carry_flag_compare (code, ix86_compare_op0,
+				       ix86_compare_op1, &compare_op))
+     return 0;
+  code = GET_CODE (compare_op);
+
+  if (GET_MODE (XEXP (compare_op, 0)) == CCFPmode
+      || GET_MODE (XEXP (compare_op, 0)) == CCFPUmode)
+    {
+      fpcmp = true;
+      code = ix86_fp_compare_code_to_integer (code);
+    }
+
+  if (code != LTU)
+    {
+      val = constm1_rtx;
+      if (fpcmp)
+	PUT_CODE (compare_op,
+		  reverse_condition_maybe_unordered
+		    (GET_CODE (compare_op)));
+      else
+	PUT_CODE (compare_op, reverse_condition (GET_CODE (compare_op)));
+    }
+  PUT_MODE (compare_op, mode);
+
+  /* Construct either adc or sbb insn.  */
+  if ((code == LTU) == (operands[3] == constm1_rtx))
+    {
+      switch (GET_MODE (operands[0]))
+	{
+	  case QImode:
+            emit_insn (gen_subqi3_carry (operands[0], operands[2], val, compare_op));
+	    break;
+	  case HImode:
+            emit_insn (gen_subhi3_carry (operands[0], operands[2], val, compare_op));
+	    break;
+	  case SImode:
+            emit_insn (gen_subsi3_carry (operands[0], operands[2], val, compare_op));
+	    break;
+	  case DImode:
+            emit_insn (gen_subdi3_carry_rex64 (operands[0], operands[2], val, compare_op));
+	    break;
+	  default:
+	    abort ();
+	}
+    }
+  else
+    {
+      switch (GET_MODE (operands[0]))
+	{
+	  case QImode:
+            emit_insn (gen_addqi3_carry (operands[0], operands[2], val, compare_op));
+	    break;
+	  case HImode:
+            emit_insn (gen_addhi3_carry (operands[0], operands[2], val, compare_op));
+	    break;
+	  case SImode:
+            emit_insn (gen_addsi3_carry (operands[0], operands[2], val, compare_op));
+	    break;
+	  case DImode:
+            emit_insn (gen_adddi3_carry_rex64 (operands[0], operands[2], val, compare_op));
+	    break;
+	  default:
+	    abort ();
+	}
+    }
+  return 1; /* DONE */
+}
+
+
 /* Split operands 0 and 1 into SImode parts.  Similar to split_di, but
    works for floating pointer parameters and nonoffsetable memories.
    For pushes, it returns just stack offsets; the values will be saved
@@ -10320,7 +10838,6 @@ ix86_expand_movstr (dst, src, count_exp,
   unsigned HOST_WIDE_INT count = 0;
   rtx insns;
 
-  start_sequence ();
 
   if (GET_CODE (align_exp) == CONST_INT)
     align = INTVAL (align_exp);
@@ -10330,7 +10847,11 @@ ix86_expand_movstr (dst, src, count_exp,
     align = 64;
 
   if (GET_CODE (count_exp) == CONST_INT)
-    count = INTVAL (count_exp);
+    {
+      count = INTVAL (count_exp);
+      if (!TARGET_INLINE_ALL_STRINGOPS && count > 64)
+	return 0;
+    }
 
   /* Figure out proper mode for counter.  For 32bits it is always SImode,
      for 64bits use SImode when possible, otherwise DImode.
@@ -10341,6 +10862,8 @@ ix86_expand_movstr (dst, src, count_exp,
   else
     counter_mode = DImode;
 
+  start_sequence ();
+
   if (counter_mode != SImode && counter_mode != DImode)
     abort ();
 
@@ -10414,8 +10937,12 @@ ix86_expand_movstr (dst, src, count_exp,
 
       /* In case we don't know anything about the alignment, default to
          library version, since it is usually equally fast and result in
-         shorter code.  */
-      if (!TARGET_INLINE_ALL_STRINGOPS && align < UNITS_PER_WORD)
+         shorter code. 
+
+	 Also emit call when we know that the count is large and call overhead
+	 will not be important.  */
+      if (!TARGET_INLINE_ALL_STRINGOPS
+	  && (align < UNITS_PER_WORD || !TARGET_REP_MOVL_OPTIMAL))
 	{
 	  end_sequence ();
 	  return 0;
@@ -10433,11 +10960,11 @@ ix86_expand_movstr (dst, src, count_exp,
          able to predict the branches) and also it is friendlier to the
          hardware branch prediction.
 
-         Using loops is benefical for generic case, because we can
+         Using loops is beneficial for generic case, because we can
          handle small counts using the loops.  Many CPUs (such as Athlon)
          have large REP prefix setup costs.
 
-         This is quite costy.  Maybe we can revisit this decision later or
+         This is quite costly.  Maybe we can revisit this decision later or
          add some customizability to this code.  */
 
       if (count == 0 && align < desired_alignment)
@@ -10554,7 +11081,11 @@ ix86_expand_clrstr (src, count_exp, alig
     align = 32;
 
   if (GET_CODE (count_exp) == CONST_INT)
-    count = INTVAL (count_exp);
+    {
+      count = INTVAL (count_exp);
+      if (!TARGET_INLINE_ALL_STRINGOPS && count > 64)
+	return 0;
+    }
   /* Figure out proper mode for counter.  For 32bits it is always SImode,
      for 64bits use SImode when possible, otherwise DImode.
      Set count to number of bytes copied when known at compile time.  */
@@ -10629,8 +11160,12 @@ ix86_expand_clrstr (src, count_exp, alig
 
       /* In case we don't know anything about the alignment, default to
          library version, since it is usually equally fast and result in
-         shorter code.  */
-      if (!TARGET_INLINE_ALL_STRINGOPS && align < UNITS_PER_WORD)
+         shorter code.
+
+	 Also emit call when we know that the count is large and call overhead
+	 will not be important.  */
+      if (!TARGET_INLINE_ALL_STRINGOPS
+	  && (align < UNITS_PER_WORD || !TARGET_REP_MOVL_OPTIMAL))
 	return 0;
 
       if (TARGET_SINGLE_STRINGOP)
@@ -10833,6 +11368,7 @@ ix86_expand_strlensi_unroll_1 (out, alig
   rtx mem;
   rtx tmpreg = gen_reg_rtx (SImode);
   rtx scratch = gen_reg_rtx (SImode);
+  rtx cmp;
 
   align = 0;
   if (GET_CODE (align_rtx) == CONST_INT)
@@ -10991,10 +11527,11 @@ ix86_expand_strlensi_unroll_1 (out, alig
   /* Avoid branch in fixing the byte.  */
   tmpreg = gen_lowpart (QImode, tmpreg);
   emit_insn (gen_addqi3_cc (tmpreg, tmpreg, tmpreg));
+  cmp = gen_rtx_LTU (Pmode, gen_rtx_REG (CCmode, 17), const0_rtx);
   if (TARGET_64BIT)
-    emit_insn (gen_subdi3_carry_rex64 (out, out, GEN_INT (3)));
+    emit_insn (gen_subdi3_carry_rex64 (out, out, GEN_INT (3), cmp));
   else
-    emit_insn (gen_subsi3_carry (out, out, GEN_INT (3)));
+    emit_insn (gen_subsi3_carry (out, out, GEN_INT (3), cmp));
 
   emit_label (end_0_label);
 }
@@ -11253,6 +11790,7 @@ ix86_issue_rate ()
     case PROCESSOR_PENTIUMPRO:
     case PROCESSOR_PENTIUM4:
     case PROCESSOR_ATHLON:
+    case PROCESSOR_K8:
       return 3;
 
     default:
@@ -11357,7 +11895,7 @@ ix86_adjust_cost (insn, link, dep_insn, 
   rtx set, set2;
   int dep_insn_code_number;
 
-  /* Anti and output depenancies have zero cost on all CPUs.  */
+  /* Anti and output dependencies have zero cost on all CPUs.  */
   if (REG_NOTE_KIND (link) != 0)
     return 0;
 
@@ -11381,7 +11919,7 @@ ix86_adjust_cost (insn, link, dep_insn, 
       if (ix86_flags_dependant (insn, dep_insn, insn_type))
 	cost = 0;
 
-      /* Floating point stores require value to be ready one cycle ealier.  */
+      /* Floating point stores require value to be ready one cycle earlier.  */
       if (insn_type == TYPE_FMOV
 	  && get_attr_memory (insn) == MEMORY_STORE
 	  && !ix86_agi_dependant (insn, dep_insn, insn_type))
@@ -11464,16 +12002,10 @@ ix86_adjust_cost (insn, link, dep_insn, 
       break;
 
     case PROCESSOR_ATHLON:
+    case PROCESSOR_K8:
       memory = get_attr_memory (insn);
       dep_memory = get_attr_memory (dep_insn);
 
-      if (dep_memory == MEMORY_LOAD || dep_memory == MEMORY_BOTH)
-	{
-	  if (dep_insn_type == TYPE_IMOV || dep_insn_type == TYPE_FMOV)
-	    cost += 2;
-	  else
-	    cost += 3;
-        }
       /* Show ability of reorder buffer to hide latency of load by executing
 	 in parallel with previous instruction in case
 	 previous instruction is not needed to compute the address.  */
@@ -11747,7 +12279,7 @@ ix86_variable_issue (dump, sched_verbose
 static int
 ia32_use_dfa_pipeline_interface ()
 {
-  if (ix86_cpu == PROCESSOR_PENTIUM)
+  if (TARGET_PENTIUM || TARGET_ATHLON_K8)
     return 1;
   return 0;
 }
@@ -12996,7 +13528,8 @@ safe_vector_operand (x, mode)
 			      : gen_rtx_SUBREG (DImode, x, 0)));
   else
     emit_insn (gen_sse_clrv4sf (mode == V4SFmode ? x
-				: gen_rtx_SUBREG (V4SFmode, x, 0)));
+				: gen_rtx_SUBREG (V4SFmode, x, 0),
+				CONST0_RTX (V4SFmode)));
   return x;
 }
 
@@ -13671,7 +14204,7 @@ ix86_expand_builtin (exp, target, subtar
 
     case IX86_BUILTIN_SSE_ZERO:
       target = gen_reg_rtx (V4SFmode);
-      emit_insn (gen_sse_clrv4sf (target));
+      emit_insn (gen_sse_clrv4sf (target, CONST0_RTX (V4SFmode)));
       return target;
 
     case IX86_BUILTIN_MMX_ZERO:
@@ -13992,10 +14525,10 @@ ix86_secondary_memory_needed (class1, cl
 	return 1;
     }
   return (FLOAT_CLASS_P (class1) != FLOAT_CLASS_P (class2)
-	  || (SSE_CLASS_P (class1) != SSE_CLASS_P (class2)
-	      && (mode) != SImode)
-	  || (MMX_CLASS_P (class1) != MMX_CLASS_P (class2)
-	      && (mode) != SImode));
+	  || ((SSE_CLASS_P (class1) != SSE_CLASS_P (class2)
+	       || MMX_CLASS_P (class1) != MMX_CLASS_P (class2))
+	      && ((mode != SImode && (mode != DImode || !TARGET_64BIT))
+		  || (!TARGET_INTER_UNIT_MOVES && !optimize_size))));
 }
 /* Return the cost of moving data from a register in class CLASS1 to
    one in class CLASS2.
@@ -14023,7 +14556,7 @@ ix86_register_move_cost (mode, class1, c
       
       /* In case of copying from general_purpose_register we may emit multiple
          stores followed by single load causing memory size mismatch stall.
-         Count this as arbitarily high cost of 20.  */
+         Count this as arbitrarily high cost of 20.  */
       if (CLASS_MAX_NREGS (class1, mode) > CLASS_MAX_NREGS (class2, mode))
 	cost += 20;
 
@@ -14282,7 +14815,7 @@ x86_order_regs_for_local_alloc ()
    for (i = FIRST_REX_SSE_REG; i <= LAST_REX_SSE_REG; i++)
      reg_alloc_order [pos++] = i;
 
-   /* x87 registerts.  */
+   /* x87 registers.  */
    if (TARGET_SSE_MATH)
      for (i = FIRST_STACK_REG; i <= LAST_STACK_REG; i++)
        reg_alloc_order [pos++] = i;
@@ -14364,7 +14897,7 @@ x86_can_output_mi_thunk (thunk, delta, v
 /* Output the assembler code for a thunk function.  THUNK_DECL is the
    declaration for the thunk function itself, FUNCTION is the decl for
    the target function.  DELTA is an immediate constant offset to be
-   added to THIS.  If VCALL_OFFSET is non-zero, the word at
+   added to THIS.  If VCALL_OFFSET is nonzero, the word at
    *(*this + vcall_offset) should be added to THIS.  */
 
 static void
@@ -14509,7 +15042,7 @@ x86_field_alignment (field, computed)
 void
 x86_function_profiler (file, labelno)
      FILE *file;
-     int labelno;
+     int labelno ATTRIBUTE_UNUSED;
 {
   if (TARGET_64BIT)
     if (flag_pic)
@@ -14544,9 +15077,121 @@ x86_function_profiler (file, labelno)
     }
 }
 
+/* We don't have exact information about the insn sizes, but we may assume
+   quite safely that we are informed about all 1 byte insns and memory
+   address sizes.  This is enought to elliminate unnecesary padding in
+   99% of cases.  */
+
+static int
+min_insn_size (insn)
+     rtx insn;
+{
+  int l = 0;
+
+  if (!INSN_P (insn) || !active_insn_p (insn))
+    return 0;
+
+  /* Discard alignments we've emit and jump instructions.  */
+  if (GET_CODE (PATTERN (insn)) == UNSPEC_VOLATILE
+      && XINT (PATTERN (insn), 1) == UNSPECV_ALIGN)
+    return 0;
+  if (GET_CODE (insn) == JUMP_INSN
+      && (GET_CODE (PATTERN (insn)) == ADDR_VEC
+	  || GET_CODE (PATTERN (insn)) == ADDR_DIFF_VEC))
+    return 0;
+
+  /* Important case - calls are always 5 bytes.
+     It is common to have many calls in the row.  */
+  if (GET_CODE (insn) == CALL_INSN
+      && symbolic_reference_mentioned_p (PATTERN (insn))
+      && !SIBLING_CALL_P (insn))
+    return 5;
+  if (get_attr_length (insn) <= 1)
+    return 1;
+
+  /* For normal instructions we may rely on the sizes of addresses
+     and the presence of symbol to require 4 bytes of encoding.
+     This is not the case for jumps where references are PC relative.  */
+  if (GET_CODE (insn) != JUMP_INSN)
+    {
+      l = get_attr_length_address (insn);
+      if (l < 4 && symbolic_reference_mentioned_p (PATTERN (insn)))
+	l = 4;
+    }
+  if (l)
+    return 1+l;
+  else
+    return 2;
+}
+
+/* AMD K8 core misspredicts jumps when there are more than 3 jumps in 16 byte
+   window.  */
+
+static void
+k8_avoid_jump_misspredicts (first)
+     rtx first;
+{
+  rtx insn, start = first;
+  int nbytes = 0, njumps = 0;
+  int isjump = 0;
+
+  /* Look for all minimal intervals of instructions containing 4 jumps.
+     The intervals are bounded by START and INSN.  NBYTES is the total
+     size of instructions in the interval including INSN and not including
+     START.  When the NBYTES is smaller than 16 bytes, it is possible
+     that the end of START and INSN ends up in the same 16byte page.
+
+     The smallest offset in the page INSN can start is the case where START
+     ends on the offset 0.  Offset of INSN is then NBYTES - sizeof (INSN).
+     We add p2align to 16byte window with maxskip 17 - NBYTES + sizeof (INSN).
+     */
+  for (insn = first; insn; insn = NEXT_INSN (insn))
+    {
+
+      nbytes += min_insn_size (insn);
+      if (rtl_dump_file)
+        fprintf(stderr,"Insn %i estimated to %i bytes\n",
+		INSN_UID (insn), min_insn_size (insn));
+      if ((GET_CODE (insn) == JUMP_INSN
+	   && GET_CODE (PATTERN (insn)) != ADDR_VEC
+	   && GET_CODE (PATTERN (insn)) != ADDR_DIFF_VEC)
+	  || GET_CODE (insn) == CALL_INSN)
+	njumps++;
+      else
+	continue;
+
+      while (njumps > 3)
+	{
+	  start = NEXT_INSN (start);
+	  if ((GET_CODE (start) == JUMP_INSN
+	       && GET_CODE (PATTERN (start)) != ADDR_VEC
+	       && GET_CODE (PATTERN (start)) != ADDR_DIFF_VEC)
+	      || GET_CODE (start) == CALL_INSN)
+	    njumps--, isjump = 1;
+	  else
+	    isjump = 0;
+	  nbytes -= min_insn_size (start);
+	}
+      if (njumps < 0)
+	abort ();
+      if (rtl_dump_file)
+        fprintf(stderr,"Interval %i to %i has %i bytes\n",
+		INSN_UID (start), INSN_UID (insn), nbytes);
+
+      if (njumps == 3 && isjump && nbytes < 16)
+	{
+	  int padsize = 15 - nbytes + min_insn_size (insn);
+
+	  if (rtl_dump_file)
+	    fprintf (rtl_dump_file, "Padding insn %i by %i bytes!\n", INSN_UID (insn), padsize);
+          emit_insn_before (gen_align (GEN_INT (padsize)), insn);
+	}
+    }
+}
+
 /* Implement machine specific optimizations.  
    At the moment we implement single transformation: AMD Athlon works faster
-   when RET is not destination of conditional jump or directly preceeded
+   when RET is not destination of conditional jump or directly preceded
    by other jump instruction.  We avoid the penalty by inserting NOP just
    before the RET instructions in such cases.  */
 void
@@ -14555,36 +15200,143 @@ x86_machine_dependent_reorg (first)
 {
   edge e;
 
-  if (!TARGET_ATHLON || !optimize || optimize_size)
+  if (!TARGET_ATHLON_K8 || !optimize || optimize_size)
     return;
   for (e = EXIT_BLOCK_PTR->pred; e; e = e->pred_next)
   {
     basic_block bb = e->src;
     rtx ret = bb->end;
     rtx prev;
-    bool insert = false;
+    bool replace = false;
 
-    if (!returnjump_p (ret) || !maybe_hot_bb_p (bb))
+    if (GET_CODE (ret) != JUMP_INSN || GET_CODE (PATTERN (ret)) != RETURN
+	|| !maybe_hot_bb_p (bb))
       continue;
-    prev = prev_nonnote_insn (ret);
+    for (prev = PREV_INSN (ret); prev; prev = PREV_INSN (prev))
+      if (active_insn_p (prev) || GET_CODE (prev) == CODE_LABEL)
+	break;
     if (prev && GET_CODE (prev) == CODE_LABEL)
       {
 	edge e;
 	for (e = bb->pred; e; e = e->pred_next)
-	  if (EDGE_FREQUENCY (e) && e->src->index > 0
+	  if (EDGE_FREQUENCY (e) && e->src->index >= 0
 	      && !(e->flags & EDGE_FALLTHRU))
-	    insert = 1;
+	    replace = true;
       }
-    if (!insert)
+    if (!replace)
       {
-	prev = prev_real_insn (ret);
+	prev = prev_active_insn (ret);
 	if (prev && GET_CODE (prev) == JUMP_INSN
 	    && any_condjump_p (prev))
-	  insert = 1;
+	  replace = true;
+	/* Empty functions get branch misspredict even when the jump destination
+	   is not visible to us.  */
+	if (!prev && cfun->function_frequency > FUNCTION_FREQUENCY_UNLIKELY_EXECUTED)
+	  replace = true;
+      }
+    if (replace)
+      {
+        emit_insn_before (gen_return_internal_long (), ret);
+	delete_insn (ret);
       }
-    if (insert)
-      emit_insn_before (gen_nop (), ret);
   }
+  k8_avoid_jump_misspredicts (first);
+}
+
+/* Return nonzero when QImode register that must be represented via REX prefix
+   is used.  */
+bool
+x86_extended_QIreg_mentioned_p (insn)
+     rtx insn;
+{
+  int i;
+  extract_insn_cached (insn);
+  for (i = 0; i < recog_data.n_operands; i++)
+    if (REG_P (recog_data.operand[i])
+	&& REGNO (recog_data.operand[i]) >= 4)
+       return true;
+  return false;
+}
+
+/* Return nonzero when P points to register encoded via REX prefix.
+   Called via for_each_rtx.  */
+static int
+extended_reg_mentioned_1 (p, data)
+	rtx *p;
+	void *data ATTRIBUTE_UNUSED;
+{
+   unsigned int regno;
+   if (!REG_P (*p))
+     return 0;
+   regno = REGNO (*p);
+   return REX_INT_REGNO_P (regno) || REX_SSE_REGNO_P (regno);
+}
+
+/* Return true when INSN mentions register that must be encoded using REX
+   prefix.  */
+bool
+x86_extended_reg_mentioned_p (insn)
+     rtx insn;
+{
+  return for_each_rtx (&PATTERN (insn), extended_reg_mentioned_1, NULL);
+}
+
+static void
+x86_optimize_local_function (decl)
+     tree decl;
+{
+  if (!TARGET_64BIT
+      && !lookup_attribute ("regparm", TYPE_ATTRIBUTES (TREE_TYPE (decl)))
+      && !lookup_attribute ("stdcall", TYPE_ATTRIBUTES (TREE_TYPE (decl)))
+      && !lookup_attribute ("fastcall", TYPE_ATTRIBUTES (TREE_TYPE (decl)))
+      /* We can't use regparm(3) for nested functions as these use
+         static chain pointer in third argument.  */
+      && !(DECL_CONTEXT (decl) && !DECL_NO_STATIC_CHAIN (decl)))
+    {
+      tree type = copy_node (TREE_TYPE (decl));
+      TYPE_ATTRIBUTES (type) = copy_list (TYPE_ATTRIBUTES (type));
+      TYPE_ATTRIBUTES (type) =
+	chainon (TYPE_ATTRIBUTES (type),
+		 build_tree_list (get_identifier ("regparm"),
+				  build_tree_list (NULL_TREE,
+						   build_int_2 (3, 0))));
+      TREE_TYPE (decl) = type;
+    }
+}
+
+/* Generate an unsigned DImode to FP conversion.  This is the same code
+   optabs would emit if we didn't have TFmode patterns.  */
+
+void
+x86_emit_floatuns (operands)
+     rtx operands[2];
+{
+  rtx neglab, donelab, i0, i1, f0, in, out;
+  enum machine_mode mode;
+
+  out = operands[0];
+  in = force_reg (DImode, operands[1]);
+  mode = GET_MODE (out);
+  neglab = gen_label_rtx ();
+  donelab = gen_label_rtx ();
+  i1 = gen_reg_rtx (Pmode);
+  f0 = gen_reg_rtx (mode);
+
+  emit_cmp_and_jump_insns (in, const0_rtx, LT, const0_rtx, Pmode, 0, neglab);
+
+  emit_insn (gen_rtx_SET (VOIDmode, out, gen_rtx_FLOAT (mode, in)));
+  emit_jump_insn (gen_jump (donelab));
+  emit_barrier ();
+
+  emit_label (neglab);
+
+  i0 = expand_simple_binop (Pmode, LSHIFTRT, in, const1_rtx, NULL, 1, OPTAB_DIRECT);
+  i1 = expand_simple_binop (Pmode, AND, in, const1_rtx, NULL, 1, OPTAB_DIRECT);
+  i0 = expand_simple_binop (Pmode, IOR, i0, i1, i0, 1, OPTAB_DIRECT);
+  expand_float (f0, i0, 0);
+  emit_insn (gen_rtx_SET (VOIDmode, out, gen_rtx_PLUS (mode, f0, f0)));
+
+  emit_label (donelab);
 }
 
 #include "gt-i386.h"
--- gcc-3.3.1/gcc/config/i386/i386.h.hammer-branch	2003-06-25 23:18:31.000000000 +0200
+++ gcc-3.3.1/gcc/config/i386/i386.h	2003-08-05 18:22:46.000000000 +0200
@@ -1,6 +1,6 @@
 /* Definitions of target machine for GNU compiler for IA-32.
    Copyright (C) 1988, 1992, 1994, 1995, 1996, 1997, 1998, 1999, 2000,
-   2001, 2002 Free Software Foundation, Inc.
+   2001, 2002, 2003 Free Software Foundation, Inc.
 
 This file is part of GNU CC.
 
@@ -41,9 +41,11 @@ struct processor_costs {
   const int lea;		/* cost of a lea instruction */
   const int shift_var;		/* variable shift costs */
   const int shift_const;	/* constant shift costs */
-  const int mult_init;		/* cost of starting a multiply */
+  const int mult_init[5];	/* cost of starting a multiply 
+				   in QImode, HImode, SImode, DImode, TImode*/
   const int mult_bit;		/* cost of multiply per each bit set */
-  const int divide;		/* cost of a divide/mod */
+  const int divide[5];		/* cost of a divide/mod 
+				   in QImode, HImode, SImode, DImode, TImode*/
   int movsx;			/* The cost of movsx operation.  */
   int movzx;			/* The cost of movzx operation.  */
   const int large_insn;		/* insns larger than this cost more */
@@ -75,6 +77,7 @@ struct processor_costs {
   const int prefetch_block;	/* bytes moved to cache for prefetch.  */
   const int simultaneous_prefetches; /* number of parallel prefetch
 				   operations.  */
+  const int branch_cost;	/* Default value for BRANCH_COST.  */
   const int fadd;		/* cost of FADD and FSUB instructions.  */
   const int fmul;		/* cost of FMUL instruction.  */
   const int fdiv;		/* cost of FDIV instruction.  */
@@ -119,7 +122,7 @@ extern int target_flags;
 #define MASK_128BIT_LONG_DOUBLE 0x00040000	/* long double size is 128bit */
 #define MASK_64BIT		0x00080000	/* Produce 64bit code */
 
-/* Unused:			0x03f0000	*/
+/* Unused:			0x03e0000	*/
 
 /* ... overlap with subtarget options starts by 0x04000000.  */
 #define MASK_NO_RED_ZONE	0x04000000	/* Do not use red zone */
@@ -204,6 +207,8 @@ extern int target_flags;
 #define TARGET_K6 (ix86_cpu == PROCESSOR_K6)
 #define TARGET_ATHLON (ix86_cpu == PROCESSOR_ATHLON)
 #define TARGET_PENTIUM4 (ix86_cpu == PROCESSOR_PENTIUM4)
+#define TARGET_K8 (ix86_cpu == PROCESSOR_K8)
+#define TARGET_ATHLON_K8 (TARGET_K8 || TARGET_ATHLON)
 
 #define CPUMASK (1 << ix86_cpu)
 extern const int x86_use_leave, x86_push_memory, x86_zero_extend_with_and;
@@ -221,6 +226,10 @@ extern const int x86_partial_reg_depende
 extern const int x86_accumulate_outgoing_args, x86_prologue_using_move;
 extern const int x86_epilogue_using_move, x86_decompose_lea;
 extern const int x86_arch_always_fancy_math_387, x86_shift1;
+extern const int x86_sse_partial_reg_dependency, x86_sse_partial_regs;
+extern const int x86_sse_typeless_stores, x86_sse_load0_by_pxor;
+extern const int x86_use_ffreep, x86_sse_partial_regs_for_cvtsd2ss;
+extern const int x86_inter_unit_moves;
 extern int x86_prefetch_sse;
 
 #define TARGET_USE_LEAVE (x86_use_leave & CPUMASK)
@@ -257,12 +266,23 @@ extern int x86_prefetch_sse;
 #define TARGET_SUB_ESP_8 (x86_sub_esp_8 & CPUMASK)
 #define TARGET_INTEGER_DFMODE_MOVES (x86_integer_DFmode_moves & CPUMASK)
 #define TARGET_PARTIAL_REG_DEPENDENCY (x86_partial_reg_dependency & CPUMASK)
+#define TARGET_SSE_PARTIAL_REG_DEPENDENCY \
+				      (x86_sse_partial_reg_dependency & CPUMASK)
+#define TARGET_SSE_PARTIAL_REGS (x86_sse_partial_regs & CPUMASK)
+#define TARGET_SSE_PARTIAL_REGS_FOR_CVTSD2SS \
+				(x86_sse_partial_regs_for_cvtsd2ss & CPUMASK)
+#define TARGET_SSE_TYPELESS_STORES (x86_sse_typeless_stores & CPUMASK)
+#define TARGET_SSE_TYPELESS_LOAD0 (x86_sse_typeless_load0 & CPUMASK)
+#define TARGET_SSE_LOAD0_BY_PXOR (x86_sse_load0_by_pxor & CPUMASK)
 #define TARGET_MEMORY_MISMATCH_STALL (x86_memory_mismatch_stall & CPUMASK)
 #define TARGET_PROLOGUE_USING_MOVE (x86_prologue_using_move & CPUMASK)
 #define TARGET_EPILOGUE_USING_MOVE (x86_epilogue_using_move & CPUMASK)
 #define TARGET_DECOMPOSE_LEA (x86_decompose_lea & CPUMASK)
 #define TARGET_PREFETCH_SSE (x86_prefetch_sse)
 #define TARGET_SHIFT1 (x86_shift1 & CPUMASK)
+#define TARGET_USE_FFREEP (x86_use_ffreep & CPUMASK)
+#define TARGET_REP_MOVL_OPTIMAL (x86_rep_movl_optimal & CPUMASK)
+#define TARGET_INTER_UNIT_MOVES (x86_inter_unit_moves & CPUMASK)
 
 #define TARGET_STACK_PROBE (target_flags & MASK_STACK_PROBE)
 
@@ -541,6 +561,8 @@ extern int x86_prefetch_sse;
 	  if (last_cpu_char != 'n')				\
 	    builtin_define ("__tune_athlon_sse__");		\
 	}							\
+      else if (TARGET_K8)					\
+	builtin_define ("__tune_k8__");				\
       else if (TARGET_PENTIUM4)					\
 	builtin_define ("__tune_pentium4__");			\
 								\
@@ -599,6 +621,11 @@ extern int x86_prefetch_sse;
 	  if (last_arch_char != 'n')				\
 	    builtin_define ("__athlon_sse__");			\
 	}							\
+      else if (ix86_arch == PROCESSOR_K8)			\
+	{							\
+	  builtin_define ("__k8");				\
+	  builtin_define ("__k8__");				\
+	}							\
       else if (ix86_arch == PROCESSOR_PENTIUM4)			\
 	{							\
 	  builtin_define ("__pentium4");			\
@@ -620,11 +647,12 @@ extern int x86_prefetch_sse;
 #define TARGET_CPU_DEFAULT_k6_3 10
 #define TARGET_CPU_DEFAULT_athlon 11
 #define TARGET_CPU_DEFAULT_athlon_sse 12
+#define TARGET_CPU_DEFAULT_k8 13
 
 #define TARGET_CPU_DEFAULT_NAMES {"i386", "i486", "pentium", "pentium-mmx",\
 				  "pentiumpro", "pentium2", "pentium3", \
 				  "pentium4", "k6", "k6-2", "k6-3",\
-				  "athlon", "athlon-4"}
+				  "athlon", "athlon-4", "k8"}
 
 #ifndef CC1_SPEC
 #define CC1_SPEC "%(cc1_cpu) "
@@ -713,12 +741,12 @@ extern int x86_prefetch_sse;
 /* Boundary (in *bits*) on which stack pointer should be aligned.  */
 #define STACK_BOUNDARY BITS_PER_WORD
 
-/* Boundary (in *bits*) on which the stack pointer preferrs to be
+/* Boundary (in *bits*) on which the stack pointer prefers to be
    aligned; the compiler cannot rely on having this alignment.  */
 #define PREFERRED_STACK_BOUNDARY ix86_preferred_stack_boundary
 
 /* As of July 2001, many runtimes to not align the stack properly when
-   entering main.  This causes expand_main_function to forcably align
+   entering main.  This causes expand_main_function to forcibly align
    the stack, which results in aligned frames for functions called from
    main, though it does nothing for the alignment of main itself.  */
 #define FORCE_PREFERRED_STACK_BOUNDARY_IN_MAIN \
@@ -739,7 +767,7 @@ extern int x86_prefetch_sse;
    might need to be aligned. No data type wants to be aligned
    rounder than this.
 
-   Pentium+ preferrs DFmode values to be aligned to 64 bit boundary
+   Pentium+ prefers DFmode values to be aligned to 64 bit boundary
    and Pentium Pro XFmode values at 128 bit boundaries.  */
 
 #define BIGGEST_ALIGNMENT 128
@@ -749,7 +777,7 @@ extern int x86_prefetch_sse;
  ((MODE) == XFmode || (MODE) == TFmode || SSE_REG_MODE_P (MODE))
 
 /* The published ABIs say that doubles should be aligned on word
-   boundaries, so lower the aligment for structure fields unless
+   boundaries, so lower the alignment for structure fields unless
    -malign-double is set.  */
 
 /* ??? Blah -- this macro is used directly by libobjc.  Since it
@@ -859,7 +887,7 @@ extern int x86_prefetch_sse;
    and are not available for the register allocator.
    On the 80386, the stack pointer is such, as is the arg pointer.
 
-   The value is an mask - bit 1 is set for fixed registers
+   The value is a mask - bit 1 is set for fixed registers
    for 32bit target, while 2 is set for fixed registers for 64bit.
    Proper value is computed in the CONDITIONAL_REGISTER_USAGE.
  */
@@ -885,7 +913,7 @@ extern int x86_prefetch_sse;
    and the register where structure-value addresses are passed.
    Aside from that, you can include as many other registers as you like.
 
-   The value is an mask - bit 1 is set for call used
+   The value is a mask - bit 1 is set for call used
    for 32bit target, while 2 is set for call used for 64bit.
    Proper value is computed in the CONDITIONAL_REGISTER_USAGE.
 */
@@ -920,7 +948,7 @@ extern int x86_prefetch_sse;
 
 /* ORDER_REGS_FOR_LOCAL_ALLOC is a macro which permits reg_alloc_order
    to be rearranged based on a particular function.  When using sse math,
-   we want to allocase SSE before x87 registers and vice vera.  */
+   we want to allocate SSE before x87 registers and vice vera.  */
 
 #define ORDER_REGS_FOR_LOCAL_ALLOC x86_order_regs_for_local_alloc ()
 
@@ -1336,6 +1364,9 @@ enum reg_class
   (((N) >= FIRST_SSE_REG && (N) <= LAST_SSE_REG) \
    || ((N) >= FIRST_REX_SSE_REG && (N) <= LAST_REX_SSE_REG))
 
+#define REX_SSE_REGNO_P(N) \
+   ((N) >= FIRST_REX_SSE_REG && (N) <= LAST_REX_SSE_REG)
+
 #define SSE_REGNO(N) \
   ((N) < 8 ? FIRST_SSE_REG + (N) : FIRST_REX_SSE_REG + (N) - 8)
 #define SSE_REG_P(N) (REG_P (N) && SSE_REGNO_P (REGNO (N)))
@@ -1406,7 +1437,7 @@ enum reg_class
    K is for signed imm8 operands.
    L is for andsi as zero-extending move.
    M is for shifts that can be executed by the "lea" opcode.
-   N is for immedaite operands for out/in instructions (0-255)
+   N is for immediate operands for out/in instructions (0-255)
    */
 
 #define CONST_OK_FOR_LETTER_P(VALUE, C)				\
@@ -2613,7 +2644,8 @@ do {							\
       return 3;							\
     if (TARGET_64BIT && !x86_64_zero_extended_value (RTX))	\
       return 2;							\
-    return flag_pic && SYMBOLIC_CONST (RTX) ? 1 : 0;		\
+    return flag_pic && SYMBOLIC_CONST (RTX)			\
+	   && (!TARGET_64BIT || (GET_CODE (RTX) != LABEL_REF && (GET_CODE (RTX) != SYMBOL_REF || !SYMBOL_REF_FLAG (RTX)))) ? 1 : 0;\
 								\
   case CONST_DOUBLE:						\
     if (GET_MODE (RTX) == VOIDmode)				\
@@ -2627,7 +2659,7 @@ do {							\
       default:							\
 	/* Start with (MEM (SYMBOL_REF)), since that's where	\
 	   it'll probably end up.  Add a penalty for size.  */	\
-	return (COSTS_N_INSNS (1) + (flag_pic != 0)		\
+	return (COSTS_N_INSNS (1) + (flag_pic != 0 && !TARGET_64BIT) \
 		+ (GET_MODE (RTX) == SFmode ? 0			\
 		   : GET_MODE (RTX) == DFmode ? 1 : 2));	\
       }
@@ -2636,6 +2668,14 @@ do {							\
 #define TOPLEVEL_COSTS_N_INSNS(N) \
   do { total = COSTS_N_INSNS (N); goto egress_rtx_costs; } while (0)
 
+/* Return index of given mode in mult and division cost tables.  */
+#define MODE_INDEX(mode)					\
+  ((mode) == QImode ? 0						\
+   : (mode) == HImode ? 1					\
+   : (mode) == SImode ? 2					\
+   : (mode) == DImode ? 3					\
+   : 4)
+
 /* Like `CONST_COSTS' but applies to nonconstant RTL expressions.
    This can be used, for example, to indicate how costly a multiply
    instruction is.  In writing this macro, you can use the construct
@@ -2721,10 +2761,12 @@ do {							\
 	  } 								\
 									\
 	TOPLEVEL_COSTS_N_INSNS (ix86_cost->mult_init			\
+				[MODE_INDEX (GET_MODE (X))]		\
 			        + nbits * ix86_cost->mult_bit);		\
       }									\
     else			/* This is arbitrary */			\
       TOPLEVEL_COSTS_N_INSNS (ix86_cost->mult_init			\
+			      [MODE_INDEX (GET_MODE (X))]		\
 			      + 7 * ix86_cost->mult_bit);		\
 									\
   case DIV:								\
@@ -2734,7 +2776,8 @@ do {							\
     if (FLOAT_MODE_P (GET_MODE (X)))					\
       TOPLEVEL_COSTS_N_INSNS (ix86_cost->fdiv);				\
     else								\
-      TOPLEVEL_COSTS_N_INSNS (ix86_cost->divide);			\
+      TOPLEVEL_COSTS_N_INSNS (ix86_cost->divide				\
+			      [MODE_INDEX (GET_MODE (X))]);		\
     break;								\
 									\
   case PLUS:								\
@@ -3275,6 +3318,7 @@ do {						\
 		       LABEL_REF, SUBREG, REG, MEM}},			\
   {"pic_symbolic_operand", {CONST}},					\
   {"call_insn_operand", {REG, SUBREG, MEM, SYMBOL_REF}},		\
+  {"sibcall_insn_operand", {REG, SUBREG, SYMBOL_REF}},			\
   {"constant_call_address_operand", {SYMBOL_REF, CONST}},		\
   {"const0_operand", {CONST_INT, CONST_DOUBLE}},			\
   {"const1_operand", {CONST_INT}},					\
@@ -3286,6 +3330,7 @@ do {						\
 			SYMBOL_REF, LABEL_REF, SUBREG, REG, MEM}},	\
   {"nonmemory_no_elim_operand", {CONST_INT, REG, SUBREG}},		\
   {"index_register_operand", {SUBREG, REG}},				\
+  {"flags_reg_operand", {REG}},						\
   {"q_regs_operand", {SUBREG, REG}},					\
   {"non_q_regs_operand", {SUBREG, REG}},				\
   {"fcmov_comparison_operator", {EQ, NE, LTU, GTU, LEU, GEU, UNORDERED, \
@@ -3297,6 +3342,8 @@ do {						\
   {"ix86_comparison_operator", {EQ, NE, LE, LT, GE, GT, LEU, LTU, GEU,	\
 			       GTU, UNORDERED, ORDERED, UNLE, UNLT,	\
 			       UNGE, UNGT, LTGT, UNEQ }},		\
+  {"ix86_carry_flag_operator", {LTU, LT, UNLT, GT, UNGT, LE, UNLE,	\
+				 GE, UNGE, LTGT, UNEQ}},		\
   {"cmp_fp_expander_operand", {CONST_DOUBLE, SUBREG, REG, MEM}},	\
   {"ext_register_operand", {SUBREG, REG}},				\
   {"binary_fp_operator", {PLUS, MINUS, MULT, DIV}},			\
@@ -3320,6 +3367,7 @@ do {						\
   {"register_and_not_any_fp_reg_operand", {REG}},			\
   {"fp_register_operand", {REG}},					\
   {"register_and_not_fp_reg_operand", {REG}},				\
+  {"zero_extended_scalar_load_operand", {MEM}},				\
   {"vector_move_operand", {CONST_VECTOR, SUBREG, REG, MEM}},		\
 
 /* A list of predicates that do special things with modes, and so
@@ -3340,6 +3388,7 @@ enum processor_type
   PROCESSOR_K6,
   PROCESSOR_ATHLON,
   PROCESSOR_PENTIUM4,
+  PROCESSOR_K8,
   PROCESSOR_max
 };
 
--- gcc-3.3.1/gcc/config/i386/i386.md.hammer-branch	2003-08-05 18:22:45.000000000 +0200
+++ gcc-3.3.1/gcc/config/i386/i386.md	2003-08-05 18:22:46.000000000 +0200
@@ -110,6 +110,7 @@
    (UNSPEC_MFENCE		59)
    (UNSPEC_LFENCE		60)
    (UNSPEC_PSADBW		61)
+   (UNSPEC_REP			66)
   ])
 
 (define_constants
@@ -120,6 +121,7 @@
    (UNSPECV_STMXCSR		40)
    (UNSPECV_FEMMS		46)
    (UNSPECV_CLFLUSH		57)
+   (UNSPECV_ALIGN		66)
   ])
 
 ;; Insns whose names begin with "x86_" are emitted by gen_FOO calls
@@ -133,7 +135,7 @@
 
 ;; Processor type.  This attribute must exactly match the processor_type
 ;; enumeration in i386.h.
-(define_attr "cpu" "i386,i486,pentium,pentiumpro,k6,athlon,pentium4"
+(define_attr "cpu" "i386,i486,pentium,pentiumpro,k6,athlon,pentium4,k8"
   (const (symbol_ref "ix86_cpu")))
 
 ;; A basic instruction type.  Refinements due to arguments to be
@@ -143,17 +145,17 @@
    alu,alu1,negnot,imov,imovx,lea,
    incdec,ishift,ishift1,rotate,rotate1,imul,idiv,
    icmp,test,ibr,setcc,icmov,
-   push,pop,call,callv,
+   push,pop,call,callv,leave,
    str,cld,
    fmov,fop,fsgn,fmul,fdiv,fpspc,fcmov,fcmp,fxch,fistp,
    sselog,sseiadd,sseishft,sseimul,
-   sse,ssemov,sseadd,ssemul,ssecmp,ssecvt,ssediv,
+   sse,ssemov,sseadd,ssemul,ssecmp,ssecomi,ssecvt,sseicvt,ssediv,
    mmx,mmxmov,mmxadd,mmxmul,mmxcmp,mmxcvt,mmxshft"
   (const_string "other"))
 
 ;; Main data type used by the insn
 (define_attr "mode"
-  "unknown,none,QI,HI,SI,DI,unknownfp,SF,DF,XF,TI,V4SF,V2DF,V2SF"
+  "unknown,none,QI,HI,SI,DI,SF,DF,XF,TI,V4SF,V2DF,V2SF"
   (const_string "unknown"))
 
 ;; The CPU unit operations uses.
@@ -161,7 +163,7 @@
   (cond [(eq_attr "type" "fmov,fop,fsgn,fmul,fdiv,fpspc,fcmov,fcmp,fxch,fistp")
 	   (const_string "i387")
 	 (eq_attr "type" "sselog,sseiadd,sseishft,sseimul,
-			  sse,ssemov,sseadd,ssemul,ssecmp,ssecvt,ssediv")
+			  sse,ssemov,sseadd,ssemul,ssecmp,ssecomi,ssecvt,sseicvt,ssediv")
 	   (const_string "sse")
 	 (eq_attr "type" "mmx,mmxmov,mmxadd,mmxmul,mmxcmp,mmxcvt,mmxshft")
 	   (const_string "mmx")
@@ -171,7 +173,7 @@
 
 ;; The (bounding maximum) length of an instruction immediate.
 (define_attr "length_immediate" ""
-  (cond [(eq_attr "type" "incdec,setcc,icmov,str,cld,lea,other,multi,idiv")
+  (cond [(eq_attr "type" "incdec,setcc,icmov,str,cld,lea,other,multi,idiv,leave")
 	   (const_int 0)
 	 (eq_attr "unit" "i387,sse,mmx")
 	   (const_int 0)
@@ -225,17 +227,29 @@
 ;; Set when 0f opcode prefix is used.
 (define_attr "prefix_0f" ""
   (if_then_else 
-    (eq_attr "type" 
-             "imovx,setcc,icmov,
-              sselog,sseiadd,sseishft,sseimul,
-              sse,ssemov,sseadd,ssemul,ssecmp,ssecvt,ssediv,
-              mmx,mmxmov,mmxadd,mmxmul,mmxcmp,mmxcvt,mmxshft")
+    (ior (eq_attr "type" "imovx,setcc,icmov")
+	 (eq_attr "unit" "sse,mmx"))
     (const_int 1)
     (const_int 0)))
 
+;; Set when 0f opcode prefix is used.
+(define_attr "prefix_rex" ""
+  (cond [(and (eq_attr "mode" "DI")
+  	      (eq_attr "type" "!push,pop,call,callv,leave,ibr"))
+	   (const_int 1)
+	 (and (eq_attr "mode" "QI")
+	      (ne (symbol_ref "x86_extended_QIreg_mentioned_p (insn)")
+		  (const_int 0)))
+	   (const_int 1)
+	 (ne (symbol_ref "x86_extended_reg_mentioned_p (insn)")
+	     (const_int 0))
+	   (const_int 1)
+	]
+	(const_int 0)))
+
 ;; Set when modrm byte is used.
 (define_attr "modrm" ""
-  (cond [(eq_attr "type" "str,cld")
+  (cond [(eq_attr "type" "str,cld,leave")
 	   (const_int 0)
 	 (eq_attr "unit" "i387")
 	   (const_int 0)
@@ -276,7 +290,8 @@
 		       (attr "length_address")))]
 	 (plus (plus (attr "modrm")
 		     (plus (attr "prefix_0f")
-			   (const_int 1)))
+			   (plus (attr "prefix_rex")
+				 (const_int 1))))
 	       (plus (attr "prefix_rep")
 		     (plus (attr "prefix_data16")
 			   (plus (attr "length_immediate")
@@ -291,17 +306,21 @@
 	   (const_string "unknown")
 	 (eq_attr "type" "lea,fcmov,fpspc,cld")
 	   (const_string "none")
-	 (eq_attr "type" "fistp")
+	 (eq_attr "type" "fistp,leave")
 	   (const_string "both")
 	 (eq_attr "type" "push")
 	   (if_then_else (match_operand 1 "memory_operand" "")
 	     (const_string "both")
 	     (const_string "store"))
-	 (eq_attr "type" "pop,setcc")
+	 (eq_attr "type" "pop")
 	   (if_then_else (match_operand 0 "memory_operand" "")
 	     (const_string "both")
 	     (const_string "load"))
-	 (eq_attr "type" "icmp,test,ssecmp,mmxcmp,fcmp")
+	 (eq_attr "type" "setcc")
+	   (if_then_else (match_operand 0 "memory_operand" "")
+	     (const_string "store")
+	     (const_string "none"))
+	 (eq_attr "type" "icmp,test,ssecmp,ssecomi,mmxcmp,fcmp")
 	   (if_then_else (ior (match_operand 0 "memory_operand" "")
 			      (match_operand 1 "memory_operand" ""))
 	     (const_string "load")
@@ -332,7 +351,7 @@
 		 "!alu1,negnot,
 		   imov,imovx,icmp,test,
 		   fmov,fcmp,fsgn,
-		   sse,ssemov,ssecmp,ssecvt,
+		   sse,ssemov,ssecmp,ssecomi,ssecvt,sseicvt,
 		   mmx,mmxmov,mmxcmp,mmxcvt")
 	      (match_operand 2 "memory_operand" ""))
 	   (const_string "load")
@@ -757,7 +776,13 @@
     return "ftst\;fnstsw\t%0";
 }
   [(set_attr "type" "multi")
-   (set_attr "mode" "unknownfp")])
+   (set (attr "mode")
+     (cond [(match_operand:SF 1 "" "")
+	      (const_string "SF")
+	    (match_operand:DF 1 "" "")
+	      (const_string "DF")
+	   ]
+	   (const_string "XF")))])
 
 ;; We may not use "#" to split and emit these, since the REG_DEAD notes
 ;; used to manage the reg stack popping would not be preserved.
@@ -860,7 +885,13 @@
    && GET_MODE (operands[0]) == GET_MODE (operands[1])"
   "* return output_fp_compare (insn, operands, 0, 1);"
   [(set_attr "type" "fcmp")
-   (set_attr "mode" "unknownfp")])
+   (set (attr "mode")
+     (cond [(match_operand:SF 1 "" "")
+	      (const_string "SF")
+	    (match_operand:DF 1 "" "")
+	      (const_string "DF")
+	   ]
+	   (const_string "XF")))])
 
 (define_insn "*cmpfp_2u_1"
   [(set (match_operand:HI 0 "register_operand" "=a")
@@ -874,7 +905,13 @@
    && GET_MODE (operands[1]) == GET_MODE (operands[2])"
   "* return output_fp_compare (insn, operands, 2, 1);"
   [(set_attr "type" "multi")
-   (set_attr "mode" "unknownfp")])
+   (set (attr "mode")
+     (cond [(match_operand:SF 1 "" "")
+	      (const_string "SF")
+	    (match_operand:DF 1 "" "")
+	      (const_string "DF")
+	   ]
+	   (const_string "XF")))])
 
 ;; Patterns to match the SImode-in-memory ficom instructions.
 ;;
@@ -914,7 +951,7 @@
 ;; FP compares, step 2
 ;; Move the fpsw to ax.
 
-(define_insn "x86_fnstsw_1"
+(define_insn "*x86_fnstsw_1"
   [(set (match_operand:HI 0 "register_operand" "=a")
 	(unspec:HI [(reg 18)] UNSPEC_FNSTSW))]
   "TARGET_80387"
@@ -949,7 +986,13 @@
    && GET_MODE (operands[0]) == GET_MODE (operands[0])"
   "* return output_fp_compare (insn, operands, 1, 0);"
   [(set_attr "type" "fcmp")
-   (set_attr "mode" "unknownfp")
+   (set (attr "mode")
+     (cond [(match_operand:SF 1 "" "")
+	      (const_string "SF")
+	    (match_operand:DF 1 "" "")
+	      (const_string "DF")
+	   ]
+	   (const_string "XF")))
    (set_attr "athlon_decode" "vector")])
 
 (define_insn "*cmpfp_i_sse"
@@ -960,8 +1003,11 @@
    && SSE_FLOAT_MODE_P (GET_MODE (operands[0]))
    && GET_MODE (operands[0]) == GET_MODE (operands[0])"
   "* return output_fp_compare (insn, operands, 1, 0);"
-  [(set_attr "type" "fcmp,ssecmp")
-   (set_attr "mode" "unknownfp")
+  [(set_attr "type" "fcmp,ssecomi")
+   (set (attr "mode")
+     (if_then_else (match_operand:SF 1 "" "")
+        (const_string "SF")
+        (const_string "DF")))
    (set_attr "athlon_decode" "vector")])
 
 (define_insn "*cmpfp_i_sse_only"
@@ -971,8 +1017,11 @@
   "SSE_FLOAT_MODE_P (GET_MODE (operands[0]))
    && GET_MODE (operands[0]) == GET_MODE (operands[0])"
   "* return output_fp_compare (insn, operands, 1, 0);"
-  [(set_attr "type" "ssecmp")
-   (set_attr "mode" "unknownfp")
+  [(set_attr "type" "ssecomi")
+   (set (attr "mode")
+     (if_then_else (match_operand:SF 1 "" "")
+        (const_string "SF")
+        (const_string "DF")))
    (set_attr "athlon_decode" "vector")])
 
 (define_insn "*cmpfp_iu"
@@ -985,7 +1034,13 @@
    && GET_MODE (operands[0]) == GET_MODE (operands[1])"
   "* return output_fp_compare (insn, operands, 1, 1);"
   [(set_attr "type" "fcmp")
-   (set_attr "mode" "unknownfp")
+   (set (attr "mode")
+     (cond [(match_operand:SF 1 "" "")
+	      (const_string "SF")
+	    (match_operand:DF 1 "" "")
+	      (const_string "DF")
+	   ]
+	   (const_string "XF")))
    (set_attr "athlon_decode" "vector")])
 
 (define_insn "*cmpfp_iu_sse"
@@ -996,8 +1051,11 @@
    && SSE_FLOAT_MODE_P (GET_MODE (operands[0]))
    && GET_MODE (operands[0]) == GET_MODE (operands[1])"
   "* return output_fp_compare (insn, operands, 1, 1);"
-  [(set_attr "type" "fcmp,ssecmp")
-   (set_attr "mode" "unknownfp")
+  [(set_attr "type" "fcmp,ssecomi")
+   (set (attr "mode")
+     (if_then_else (match_operand:SF 1 "" "")
+        (const_string "SF")
+        (const_string "DF")))
    (set_attr "athlon_decode" "vector")])
 
 (define_insn "*cmpfp_iu_sse_only"
@@ -1007,8 +1065,11 @@
   "SSE_FLOAT_MODE_P (GET_MODE (operands[0]))
    && GET_MODE (operands[0]) == GET_MODE (operands[1])"
   "* return output_fp_compare (insn, operands, 1, 1);"
-  [(set_attr "type" "ssecmp")
-   (set_attr "mode" "unknownfp")
+  [(set_attr "type" "ssecomi")
+   (set (attr "mode")
+     (if_then_else (match_operand:SF 1 "" "")
+        (const_string "SF")
+        (const_string "DF")))
    (set_attr "athlon_decode" "vector")])
 
 ;; Move instructions.
@@ -1103,9 +1164,10 @@
    (set_attr "length_immediate" "1")])
 
 (define_insn "*movsi_1"
-  [(set (match_operand:SI 0 "nonimmediate_operand" "=r,m,!*y,!rm,!*y,!*Y,!*Y,!rm")
-	(match_operand:SI 1 "general_operand" "rinm,rin,rm,*y,*y,*Y,rm,*Y"))]
-  "GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM"
+  [(set (match_operand:SI 0 "nonimmediate_operand" "=r,m,!*y,!rm,!*y,!*Y,!rm,!*Y")
+	(match_operand:SI 1 "general_operand" "rinm,rin,*y,*y,rm,*Y,*Y,rm"))]
+  "(TARGET_INTER_UNIT_MOVES || optimize_size)
+   && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
 {
   switch (get_attr_type (insn))
     {
@@ -1138,36 +1200,70 @@
 	      (const_string "lea")
 	   ]
 	   (const_string "imov")))
-   (set_attr "mode" "SI,SI,SI,SI,DI,TI,SI,SI")])
+   (set_attr "mode" "SI,SI,DI,SI,SI,TI,SI,SI")])
 
-;; Stores and loads of ax to arbitary constant address.
+(define_insn "*movsi_1_nointernunit"
+  [(set (match_operand:SI 0 "nonimmediate_operand" "=r,m,!*y,!m,!*y,!*Y,!m,!*Y")
+	(match_operand:SI 1 "general_operand" "rinm,rin,*y,*y,m,*Y,*Y,m"))]
+  "(!TARGET_INTER_UNIT_MOVES && !optimize_size)
+   && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
+{
+  switch (get_attr_type (insn))
+    {
+    case TYPE_SSEMOV:
+      if (get_attr_mode (insn) == MODE_TI)
+        return "movdqa\t{%1, %0|%0, %1}";
+      return "movd\t{%1, %0|%0, %1}";
+
+    case TYPE_MMXMOV:
+      if (get_attr_mode (insn) == MODE_DI)
+	return "movq\t{%1, %0|%0, %1}";
+      return "movd\t{%1, %0|%0, %1}";
+
+    case TYPE_LEA:
+      return "lea{l}\t{%1, %0|%0, %1}";
+
+    default:
+      if (flag_pic && !LEGITIMATE_PIC_OPERAND_P (operands[1]))
+	abort();
+      return "mov{l}\t{%1, %0|%0, %1}";
+    }
+}
+  [(set (attr "type")
+     (cond [(eq_attr "alternative" "2,3,4")
+	      (const_string "mmxmov")
+	    (eq_attr "alternative" "5,6,7")
+	      (const_string "ssemov")
+	    (and (ne (symbol_ref "flag_pic") (const_int 0))
+		 (match_operand:SI 1 "symbolic_operand" ""))
+	      (const_string "lea")
+	   ]
+	   (const_string "imov")))
+   (set_attr "mode" "SI,SI,DI,SI,SI,TI,SI,SI")])
+
+;; Stores and loads of ax to arbitrary constant address.
 ;; We fake an second form of instruction to force reload to load address
 ;; into register when rax is not available
 (define_insn "*movabssi_1_rex64"
-  [(set (mem:SI (match_operand:DI 0 "x86_64_movabs_operand" "i,r,r"))
-	(match_operand:SI 1 "nonmemory_operand" "a,er,i"))]
+  [(set (mem:SI (match_operand:DI 0 "x86_64_movabs_operand" "i"))
+	(match_operand:SI 1 "nonmemory_operand" "a"))]
   "TARGET_64BIT"
-  "@
-   movabs{l}\t{%1, %P0|%P0, %1}
-   mov{l}\t{%1, %a0|%a0, %1}
-   movabs{l}\t{%1, %a0|%a0, %1}"
+  "movabs{l}\t{%1, %P0|%P0, %1}"
   [(set_attr "type" "imov")
-   (set_attr "modrm" "0,*,*")
-   (set_attr "length_address" "8,0,0")
-   (set_attr "length_immediate" "0,*,*")
+   (set_attr "modrm" "0")
+   (set_attr "length_address" "8")
+   (set_attr "length_immediate" "0")
    (set_attr "memory" "store")
    (set_attr "mode" "SI")])
 
 (define_insn "*movabssi_2_rex64"
-  [(set (match_operand:SI 0 "register_operand" "=a,r")
-        (mem:SI (match_operand:DI 1 "x86_64_movabs_operand" "i,r")))]
+  [(set (match_operand:SI 0 "register_operand" "=a")
+        (mem:SI (match_operand:DI 1 "x86_64_movabs_operand" "i")))]
   "TARGET_64BIT"
-  "@
-   movabs{l}\t{%P1, %0|%0, %P1}
-   mov{l}\t{%a1, %0|%0, %a1}"
+  "movabs{l}\t{%P1, %0|%0, %P1}"
   [(set_attr "type" "imov")
-   (set_attr "modrm" "0,*")
-   (set_attr "length_address" "8,0")
+   (set_attr "modrm" "0")
+   (set_attr "length_address" "8")
    (set_attr "length_immediate" "0")
    (set_attr "memory" "load")
    (set_attr "mode" "SI")])
@@ -1260,34 +1356,29 @@
 	    ]
 	    (const_string "HI")))])
 
-;; Stores and loads of ax to arbitary constant address.
+;; Stores and loads of ax to arbitrary constant address.
 ;; We fake an second form of instruction to force reload to load address
 ;; into register when rax is not available
 (define_insn "*movabshi_1_rex64"
-  [(set (mem:HI (match_operand:DI 0 "x86_64_movabs_operand" "i,r,r"))
-	(match_operand:HI 1 "nonmemory_operand" "a,er,i"))]
+  [(set (mem:HI (match_operand:DI 0 "x86_64_movabs_operand" "i"))
+	(match_operand:HI 1 "nonmemory_operand" "a"))]
   "TARGET_64BIT"
-  "@
-   movabs{w}\t{%1, %P0|%P0, %1}
-   mov{w}\t{%1, %a0|%a0, %1}
-   movabs{w}\t{%1, %a0|%a0, %1}"
+  "movabs{w}\t{%1, %P0|%P0, %1}"
   [(set_attr "type" "imov")
-   (set_attr "modrm" "0,*,*")
-   (set_attr "length_address" "8,0,0")
-   (set_attr "length_immediate" "0,*,*")
+   (set_attr "modrm" "0")
+   (set_attr "length_address" "8")
+   (set_attr "length_immediate" "0")
    (set_attr "memory" "store")
    (set_attr "mode" "HI")])
 
 (define_insn "*movabshi_2_rex64"
-  [(set (match_operand:HI 0 "register_operand" "=a,r")
-        (mem:HI (match_operand:DI 1 "x86_64_movabs_operand" "i,r")))]
+  [(set (match_operand:HI 0 "register_operand" "=a")
+        (mem:HI (match_operand:DI 1 "x86_64_movabs_operand" "i")))]
   "TARGET_64BIT"
-  "@
-   movabs{w}\t{%P1, %0|%0, %P1}
-   mov{w}\t{%a1, %0|%0, %a1}"
+  "movabs{w}\t{%P1, %0|%0, %P1}"
   [(set_attr "type" "imov")
-   (set_attr "modrm" "0,*")
-   (set_attr "length_address" "8,0")
+   (set_attr "modrm" "0")
+   (set_attr "length_address" "8")
    (set_attr "length_immediate" "0")
    (set_attr "memory" "load")
    (set_attr "mode" "HI")])
@@ -1579,34 +1670,29 @@
 	(const_string "SI")
 	(const_string "QI")))])
 
-;; Stores and loads of ax to arbitary constant address.
+;; Stores and loads of ax to arbitrary constant address.
 ;; We fake an second form of instruction to force reload to load address
 ;; into register when rax is not available
 (define_insn "*movabsqi_1_rex64"
-  [(set (mem:QI (match_operand:DI 0 "x86_64_movabs_operand" "i,r,r"))
-	(match_operand:QI 1 "nonmemory_operand" "a,er,i"))]
+  [(set (mem:QI (match_operand:DI 0 "x86_64_movabs_operand" "i"))
+	(match_operand:QI 1 "nonmemory_operand" "a"))]
   "TARGET_64BIT"
-  "@
-   movabs{b}\t{%1, %P0|%P0, %1}
-   mov{b}\t{%1, %a0|%a0, %1}
-   movabs{b}\t{%1, %a0|%a0, %1}"
+  "movabs{b}\t{%1, %P0|%P0, %1}"
   [(set_attr "type" "imov")
-   (set_attr "modrm" "0,*,*")
-   (set_attr "length_address" "8,0,0")
-   (set_attr "length_immediate" "0,*,*")
+   (set_attr "modrm" "0")
+   (set_attr "length_address" "8")
+   (set_attr "length_immediate" "0")
    (set_attr "memory" "store")
    (set_attr "mode" "QI")])
 
 (define_insn "*movabsqi_2_rex64"
-  [(set (match_operand:QI 0 "register_operand" "=a,r")
-        (mem:QI (match_operand:DI 1 "x86_64_movabs_operand" "i,r")))]
+  [(set (match_operand:QI 0 "register_operand" "=a")
+        (mem:QI (match_operand:DI 1 "x86_64_movabs_operand" "i")))]
   "TARGET_64BIT"
-  "@
-   movabs{b}\t{%P1, %0|%0, %P1}
-   mov{b}\t{%a1, %0|%0, %a1}"
+  "movabs{b}\t{%P1, %0|%0, %P1}"
   [(set_attr "type" "imov")
-   (set_attr "modrm" "0,*")
-   (set_attr "length_address" "8,0")
+   (set_attr "modrm" "0")
+   (set_attr "length_address" "8")
    (set_attr "length_immediate" "0")
    (set_attr "memory" "load")
    (set_attr "mode" "QI")])
@@ -1863,19 +1949,23 @@
   "ix86_split_long_move (operands); DONE;")
 
 (define_insn "*movdi_1_rex64"
-  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,r,r,mr,!mr,!m*y,!*y,!*Y,!m,!*Y")
-	(match_operand:DI 1 "general_operand" "Z,rem,i,re,n,*y,m,*Y,*Y,*m"))]
+  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,r,r,mr,!mr,!*y,!rm,!*y,!*Y,!rm,!*Y")
+	(match_operand:DI 1 "general_operand" "Z,rem,i,re,n,*y,*y,rm,*Y,*Y,rm"))]
   "TARGET_64BIT
+   && (TARGET_INTER_UNIT_MOVES || optimize_size)
    && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
 {
   switch (get_attr_type (insn))
     {
     case TYPE_SSEMOV:
-      if (register_operand (operands[0], DImode)
-	  && register_operand (operands[1], DImode))
+      if (get_attr_mode (insn) == MODE_TI)
 	  return "movdqa\t{%1, %0|%0, %1}";
       /* FALLTHRU */
     case TYPE_MMXMOV:
+      /* Moves from and into integer register is done using movd opcode with
+ 	 REX prefix.  */
+      if (GENERAL_REG_P (operands[0]) || GENERAL_REG_P (operands[1]))
+	  return "movd\t{%1, %0|%0, %1}";
       return "movq\t{%1, %0|%0, %1}";
     case TYPE_MULTI:
       return "#";
@@ -1893,9 +1983,9 @@
     }
 }
   [(set (attr "type")
-     (cond [(eq_attr "alternative" "5,6")
+     (cond [(eq_attr "alternative" "5,6,7")
 	      (const_string "mmxmov")
-	    (eq_attr "alternative" "7,8")
+	    (eq_attr "alternative" "8,9,10")
 	      (const_string "ssemov")
 	    (eq_attr "alternative" "4")
 	      (const_string "multi")
@@ -1904,38 +1994,79 @@
 	      (const_string "lea")
 	   ]
 	   (const_string "imov")))
-   (set_attr "modrm" "*,0,0,*,*,*,*,*,*,*")
-   (set_attr "length_immediate" "*,4,8,*,*,*,*,*,*,*")
-   (set_attr "mode" "SI,DI,DI,DI,SI,DI,DI,DI,TI,DI")])
+   (set_attr "modrm" "*,0,0,*,*,*,*,*,*,*,*")
+   (set_attr "length_immediate" "*,4,8,*,*,*,*,*,*,*,*")
+   (set_attr "mode" "SI,DI,DI,DI,SI,DI,DI,DI,TI,DI,DI")])
+
+(define_insn "*movdi_1_rex64_nointerunit"
+  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,r,r,mr,!mr,!*y,!m,!*y,!*Y,!m,!*Y")
+	(match_operand:DI 1 "general_operand" "Z,rem,i,re,n,*y,*y,m,*Y,*Y,m"))]
+  "TARGET_64BIT
+   && (!TARGET_INTER_UNIT_MOVES && !optimize_size)
+   && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
+{
+  switch (get_attr_type (insn))
+    {
+    case TYPE_SSEMOV:
+      if (get_attr_mode (insn) == MODE_TI)
+	  return "movdqa\t{%1, %0|%0, %1}";
+      /* FALLTHRU */
+    case TYPE_MMXMOV:
+      return "movq\t{%1, %0|%0, %1}";
+    case TYPE_MULTI:
+      return "#";
+    case TYPE_LEA:
+      return "lea{q}\t{%a1, %0|%0, %a1}";
+    default:
+      if (flag_pic && !LEGITIMATE_PIC_OPERAND_P (operands[1]))
+	abort ();
+      if (get_attr_mode (insn) == MODE_SI)
+	return "mov{l}\t{%k1, %k0|%k0, %k1}";
+      else if (which_alternative == 2)
+	return "movabs{q}\t{%1, %0|%0, %1}";
+      else
+	return "mov{q}\t{%1, %0|%0, %1}";
+    }
+}
+  [(set (attr "type")
+     (cond [(eq_attr "alternative" "5,6,7")
+	      (const_string "mmxmov")
+	    (eq_attr "alternative" "8,9,10")
+	      (const_string "ssemov")
+	    (eq_attr "alternative" "4")
+	      (const_string "multi")
+ 	    (and (ne (symbol_ref "flag_pic") (const_int 0))
+		 (match_operand:DI 1 "symbolic_operand" ""))
+	      (const_string "lea")
+	   ]
+	   (const_string "imov")))
+   (set_attr "modrm" "*,0,0,*,*,*,*,*,*,*,*")
+   (set_attr "length_immediate" "*,4,8,*,*,*,*,*,*,*,*")
+   (set_attr "mode" "SI,DI,DI,DI,SI,DI,DI,DI,TI,DI,DI")])
 
-;; Stores and loads of ax to arbitary constant address.
+;; Stores and loads of ax to arbitrary constant address.
 ;; We fake an second form of instruction to force reload to load address
 ;; into register when rax is not available
 (define_insn "*movabsdi_1_rex64"
-  [(set (mem:DI (match_operand:DI 0 "x86_64_movabs_operand" "i,r,r"))
-	(match_operand:DI 1 "nonmemory_operand" "a,er,i"))]
+  [(set (mem:DI (match_operand:DI 0 "x86_64_movabs_operand" "i"))
+	(match_operand:DI 1 "nonmemory_operand" "a"))]
   "TARGET_64BIT"
-  "@
-   movabs{q}\t{%1, %P0|%P0, %1}
-   mov{q}\t{%1, %a0|%a0, %1}
-   movabs{q}\t{%1, %a0|%a0, %1}"
+  "movabs{q}\t{%1, %P0|%P0, %1}"
   [(set_attr "type" "imov")
-   (set_attr "modrm" "0,*,*")
-   (set_attr "length_address" "8,0,0")
-   (set_attr "length_immediate" "0,*,*")
+   (set_attr "modrm" "0")
+   (set_attr "length_address" "8")
+   (set_attr "length_immediate" "0")
    (set_attr "memory" "store")
    (set_attr "mode" "DI")])
 
 (define_insn "*movabsdi_2_rex64"
-  [(set (match_operand:DI 0 "register_operand" "=a,r")
-        (mem:DI (match_operand:DI 1 "x86_64_movabs_operand" "i,r")))]
+  [(set (match_operand:DI 0 "register_operand" "=a")
+        (mem:DI (match_operand:DI 1 "x86_64_movabs_operand" "i")))]
   "TARGET_64BIT"
-  "@
-   movabs{q}\t{%P1, %0|%0, %P1}
-   mov{q}\t{%a1, %0|%0, %a1}"
+  "movabs{q}\t{%P1, %0|%0, %P1}"
   [(set_attr "type" "imov")
-   (set_attr "modrm" "0,*")
-   (set_attr "length_address" "8,0")
+   (set_attr "modrm" "0")
+   (set_attr "length_address" "8")
    (set_attr "length_immediate" "0")
    (set_attr "memory" "load")
    (set_attr "mode" "DI")])
@@ -2002,22 +2133,11 @@
 {
   switch (which_alternative)
     {
-    case 0:
-      /* %%% We loose REG_DEAD notes for controling pops if we split late.  */
-      operands[0] = gen_rtx_MEM (SFmode, stack_pointer_rtx);
-      operands[2] = stack_pointer_rtx;
-      operands[3] = GEN_INT (4);
-      if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-	return "sub{l}\t{%3, %2|%2, %3}\;fstp%z0\t%y0";
-      else
-	return "sub{l}\t{%3, %2|%2, %3}\;fst%z0\t%y0";
-
     case 1:
       return "push{l}\t%1";
-    case 2:
-      return "#";
 
     default:
+      /* This insn should be already splitted before reg-stack.  */
       abort ();
     }
 }
@@ -2031,23 +2151,11 @@
 {
   switch (which_alternative)
     {
-    case 0:
-      /* %%% We loose REG_DEAD notes for controling pops if we split late.  */
-      operands[0] = gen_rtx_MEM (SFmode, stack_pointer_rtx);
-      operands[2] = stack_pointer_rtx;
-      operands[3] = GEN_INT (8);
-      if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-	return "sub{q}\t{%3, %2|%2, %3}\;fstp%z0\t%y0";
-      else
-	return "sub{q}\t{%3, %2|%2, %3}\;fst%z0\t%y0";
-
     case 1:
       return "push{q}\t%q1";
 
-    case 2:
-      return "#";
-
     default:
+      /* This insn should be already splitted before reg-stack.  */
       abort ();
     }
 }
@@ -2084,7 +2192,8 @@
 (define_insn "*movsf_1"
   [(set (match_operand:SF 0 "nonimmediate_operand" "=f#xr,m,f#xr,r#xf,m,x#rf,x#rf,x#rf,m,!*y,!rm,!*y")
 	(match_operand:SF 1 "general_operand" "fm#rx,f#rx,G,rmF#fx,Fr#fx,C,x,xm#rf,x#rf,rm,*y,*y"))]
-  "(GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)
+  "(TARGET_INTER_UNIT_MOVES || optimize_size)
+   && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)
    && (reload_in_progress || reload_completed
        || (ix86_cmodel == CM_MEDIUM || ix86_cmodel == CM_LARGE)
        || GET_CODE (operands[1]) != CONST_DOUBLE
@@ -2108,25 +2217,117 @@
         return "fst%z0\t%y0";
 
     case 2:
-      switch (standard_80387_constant_p (operands[1]))
-        {
-        case 1:
-	  return "fldz";
-	case 2:
-	  return "fld1";
-	}
+      return standard_80387_constant_opcode (operands[1]);
+
+    case 3:
+    case 4:
+      return "mov{l}\t{%1, %0|%0, %1}";
+    case 5:
+      if (get_attr_mode (insn) == MODE_TI)
+	return "pxor\t%0, %0";
+      else
+	return "xorps\t%0, %0";
+    case 6:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "movaps\t{%1, %0|%0, %1}";
+      else
+	return "movss\t{%1, %0|%0, %1}";
+    case 7:
+    case 8:
+      return "movss\t{%1, %0|%0, %1}";
+
+    case 9:
+    case 10:
+      return "movd\t{%1, %0|%0, %1}";
+
+    case 11:
+      return "movq\t{%1, %0|%0, %1}";
+
+    default:
       abort();
+    }
+}
+  [(set_attr "type" "fmov,fmov,fmov,imov,imov,ssemov,ssemov,ssemov,ssemov,mmxmov,mmxmov,mmxmov")
+   (set (attr "mode")
+        (cond [(eq_attr "alternative" "3,4,9,10")
+		 (const_string "SI")
+	       (eq_attr "alternative" "5")
+		 (if_then_else
+		   (and (and (ne (symbol_ref "TARGET_SSE_LOAD0_BY_PXOR")
+			    	 (const_int 0))
+			     (ne (symbol_ref "TARGET_SSE2")
+				 (const_int 0)))
+			(eq (symbol_ref "optimize_size")
+			    (const_int 0)))
+		   (const_string "TI")
+		   (const_string "V4SF"))
+	       /* For architectures resolving dependencies on
+		  whole SSE registers use APS move to break dependency
+		  chains, otherwise use short move to avoid extra work. 
+
+		  Do the same for architectures resolving dependencies on
+		  the parts.  While in DF mode it is better to always handle
+		  just register parts, the SF mode is different due to lack
+		  of instructions to load just part of the register.  It is
+		  better to maintain the whole registers in single format
+		  to avoid problems on using packed logical operations.  */
+	       (eq_attr "alternative" "6")
+		 (if_then_else
+		   (ior (ne (symbol_ref "TARGET_SSE_PARTIAL_REG_DEPENDENCY")
+			    (const_int 0))
+			(ne (symbol_ref "TARGET_SSE_PARTIAL_REGS")
+			    (const_int 0)))
+		   (const_string "V4SF")
+		   (const_string "SF"))
+	       (eq_attr "alternative" "11")
+		 (const_string "DI")]
+	       (const_string "SF")))])
+
+(define_insn "*movsf_1_nointerunit"
+  [(set (match_operand:SF 0 "nonimmediate_operand" "=f#xr,m,f#xr,r#xf,m,x#rf,x#rf,x#rf,m,!*y,!m,!*y")
+	(match_operand:SF 1 "general_operand" "fm#rx,f#rx,G,rmF#fx,Fr#fx,C,x,xm#rf,x#rf,m,*y,*y"))]
+  "(!TARGET_INTER_UNIT_MOVES && !optimize_size)
+   && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)
+   && (reload_in_progress || reload_completed
+       || (ix86_cmodel == CM_MEDIUM || ix86_cmodel == CM_LARGE)
+       || GET_CODE (operands[1]) != CONST_DOUBLE
+       || memory_operand (operands[0], SFmode))" 
+{
+  switch (which_alternative)
+    {
+    case 0:
+      if (REG_P (operands[1])
+          && find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
+	{
+	  if (REGNO (operands[0]) == FIRST_STACK_REG
+	      && TARGET_USE_FFREEP)
+	    return "ffreep\t%y0";
+          return "fstp\t%y0";
+	}
+      else if (STACK_TOP_P (operands[0]))
+        return "fld%z1\t%y1";
+      else
+        return "fst\t%y0";
+
+    case 1:
+      if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
+        return "fstp%z0\t%y0";
+      else
+        return "fst%z0\t%y0";
+
+    case 2:
+      return standard_80387_constant_opcode (operands[1]);
 
     case 3:
     case 4:
       return "mov{l}\t{%1, %0|%0, %1}";
     case 5:
-      if (TARGET_SSE2 && !TARGET_ATHLON)
+      if (get_attr_mode (insn) == MODE_TI)
 	return "pxor\t%0, %0";
       else
 	return "xorps\t%0, %0";
     case 6:
-      if (TARGET_PARTIAL_REG_DEPENDENCY)
+      if (get_attr_mode (insn) == MODE_V4SF)
 	return "movaps\t{%1, %0|%0, %1}";
       else
 	return "movss\t{%1, %0|%0, %1}";
@@ -2146,7 +2347,40 @@
     }
 }
   [(set_attr "type" "fmov,fmov,fmov,imov,imov,ssemov,ssemov,ssemov,ssemov,mmxmov,mmxmov,mmxmov")
-   (set_attr "mode" "SF,SF,SF,SI,SI,TI,SF,SF,SF,SI,SI,DI")])
+   (set (attr "mode")
+        (cond [(eq_attr "alternative" "3,4,9,10")
+		 (const_string "SI")
+	       (eq_attr "alternative" "5")
+		 (if_then_else
+		   (and (and (ne (symbol_ref "TARGET_SSE_LOAD0_BY_PXOR")
+			    	 (const_int 0))
+			     (ne (symbol_ref "TARGET_SSE2")
+				 (const_int 0)))
+			(eq (symbol_ref "optimize_size")
+			    (const_int 0)))
+		   (const_string "TI")
+		   (const_string "V4SF"))
+	       /* For architectures resolving dependencies on
+		  whole SSE registers use APS move to break dependency
+		  chains, otherwise use short move to avoid extra work. 
+
+		  Do the same for architectures resolving dependencies on
+		  the parts.  While in DF mode it is better to always handle
+		  just register parts, the SF mode is different due to lack
+		  of instructions to load just part of the register.  It is
+		  better to maintain the whole registers in single format
+		  to avoid problems on using packed logical operations.  */
+	       (eq_attr "alternative" "6")
+		 (if_then_else
+		   (ior (ne (symbol_ref "TARGET_SSE_PARTIAL_REG_DEPENDENCY")
+			    (const_int 0))
+			(ne (symbol_ref "TARGET_SSE_PARTIAL_REGS")
+			    (const_int 0)))
+		   (const_string "V4SF")
+		   (const_string "SF"))
+	       (eq_attr "alternative" "11")
+		 (const_string "DI")]
+	       (const_string "SF")))])
 
 (define_insn "*swapsf"
   [(set (match_operand:SF 0 "register_operand" "+f")
@@ -2170,7 +2404,7 @@
   "ix86_expand_move (DFmode, operands); DONE;")
 
 ;; Size of pushdf is 3 (for sub) + 2 (for fstp) + memory operand size.
-;; Size of pushdf using integer insturctions is 2+2*memory operand size
+;; Size of pushdf using integer instructions is 2+2*memory operand size
 ;; On the average, pushdf using integers can be still shorter.  Allow this
 ;; pattern for optimize_size too.
 
@@ -2179,26 +2413,8 @@
 	(match_operand:DF 1 "general_no_elim_operand" "f#Y,Fo#fY,*r#fY,Y#f"))]
   "!TARGET_64BIT && !TARGET_INTEGER_DFMODE_MOVES"
 {
-  switch (which_alternative)
-    {
-    case 0:
-      /* %%% We loose REG_DEAD notes for controling pops if we split late.  */
-      operands[0] = gen_rtx_MEM (DFmode, stack_pointer_rtx);
-      operands[2] = stack_pointer_rtx;
-      operands[3] = GEN_INT (8);
-      if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-	return "sub{l}\t{%3, %2|%2, %3}\;fstp%z0\t%y0";
-      else
-	return "sub{l}\t{%3, %2|%2, %3}\;fst%z0\t%y0";
-
-    case 1:
-    case 2:
-    case 3:
-      return "#";
-
-    default:
-      abort ();
-    }
+  /* This insn should be already splitted before reg-stack.  */
+  abort ();
 }
   [(set_attr "type" "multi")
    (set_attr "mode" "DF,SI,SI,DF")])
@@ -2208,32 +2424,8 @@
 	(match_operand:DF 1 "general_no_elim_operand" "f#rY,rFo#fY,Y#rf"))]
   "TARGET_64BIT || TARGET_INTEGER_DFMODE_MOVES"
 {
-  switch (which_alternative)
-    {
-    case 0:
-      /* %%% We loose REG_DEAD notes for controling pops if we split late.  */
-      operands[0] = gen_rtx_MEM (DFmode, stack_pointer_rtx);
-      operands[2] = stack_pointer_rtx;
-      operands[3] = GEN_INT (8);
-      if (TARGET_64BIT)
-	if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-	  return "sub{q}\t{%3, %2|%2, %3}\;fstp%z0\t%y0";
-	else
-	  return "sub{q}\t{%3, %2|%2, %3}\;fst%z0\t%y0";
-      else
-	if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-	  return "sub{l}\t{%3, %2|%2, %3}\;fstp%z0\t%y0";
-	else
-	  return "sub{l}\t{%3, %2|%2, %3}\;fst%z0\t%y0";
-
-
-    case 1:
-    case 2:
-      return "#";
-
-    default:
-      abort ();
-    }
+  /* This insn should be already splitted before reg-stack.  */
+  abort ();
 }
   [(set_attr "type" "multi")
    (set_attr "mode" "DF,SI,DF")])
@@ -2270,7 +2462,7 @@
   [(set (match_operand:DF 0 "nonimmediate_operand" "=f#Y,m,f#Y,*r,o,Y#f,Y#f,Y#f,m")
 	(match_operand:DF 1 "general_operand" "fm#Y,f#Y,G,*roF,F*r,C,Y#f,YHm#f,Y#f"))]
   "(GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)
-   && (optimize_size || !TARGET_INTEGER_DFMODE_MOVES)
+   && ((optimize_size || !TARGET_INTEGER_DFMODE_MOVES) && !TARGET_64BIT)
    && (reload_in_progress || reload_completed
        || (ix86_cmodel == CM_MEDIUM || ix86_cmodel == CM_LARGE)
        || GET_CODE (operands[1]) != CONST_DOUBLE
@@ -2281,7 +2473,12 @@
     case 0:
       if (REG_P (operands[1])
           && find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-        return "fstp\t%y0";
+	{
+	  if (REGNO (operands[0]) == FIRST_STACK_REG
+	      && TARGET_USE_FFREEP)
+	    return "ffreep\t%y0";
+          return "fstp\t%y0";
+	}
       else if (STACK_TOP_P (operands[0]))
         return "fld%z1\t%y1";
       else
@@ -2294,44 +2491,90 @@
         return "fst%z0\t%y0";
 
     case 2:
-      switch (standard_80387_constant_p (operands[1]))
-        {
-        case 1:
-	  return "fldz";
-	case 2:
-	  return "fld1";
-	}
-      abort();
+      return standard_80387_constant_opcode (operands[1]);
 
     case 3:
     case 4:
       return "#";
     case 5:
-      if (TARGET_ATHLON)
-        return "xorpd\t%0, %0";
-      else
-        return "pxor\t%0, %0";
+      switch (get_attr_mode (insn))
+	{
+	case MODE_V4SF:
+	  return "xorps\t%0, %0";
+	case MODE_V2DF:
+	  return "xorpd\t%0, %0";
+	case MODE_TI:
+	  return "pxor\t%0, %0";
+	default:
+	  abort ();
+	}
     case 6:
-      if (TARGET_PARTIAL_REG_DEPENDENCY)
-	return "movapd\t{%1, %0|%0, %1}";
+      switch (get_attr_mode (insn))
+	{
+	case MODE_V4SF:
+	  return "movaps\t{%1, %0|%0, %1}";
+	case MODE_V2DF:
+	  return "movapd\t{%1, %0|%0, %1}";
+	case MODE_DF:
+	  return "movsd\t{%1, %0|%0, %1}";
+	default:
+	  abort ();
+	}
+    case 7:
+      if (get_attr_mode (insn) == MODE_V2DF)
+	return "movlpd\t{%1, %0|%0, %1}";
       else
 	return "movsd\t{%1, %0|%0, %1}";
-    case 7:
     case 8:
-        return "movsd\t{%1, %0|%0, %1}";
+      return "movsd\t{%1, %0|%0, %1}";
 
     default:
       abort();
     }
 }
   [(set_attr "type" "fmov,fmov,fmov,multi,multi,ssemov,ssemov,ssemov,ssemov")
-   (set_attr "mode" "DF,DF,DF,SI,SI,TI,DF,DF,DF")])
+   (set (attr "mode")
+        (cond [(eq_attr "alternative" "3,4")
+		 (const_string "SI")
+	       /* xorps is one byte shorter.  */
+	       (eq_attr "alternative" "5")
+		 (cond [(ne (symbol_ref "optimize_size")
+			    (const_int 0))
+			  (const_string "V4SF")
+			(ne (symbol_ref "TARGET_SSE_LOAD0_BY_PXOR")
+			    (const_int 0))
+			  (const_string "TI")]
+		       (const_string "V2DF"))
+	       /* For architectures resolving dependencies on
+		  whole SSE registers use APD move to break dependency
+		  chains, otherwise use short move to avoid extra work.
+
+		  movaps encodes one byte shorter.  */
+	       (eq_attr "alternative" "6")
+		 (cond
+		  [(ne (symbol_ref "optimize_size")
+		       (const_int 0))
+		     (const_string "V4SF")
+		   (ne (symbol_ref "TARGET_SSE_PARTIAL_REG_DEPENDENCY")
+		       (const_int 0))
+		     (const_string "V2DF")]
+		   (const_string "DF"))
+	       /* For architectures resolving dependencies on register
+		  parts we may avoid extra work to zero out upper part
+		  of register.  */
+	       (eq_attr "alternative" "7")
+		 (if_then_else
+		   (ne (symbol_ref "TARGET_SSE_PARTIAL_REGS")
+		       (const_int 0))
+		   (const_string "V2DF")
+		   (const_string "DF"))]
+	       (const_string "DF")))])
 
 (define_insn "*movdf_integer"
   [(set (match_operand:DF 0 "nonimmediate_operand" "=f#Yr,m,f#Yr,r#Yf,o,Y#rf,Y#rf,Y#rf,m")
 	(match_operand:DF 1 "general_operand" "fm#Yr,f#Yr,G,roF#Yf,Fr#Yf,C,Y#rf,Ym#rf,Y#rf"))]
   "(GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)
-   && !optimize_size && TARGET_INTEGER_DFMODE_MOVES
+   && ((!optimize_size && TARGET_INTEGER_DFMODE_MOVES) || TARGET_64BIT)
    && (reload_in_progress || reload_completed
        || (ix86_cmodel == CM_MEDIUM || ix86_cmodel == CM_LARGE)
        || GET_CODE (operands[1]) != CONST_DOUBLE
@@ -2342,7 +2585,12 @@
     case 0:
       if (REG_P (operands[1])
           && find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-        return "fstp\t%y0";
+	{
+	  if (REGNO (operands[0]) == FIRST_STACK_REG
+	      && TARGET_USE_FFREEP)
+	    return "ffreep\t%y0";
+          return "fstp\t%y0";
+	}
       else if (STACK_TOP_P (operands[0]))
         return "fld%z1\t%y1";
       else
@@ -2355,30 +2603,41 @@
         return "fst%z0\t%y0";
 
     case 2:
-      switch (standard_80387_constant_p (operands[1]))
-        {
-        case 1:
-	  return "fldz";
-	case 2:
-	  return "fld1";
-	}
-      abort();
+      return standard_80387_constant_opcode (operands[1]);
 
     case 3:
     case 4:
       return "#";
 
     case 5:
-      if (TARGET_ATHLON)
-        return "xorpd\t%0, %0";
-      else
-        return "pxor\t%0, %0";
+      switch (get_attr_mode (insn))
+	{
+	case MODE_V4SF:
+	  return "xorps\t%0, %0";
+	case MODE_V2DF:
+	  return "xorpd\t%0, %0";
+	case MODE_TI:
+	  return "pxor\t%0, %0";
+	default:
+	  abort ();
+	}
     case 6:
-      if (TARGET_PARTIAL_REG_DEPENDENCY)
-	return "movapd\t{%1, %0|%0, %1}";
+      switch (get_attr_mode (insn))
+	{
+	case MODE_V4SF:
+	  return "movaps\t{%1, %0|%0, %1}";
+	case MODE_V2DF:
+	  return "movapd\t{%1, %0|%0, %1}";
+	case MODE_DF:
+	  return "movsd\t{%1, %0|%0, %1}";
+	default:
+	  abort ();
+	}
+    case 7:
+      if (get_attr_mode (insn) == MODE_V2DF)
+	return "movlpd\t{%1, %0|%0, %1}";
       else
 	return "movsd\t{%1, %0|%0, %1}";
-    case 7:
     case 8:
       return "movsd\t{%1, %0|%0, %1}";
 
@@ -2387,7 +2646,42 @@
     }
 }
   [(set_attr "type" "fmov,fmov,fmov,multi,multi,ssemov,ssemov,ssemov,ssemov")
-   (set_attr "mode" "DF,DF,DF,SI,SI,TI,DF,DF,DF")])
+   (set (attr "mode")
+        (cond [(eq_attr "alternative" "3,4")
+		 (const_string "SI")
+	       /* xorps is one byte shorter.  */
+	       (eq_attr "alternative" "5")
+		 (cond [(ne (symbol_ref "optimize_size")
+			    (const_int 0))
+			  (const_string "V4SF")
+			(ne (symbol_ref "TARGET_SSE_LOAD0_BY_PXOR")
+			    (const_int 0))
+			  (const_string "TI")]
+		       (const_string "V2DF"))
+	       /* For architectures resolving dependencies on
+		  whole SSE registers use APD move to break dependency
+		  chains, otherwise use short move to avoid extra work.  
+
+		  movaps encodes one byte shorter.  */
+	       (eq_attr "alternative" "6")
+		 (cond
+		  [(ne (symbol_ref "optimize_size")
+		       (const_int 0))
+		     (const_string "V4SF")
+		   (ne (symbol_ref "TARGET_SSE_PARTIAL_REG_DEPENDENCY")
+		       (const_int 0))
+		     (const_string "V2DF")]
+		   (const_string "DF"))
+	       /* For architectures resolving dependencies on register
+		  parts we may avoid extra work to zero out upper part
+		  of register.  */
+	       (eq_attr "alternative" "7")
+		 (if_then_else
+		   (ne (symbol_ref "TARGET_SSE_PARTIAL_REGS")
+		       (const_int 0))
+		   (const_string "V2DF")
+		   (const_string "DF"))]
+	       (const_string "DF")))])
 
 (define_split
   [(set (match_operand:DF 0 "nonimmediate_operand" "")
@@ -2431,7 +2725,7 @@
   "ix86_expand_move (TFmode, operands); DONE;")
 
 ;; Size of pushdf is 3 (for sub) + 2 (for fstp) + memory operand size.
-;; Size of pushdf using integer insturctions is 3+3*memory operand size
+;; Size of pushdf using integer instructions is 3+3*memory operand size
 ;; Pushing using integer instructions is longer except for constants
 ;; and direct memory references.
 ;; (assuming that any given constant is pushed only once, but this ought to be
@@ -2442,25 +2736,8 @@
 	(match_operand:XF 1 "general_no_elim_operand" "f,Fo,*r"))]
   "!TARGET_64BIT && optimize_size"
 {
-  switch (which_alternative)
-    {
-    case 0:
-      /* %%% We loose REG_DEAD notes for controling pops if we split late.  */
-      operands[0] = gen_rtx_MEM (XFmode, stack_pointer_rtx);
-      operands[2] = stack_pointer_rtx;
-      operands[3] = GEN_INT (12);
-      if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-	return "sub{l}\t{%3, %2|%2, %3}\;fstp%z0\t%y0";
-      else
-	return "sub{l}\t{%3, %2|%2, %3}\;fst%z0\t%y0";
-
-    case 1:
-    case 2:
-      return "#";
-
-    default:
-      abort ();
-    }
+  /* This insn should be already splitted before reg-stack.  */
+  abort ();
 }
   [(set_attr "type" "multi")
    (set_attr "mode" "XF,SI,SI")])
@@ -2470,25 +2747,8 @@
 	(match_operand:TF 1 "general_no_elim_operand" "f,Fo,*r"))]
   "optimize_size"
 {
-  switch (which_alternative)
-    {
-    case 0:
-      /* %%% We loose REG_DEAD notes for controling pops if we split late.  */
-      operands[0] = gen_rtx_MEM (XFmode, stack_pointer_rtx);
-      operands[2] = stack_pointer_rtx;
-      operands[3] = GEN_INT (16);
-      if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-	return "sub{l}\t{%3, %2|%2, %3}\;fstp%z0\t%y0";
-      else
-	return "sub{l}\t{%3, %2|%2, %3}\;fst%z0\t%y0";
-
-    case 1:
-    case 2:
-      return "#";
-
-    default:
-      abort ();
-    }
+  /* This insn should be already splitted before reg-stack.  */
+  abort ();
 }
   [(set_attr "type" "multi")
    (set_attr "mode" "XF,SI,SI")])
@@ -2498,24 +2758,8 @@
 	(match_operand:XF 1 "general_no_elim_operand" "f#r,ro#f"))]
   "!TARGET_64BIT && !optimize_size"
 {
-  switch (which_alternative)
-    {
-    case 0:
-      /* %%% We loose REG_DEAD notes for controling pops if we split late.  */
-      operands[0] = gen_rtx_MEM (XFmode, stack_pointer_rtx);
-      operands[2] = stack_pointer_rtx;
-      operands[3] = GEN_INT (12);
-      if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-	return "sub{l}\t{%3, %2|%2, %3}\;fstp%z0\t%y0";
-      else
-	return "sub{l}\t{%3, %2|%2, %3}\;fst%z0\t%y0";
-
-    case 1:
-      return "#";
-
-    default:
-      abort ();
-    }
+  /* This insn should be already splitted before reg-stack.  */
+  abort ();
 }
   [(set_attr "type" "multi")
    (set_attr "mode" "XF,SI")])
@@ -2525,30 +2769,8 @@
 	(match_operand:TF 1 "general_no_elim_operand" "f#r,rFo#f"))]
   "!optimize_size"
 {
-  switch (which_alternative)
-    {
-    case 0:
-      /* %%% We loose REG_DEAD notes for controling pops if we split late.  */
-      operands[0] = gen_rtx_MEM (XFmode, stack_pointer_rtx);
-      operands[2] = stack_pointer_rtx;
-      operands[3] = GEN_INT (16);
-      if (TARGET_64BIT)
-	if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-	  return "sub{q}\t{%3, %2|%2, %3}\;fstp%z0\t%y0";
-	else
-	  return "sub{q}\t{%3, %2|%2, %3}\;fst%z0\t%y0";
-      else
-	if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-	  return "sub{l}\t{%3, %2|%2, %3}\;fstp%z0\t%y0";
-	else
-	  return "sub{l}\t{%3, %2|%2, %3}\;fst%z0\t%y0";
-
-    case 1:
-      return "#";
-
-    default:
-      abort ();
-    }
+  /* This insn should be already splitted before reg-stack.  */
+  abort ();
 }
   [(set_attr "type" "multi")
    (set_attr "mode" "XF,SI")])
@@ -2601,7 +2823,12 @@
     case 0:
       if (REG_P (operands[1])
           && find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-        return "fstp\t%y0";
+	{
+	  if (REGNO (operands[0]) == FIRST_STACK_REG
+	      && TARGET_USE_FFREEP)
+	    return "ffreep\t%y0";
+          return "fstp\t%y0";
+	}
       else if (STACK_TOP_P (operands[0]))
         return "fld%z1\t%y1";
       else
@@ -2616,14 +2843,7 @@
         return "fstp%z0\t%y0";
 
     case 2:
-      switch (standard_80387_constant_p (operands[1]))
-        {
-        case 1:
-	  return "fldz";
-	case 2:
-	  return "fld1";
-	}
-      break;
+      return standard_80387_constant_opcode (operands[1]);
 
     case 3: case 4:
       return "#";
@@ -2648,7 +2868,12 @@
     case 0:
       if (REG_P (operands[1])
           && find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-        return "fstp\t%y0";
+	{
+	  if (REGNO (operands[0]) == FIRST_STACK_REG
+	      && TARGET_USE_FFREEP)
+	    return "ffreep\t%y0";
+          return "fstp\t%y0";
+	}
       else if (STACK_TOP_P (operands[0]))
         return "fld%z1\t%y1";
       else
@@ -2663,14 +2888,7 @@
         return "fstp%z0\t%y0";
 
     case 2:
-      switch (standard_80387_constant_p (operands[1]))
-        {
-        case 1:
-	  return "fldz";
-	case 2:
-	  return "fld1";
-	}
-      break;
+      return standard_80387_constant_opcode (operands[1]);
 
     case 3: case 4:
       return "#";
@@ -2695,7 +2913,12 @@
     case 0:
       if (REG_P (operands[1])
           && find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-        return "fstp\t%y0";
+	{
+	  if (REGNO (operands[0]) == FIRST_STACK_REG
+	      && TARGET_USE_FFREEP)
+	    return "ffreep\t%y0";
+          return "fstp\t%y0";
+	}
       else if (STACK_TOP_P (operands[0]))
         return "fld%z1\t%y1";
       else
@@ -2710,14 +2933,7 @@
         return "fstp%z0\t%y0";
 
     case 2:
-      switch (standard_80387_constant_p (operands[1]))
-        {
-        case 1:
-	  return "fldz";
-	case 2:
-	  return "fld1";
-	}
-      break;
+      return standard_80387_constant_opcode (operands[1]);
 
     case 3: case 4:
       return "#";
@@ -2742,7 +2958,12 @@
     case 0:
       if (REG_P (operands[1])
           && find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
-        return "fstp\t%y0";
+	{
+	  if (REGNO (operands[0]) == FIRST_STACK_REG
+	      && TARGET_USE_FFREEP)
+	    return "ffreep\t%y0";
+          return "fstp\t%y0";
+	}
       else if (STACK_TOP_P (operands[0]))
         return "fld%z1\t%y1";
       else
@@ -2757,14 +2978,7 @@
         return "fstp%z0\t%y0";
 
     case 2:
-      switch (standard_80387_constant_p (operands[1]))
-        {
-        case 1:
-	  return "fldz";
-	case 2:
-	  return "fld1";
-	}
-      break;
+      return standard_80387_constant_opcode (operands[1]);
 
     case 3: case 4:
       return "#";
@@ -3690,11 +3904,11 @@
    (set_attr "mode" "SF,SF,SF,SF")])
 
 (define_insn "*truncdfsf2_1_sse"
-  [(set (match_operand:SF 0 "nonimmediate_operand" "=*!m,?f#rx,?r#fx,?x#rf,Y")
+  [(set (match_operand:SF 0 "nonimmediate_operand" "=*!m#fxr,?f#xr,?r#fx,?x#fr,Y#fr")
 	(float_truncate:SF
-	 (match_operand:DF 1 "nonimmediate_operand" "f,f,f,f,mY")))
+	 (match_operand:DF 1 "nonimmediate_operand" "f#Y,f#Y,f#Y,f#Y,mY#f")))
    (clobber (match_operand:SF 2 "memory_operand" "=X,m,m,m,X"))]
-  "TARGET_80387 && TARGET_SSE2"
+  "TARGET_80387 && TARGET_SSE2 && !TARGET_SSE_PARTIAL_REGS_FOR_CVTSD2SS"
 {
   switch (which_alternative)
     {
@@ -3704,7 +3918,30 @@
       else
 	return "fst%z0\t%y0";
     case 4:
-      return "cvtsd2ss\t{%1, %0|%0, %1}";
+      return "#";
+    default:
+      abort ();
+    }
+}
+  [(set_attr "type" "fmov,multi,multi,multi,ssecvt")
+   (set_attr "mode" "SF,SF,SF,SF,DF")])
+
+(define_insn "*truncdfsf2_1_sse_nooverlap"
+  [(set (match_operand:SF 0 "nonimmediate_operand" "=*!m,?f#rx,?r#fx,?x#rf,&Y")
+	(float_truncate:SF
+	 (match_operand:DF 1 "nonimmediate_operand" "f#Y,f#Y,f#Y,f#Y,mY#f")))
+   (clobber (match_operand:SF 2 "memory_operand" "=X,m,m,m,X"))]
+  "TARGET_80387 && TARGET_SSE2 && TARGET_SSE_PARTIAL_REGS_FOR_CVTSD2SS"
+{
+  switch (which_alternative)
+    {
+    case 0:
+      if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
+	return "fstp%z0\t%y0";
+      else
+	return "fst%z0\t%y0";
+    case 4:
+      return "#";
     default:
       abort ();
     }
@@ -3713,16 +3950,41 @@
    (set_attr "mode" "SF,SF,SF,SF,DF")])
 
 (define_insn "*truncdfsf2_2"
-  [(set (match_operand:SF 0 "nonimmediate_operand" "=Y,!m")
+  [(set (match_operand:SF 0 "nonimmediate_operand" "=Y,Y,!m")
 	(float_truncate:SF
-	 (match_operand:DF 1 "nonimmediate_operand" "mY,f")))]
-  "TARGET_80387 && TARGET_SSE2
+	 (match_operand:DF 1 "nonimmediate_operand" "Y,mY,f#Y")))]
+  "TARGET_80387 && TARGET_SSE2 && !TARGET_SSE_PARTIAL_REGS_FOR_CVTSD2SS
    && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
 {
   switch (which_alternative)
     {
     case 0:
+    case 1:
       return "cvtsd2ss\t{%1, %0|%0, %1}";
+    case 2:
+      if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
+	return "fstp%z0\t%y0";
+      else
+	return "fst%z0\t%y0";
+    default:
+      abort ();
+    }
+}
+  [(set_attr "type" "ssecvt,ssecvt,fmov")
+   (set_attr "athlon_decode" "vector,double,*")
+   (set_attr "mode" "DF,DF,SF")])
+
+(define_insn "*truncdfsf2_2_nooverlap"
+  [(set (match_operand:SF 0 "nonimmediate_operand" "=&Y,!m")
+	(float_truncate:SF
+	 (match_operand:DF 1 "nonimmediate_operand" "mY,f")))]
+  "TARGET_80387 && TARGET_SSE2 && TARGET_SSE_PARTIAL_REGS_FOR_CVTSD2SS
+   && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
+{
+  switch (which_alternative)
+    {
+    case 0:
+      return "#";
     case 1:
       if (find_regno_note (insn, REG_DEAD, REGNO (operands[1])))
 	return "fstp%z0\t%y0";
@@ -3735,7 +3997,7 @@
   [(set_attr "type" "ssecvt,fmov")
    (set_attr "mode" "DF,SF")])
 
-(define_insn "truncdfsf2_3"
+(define_insn "*truncdfsf2_3"
   [(set (match_operand:SF 0 "memory_operand" "=m")
 	(float_truncate:SF
 	 (match_operand:DF 1 "register_operand" "f")))]
@@ -3750,12 +4012,22 @@
    (set_attr "mode" "SF")])
 
 (define_insn "truncdfsf2_sse_only"
-  [(set (match_operand:SF 0 "register_operand" "=Y")
+  [(set (match_operand:SF 0 "register_operand" "=Y,Y")
 	(float_truncate:SF
-	 (match_operand:DF 1 "nonimmediate_operand" "mY")))]
-  "!TARGET_80387 && TARGET_SSE2"
+	 (match_operand:DF 1 "nonimmediate_operand" "Y,mY")))]
+  "!TARGET_80387 && TARGET_SSE2 && !TARGET_SSE_PARTIAL_REGS_FOR_CVTSD2SS"
   "cvtsd2ss\t{%1, %0|%0, %1}"
   [(set_attr "type" "ssecvt")
+   (set_attr "athlon_decode" "vector,double")
+   (set_attr "mode" "DF")])
+
+(define_insn "*truncdfsf2_sse_only_nooverlap"
+  [(set (match_operand:SF 0 "register_operand" "=&Y")
+	(float_truncate:SF
+	 (match_operand:DF 1 "nonimmediate_operand" "mY")))]
+  "!TARGET_80387 && TARGET_SSE2 && TARGET_SSE_PARTIAL_REGS_FOR_CVTSD2SS"
+  "#"
+  [(set_attr "type" "ssecvt")
    (set_attr "mode" "DF")])
 
 (define_split
@@ -3767,15 +4039,56 @@
   [(set (match_dup 0) (float_truncate:SF (match_dup 1)))]
   "")
 
+; Avoid possible reformatting penalty on the destination by first
+; zeroing it out
 (define_split
-  [(set (match_operand:SF 0 "nonimmediate_operand" "")
+  [(set (match_operand:SF 0 "register_operand" "")
 	(float_truncate:SF
 	 (match_operand:DF 1 "nonimmediate_operand" "")))
    (clobber (match_operand 2 "" ""))]
   "TARGET_80387 && reload_completed
-   && !FP_REG_P (operands[0]) && !FP_REG_P (operands[1])"
-  [(set (match_dup 0) (float_truncate:SF (match_dup 1)))]
-  "")
+   && SSE_REG_P (operands[0])
+   && !STACK_REG_P (operands[1])"
+  [(const_int 0)]
+{
+  rtx src, dest;
+  if (!TARGET_SSE_PARTIAL_REGS_FOR_CVTSD2SS)
+    emit_insn (gen_truncdfsf2_sse_only (operands[0], operands[1]));
+  else
+    {
+      dest = simplify_gen_subreg (V4SFmode, operands[0], SFmode, 0);
+      src = simplify_gen_subreg (V2DFmode, operands[1], DFmode, 0);
+      /* simplify_gen_subreg refuses to widen memory references.  */
+      if (GET_CODE (src) == SUBREG)
+	alter_subreg (&src);
+      if (reg_overlap_mentioned_p (operands[0], operands[1]))
+	abort ();
+      emit_insn (gen_sse_clrv4sf (dest, CONST0_RTX (V4SFmode)));
+      emit_insn (gen_cvtsd2ss (dest, dest, src));
+    }
+  DONE;
+})
+
+(define_split
+  [(set (match_operand:SF 0 "register_operand" "")
+	(float_truncate:SF
+	 (match_operand:DF 1 "nonimmediate_operand" "")))]
+  "TARGET_80387 && reload_completed
+   && SSE_REG_P (operands[0]) && TARGET_SSE_PARTIAL_REGS_FOR_CVTSD2SS"
+  [(const_int 0)]
+{
+  rtx src, dest;
+  dest = simplify_gen_subreg (V4SFmode, operands[0], SFmode, 0);
+  src = simplify_gen_subreg (V2DFmode, operands[1], DFmode, 0);
+  /* simplify_gen_subreg refuses to widen memory references.  */
+  if (GET_CODE (src) == SUBREG)
+    alter_subreg (&src);
+  if (reg_overlap_mentioned_p (operands[0], operands[1]))
+    abort ();
+  emit_insn (gen_sse_clrv4sf (dest, CONST0_RTX (V4SFmode)));
+  emit_insn (gen_cvtsd2ss (dest, dest, src));
+  DONE;
+})
 
 (define_split
   [(set (match_operand:SF 0 "register_operand" "")
@@ -4167,18 +4480,40 @@
 
 ;; When SSE available, it is always faster to use it!
 (define_insn "fix_truncsfdi_sse"
-  [(set (match_operand:DI 0 "register_operand" "=r")
-	(fix:DI (match_operand:SF 1 "nonimmediate_operand" "xm")))]
+  [(set (match_operand:DI 0 "register_operand" "=r,r")
+	(fix:DI (match_operand:SF 1 "nonimmediate_operand" "x,xm")))]
   "TARGET_64BIT && TARGET_SSE"
   "cvttss2si{q}\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecvt")])
+  [(set_attr "type" "sseicvt")
+   (set_attr "athlon_decode" "double,vector")])
+
+;; Avoid vector decoded form of the instruction.
+(define_peephole2
+  [(match_scratch:SF 2 "x")
+   (set (match_operand:DI 0 "register_operand" "")
+	(fix:DI (match_operand:SF 1 "memory_operand" "")))]
+  "TARGET_K8 && !optimize_size"
+  [(set (match_dup 2) (match_dup 1))
+   (set (match_dup 0) (fix:DI (match_dup 2)))]
+  "")
 
 (define_insn "fix_truncdfdi_sse"
-  [(set (match_operand:DI 0 "register_operand" "=r")
-	(fix:DI (match_operand:DF 1 "nonimmediate_operand" "Ym")))]
+  [(set (match_operand:DI 0 "register_operand" "=r,r")
+	(fix:DI (match_operand:DF 1 "nonimmediate_operand" "Y,Ym")))]
   "TARGET_64BIT && TARGET_SSE2"
   "cvttsd2si{q}\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecvt")])
+  [(set_attr "type" "sseicvt,sseicvt")
+   (set_attr "athlon_decode" "double,vector")])
+
+;; Avoid vector decoded form of the instruction.
+(define_peephole2
+  [(match_scratch:DF 2 "Y")
+   (set (match_operand:DI 0 "register_operand" "")
+	(fix:DI (match_operand:DF 1 "memory_operand" "")))]
+  "TARGET_K8 && !optimize_size"
+  [(set (match_dup 2) (match_dup 1))
+   (set (match_dup 0) (fix:DI (match_dup 2)))]
+  "")
 
 ;; Signed conversion to SImode.
 
@@ -4275,18 +4610,40 @@
 
 ;; When SSE available, it is always faster to use it!
 (define_insn "fix_truncsfsi_sse"
-  [(set (match_operand:SI 0 "register_operand" "=r")
-	(fix:SI (match_operand:SF 1 "nonimmediate_operand" "xm")))]
+  [(set (match_operand:SI 0 "register_operand" "=r,r")
+	(fix:SI (match_operand:SF 1 "nonimmediate_operand" "x,xm")))]
   "TARGET_SSE"
   "cvttss2si\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecvt")])
+  [(set_attr "type" "sseicvt")
+   (set_attr "athlon_decode" "double,vector")])
+
+;; Avoid vector decoded form of the instruction.
+(define_peephole2
+  [(match_scratch:SF 2 "x")
+   (set (match_operand:SI 0 "register_operand" "")
+	(fix:SI (match_operand:SF 1 "memory_operand" "")))]
+  "TARGET_K8 && !optimize_size"
+  [(set (match_dup 2) (match_dup 1))
+   (set (match_dup 0) (fix:SI (match_dup 2)))]
+  "")
 
 (define_insn "fix_truncdfsi_sse"
-  [(set (match_operand:SI 0 "register_operand" "=r")
-	(fix:SI (match_operand:DF 1 "nonimmediate_operand" "Ym")))]
+  [(set (match_operand:SI 0 "register_operand" "=r,r")
+	(fix:SI (match_operand:DF 1 "nonimmediate_operand" "Y,Ym")))]
   "TARGET_SSE2"
   "cvttsd2si\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecvt")])
+  [(set_attr "type" "sseicvt")
+   (set_attr "athlon_decode" "double,vector")])
+
+;; Avoid vector decoded form of the instruction.
+(define_peephole2
+  [(match_scratch:DF 2 "Y")
+   (set (match_operand:SI 0 "register_operand" "")
+	(fix:SI (match_operand:DF 1 "memory_operand" "")))]
+  "TARGET_K8 && !optimize_size"
+  [(set (match_dup 2) (match_dup 1))
+   (set (match_dup 0) (fix:SI (match_dup 2)))]
+  "")
 
 (define_split 
   [(set (match_operand:SI 0 "register_operand" "")
@@ -4441,10 +4798,23 @@
 ;; Even though we only accept memory inputs, the backend _really_
 ;; wants to be able to do this between registers.
 
-(define_insn "floathisf2"
+(define_expand "floathisf2"
+  [(set (match_operand:SF 0 "register_operand" "")
+	(float:SF (match_operand:HI 1 "nonimmediate_operand" "")))]
+  "TARGET_SSE || TARGET_80387"
+{
+  if (TARGET_SSE && TARGET_SSE_MATH)
+    {
+      emit_insn (gen_floatsisf2 (operands[0],
+				 convert_to_mode (SImode, operands[1], 0)));
+      DONE;
+    }
+})
+
+(define_insn "*floathisf2_1"
   [(set (match_operand:SF 0 "register_operand" "=f,f")
 	(float:SF (match_operand:HI 1 "nonimmediate_operand" "m,r")))]
-  "TARGET_80387 && !TARGET_SSE"
+  "TARGET_80387 && (!TARGET_SSE || !TARGET_SSE_MATH)"
   "@
    fild%z1\t%1
    #"
@@ -4459,26 +4829,45 @@
   "")
 
 (define_insn "*floatsisf2_i387"
-  [(set (match_operand:SF 0 "register_operand" "=f,?f,x")
-	(float:SF (match_operand:SI 1 "nonimmediate_operand" "m,r,mr")))]
+  [(set (match_operand:SF 0 "register_operand" "=f#x,?f#x,x#f,x#f")
+	(float:SF (match_operand:SI 1 "nonimmediate_operand" "m,r,r,mr")))]
   "TARGET_80387 && (!TARGET_SSE || TARGET_MIX_SSE_I387)"
   "@
    fild%z1\t%1
    #
+   cvtsi2ss\t{%1, %0|%0, %1}
    cvtsi2ss\t{%1, %0|%0, %1}"
-  [(set_attr "type" "fmov,multi,ssecvt")
+  [(set_attr "type" "fmov,multi,sseicvt,sseicvt")
    (set_attr "mode" "SF")
+   (set_attr "athlon_decode" "*,*,vector,double")
    (set_attr "fp_int_src" "true")])
 
 (define_insn "*floatsisf2_sse"
-  [(set (match_operand:SF 0 "register_operand" "=x")
-	(float:SF (match_operand:SI 1 "nonimmediate_operand" "mr")))]
+  [(set (match_operand:SF 0 "register_operand" "=x,x")
+	(float:SF (match_operand:SI 1 "nonimmediate_operand" "r,mr")))]
   "TARGET_SSE"
   "cvtsi2ss\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecvt")
+  [(set_attr "type" "sseicvt")
    (set_attr "mode" "SF")
+   (set_attr "athlon_decode" "vector,double")
    (set_attr "fp_int_src" "true")])
 
+; Avoid possible reformatting penalty on the destination by first
+; zeroing it out
+(define_split
+  [(set (match_operand:SF 0 "register_operand" "")
+	(float:SF (match_operand:SI 1 "nonimmediate_operand" "")))]
+  "TARGET_80387 && reload_completed && TARGET_SSE_PARTIAL_REGS
+   && SSE_REG_P (operands[0])"
+  [(const_int 0)]
+{
+  rtx dest;
+  dest = simplify_gen_subreg (V4SFmode, operands[0], SFmode, 0);
+  emit_insn (gen_sse_clrv4sf (dest, CONST0_RTX (V4SFmode)));
+  emit_insn (gen_cvtsi2ss (dest, dest, operands[1]));
+  DONE;
+})
+
 (define_expand "floatdisf2"
   [(set (match_operand:SF 0 "register_operand" "")
 	(float:SF (match_operand:DI 1 "nonimmediate_operand" "")))]
@@ -4497,30 +4886,62 @@
    (set_attr "fp_int_src" "true")])
 
 (define_insn "*floatdisf2_i387"
-  [(set (match_operand:SF 0 "register_operand" "=f,?f,x")
-	(float:SF (match_operand:DI 1 "nonimmediate_operand" "m,r,mr")))]
+  [(set (match_operand:SF 0 "register_operand" "=f#x,?f#x,x#f,x#f")
+	(float:SF (match_operand:DI 1 "nonimmediate_operand" "m,r,r,mr")))]
   "TARGET_64BIT && TARGET_80387 && (!TARGET_SSE || TARGET_MIX_SSE_I387)"
   "@
    fild%z1\t%1
    #
+   cvtsi2ss{q}\t{%1, %0|%0, %1}
    cvtsi2ss{q}\t{%1, %0|%0, %1}"
-  [(set_attr "type" "fmov,multi,ssecvt")
+  [(set_attr "type" "fmov,multi,sseicvt,sseicvt")
    (set_attr "mode" "SF")
+   (set_attr "athlon_decode" "*,*,vector,double")
    (set_attr "fp_int_src" "true")])
 
 (define_insn "*floatdisf2_sse"
-  [(set (match_operand:SF 0 "register_operand" "=x")
-	(float:SF (match_operand:DI 1 "nonimmediate_operand" "mr")))]
+  [(set (match_operand:SF 0 "register_operand" "=x,x")
+	(float:SF (match_operand:DI 1 "nonimmediate_operand" "r,mr")))]
   "TARGET_64BIT && TARGET_SSE"
   "cvtsi2ss{q}\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecvt")
+  [(set_attr "type" "sseicvt")
    (set_attr "mode" "SF")
+   (set_attr "athlon_decode" "vector,double")
    (set_attr "fp_int_src" "true")])
 
-(define_insn "floathidf2"
+; Avoid possible reformatting penalty on the destination by first
+; zeroing it out
+(define_split
+  [(set (match_operand:SF 0 "register_operand" "")
+	(float:SF (match_operand:DI 1 "nonimmediate_operand" "")))]
+  "TARGET_80387 && reload_completed && TARGET_SSE_PARTIAL_REGS
+   && SSE_REG_P (operands[0])"
+  [(const_int 0)]
+{
+  rtx dest;
+  dest = simplify_gen_subreg (V4SFmode, operands[0], SFmode, 0);
+  emit_insn (gen_sse_clrv4sf (dest, CONST0_RTX (V4SFmode)));
+  emit_insn (gen_cvtsi2ssq (dest, dest, operands[1]));
+  DONE;
+})
+
+(define_expand "floathidf2"
+  [(set (match_operand:DF 0 "register_operand" "")
+	(float:DF (match_operand:HI 1 "nonimmediate_operand" "")))]
+  "TARGET_SSE2 || TARGET_80387"
+{
+  if (TARGET_SSE && TARGET_SSE_MATH)
+    {
+      emit_insn (gen_floatsidf2 (operands[0],
+				 convert_to_mode (SImode, operands[1], 0)));
+      DONE;
+    }
+})
+
+(define_insn "*floathidf2_1"
   [(set (match_operand:DF 0 "register_operand" "=f,f")
 	(float:DF (match_operand:HI 1 "nonimmediate_operand" "m,r")))]
-  "TARGET_80387 && !TARGET_SSE2"
+  "TARGET_80387 && (!TARGET_SSE2 || !TARGET_SSE_MATH)"
   "@
    fild%z1\t%1
    #"
@@ -4535,24 +4956,27 @@
   "")
 
 (define_insn "*floatsidf2_i387"
-  [(set (match_operand:DF 0 "register_operand" "=f,?f,Y")
-	(float:DF (match_operand:SI 1 "nonimmediate_operand" "m,r,mr")))]
+  [(set (match_operand:DF 0 "register_operand" "=f#Y,?f#Y,Y#f,Y#f")
+	(float:DF (match_operand:SI 1 "nonimmediate_operand" "m,r,r,mr")))]
   "TARGET_80387 && (!TARGET_SSE2 || TARGET_MIX_SSE_I387)"
   "@
    fild%z1\t%1
    #
+   cvtsi2sd\t{%1, %0|%0, %1}
    cvtsi2sd\t{%1, %0|%0, %1}"
-  [(set_attr "type" "fmov,multi,ssecvt")
+  [(set_attr "type" "fmov,multi,sseicvt,sseicvt")
    (set_attr "mode" "DF")
+   (set_attr "athlon_decode" "*,*,double,direct")
    (set_attr "fp_int_src" "true")])
 
 (define_insn "*floatsidf2_sse"
-  [(set (match_operand:DF 0 "register_operand" "=Y")
-	(float:DF (match_operand:SI 1 "nonimmediate_operand" "mr")))]
+  [(set (match_operand:DF 0 "register_operand" "=Y,Y")
+	(float:DF (match_operand:SI 1 "nonimmediate_operand" "r,mr")))]
   "TARGET_SSE2"
   "cvtsi2sd\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecvt")
+  [(set_attr "type" "sseicvt")
    (set_attr "mode" "DF")
+   (set_attr "athlon_decode" "double,direct")
    (set_attr "fp_int_src" "true")])
 
 (define_expand "floatdidf2"
@@ -4573,24 +4997,27 @@
    (set_attr "fp_int_src" "true")])
 
 (define_insn "*floatdidf2_i387"
-  [(set (match_operand:DF 0 "register_operand" "=f,?f,Y")
-	(float:DF (match_operand:DI 1 "nonimmediate_operand" "m,r,mr")))]
+  [(set (match_operand:DF 0 "register_operand" "=f#Y,?f#Y,Y#f,Y#f")
+	(float:DF (match_operand:DI 1 "nonimmediate_operand" "m,r,r,mr")))]
   "TARGET_64BIT && TARGET_80387 && (!TARGET_SSE2 || TARGET_MIX_SSE_I387)"
   "@
    fild%z1\t%1
    #
+   cvtsi2sd{q}\t{%1, %0|%0, %1}
    cvtsi2sd{q}\t{%1, %0|%0, %1}"
-  [(set_attr "type" "fmov,multi,ssecvt")
+  [(set_attr "type" "fmov,multi,sseicvt,sseicvt")
    (set_attr "mode" "DF")
+   (set_attr "athlon_decode" "*,*,double,direct")
    (set_attr "fp_int_src" "true")])
 
 (define_insn "*floatdidf2_sse"
-  [(set (match_operand:DF 0 "register_operand" "=Y")
-	(float:DF (match_operand:DI 1 "nonimmediate_operand" "mr")))]
+  [(set (match_operand:DF 0 "register_operand" "=Y,Y")
+	(float:DF (match_operand:DI 1 "nonimmediate_operand" "r,mr")))]
   "TARGET_SSE2"
   "cvtsi2sd{q}\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecvt")
+  [(set_attr "type" "sseicvt")
    (set_attr "mode" "DF")
+   (set_attr "athlon_decode" "double,direct")
    (set_attr "fp_int_src" "true")])
 
 (define_insn "floathixf2"
@@ -4672,6 +5099,24 @@
   ix86_free_from_memory (GET_MODE (operands[1]));
   DONE;
 })
+
+(define_expand "floatunssisf2"
+  [(use (match_operand:SF 0 "register_operand" ""))
+   (use (match_operand:SI 1 "register_operand" ""))]
+  "TARGET_SSE && TARGET_SSE_MATH && !TARGET_64BIT"
+  "x86_emit_floatuns (operands); DONE;")
+
+(define_expand "floatunsdisf2"
+  [(use (match_operand:SF 0 "register_operand" ""))
+   (use (match_operand:DI 1 "register_operand" ""))]
+  "TARGET_SSE && TARGET_SSE_MATH && TARGET_64BIT"
+  "x86_emit_floatuns (operands); DONE;")
+
+(define_expand "floatunsdidf2"
+  [(use (match_operand:DF 0 "register_operand" ""))
+   (use (match_operand:DI 1 "register_operand" ""))]
+  "TARGET_SSE2 && TARGET_SSE_MATH && TARGET_64BIT"
+  "x86_emit_floatuns (operands); DONE;")
 
 ;; Add instructions
 
@@ -4714,9 +5159,9 @@
    split_di (operands+1, 1, operands+1, operands+4);
    split_di (operands+2, 1, operands+2, operands+5);")
 
-(define_insn "*adddi3_carry_rex64"
+(define_insn "adddi3_carry_rex64"
   [(set (match_operand:DI 0 "nonimmediate_operand" "=rm,r")
-	  (plus:DI (plus:DI (ltu:DI (reg:CC 17) (const_int 0))
+	  (plus:DI (plus:DI (match_operand:DI 3 "ix86_carry_flag_operator" "")
 			    (match_operand:DI 1 "nonimmediate_operand" "%0,0"))
 		   (match_operand:DI 2 "x86_64_general_operand" "re,rm")))
    (clobber (reg:CC 17))]
@@ -4739,9 +5184,35 @@
   [(set_attr "type" "alu")
    (set_attr "mode" "DI")])
 
-(define_insn "*addsi3_carry"
+(define_insn "addqi3_carry"
+  [(set (match_operand:QI 0 "nonimmediate_operand" "=rm,r")
+	  (plus:QI (plus:QI (match_operand:QI 3 "ix86_carry_flag_operator" "")
+			    (match_operand:QI 1 "nonimmediate_operand" "%0,0"))
+		   (match_operand:QI 2 "general_operand" "ri,rm")))
+   (clobber (reg:CC 17))]
+  "ix86_binary_operator_ok (PLUS, QImode, operands)"
+  "adc{b}\t{%2, %0|%0, %2}"
+  [(set_attr "type" "alu")
+   (set_attr "pent_pair" "pu")
+   (set_attr "mode" "QI")
+   (set_attr "ppro_uops" "few")])
+
+(define_insn "addhi3_carry"
+  [(set (match_operand:HI 0 "nonimmediate_operand" "=rm,r")
+	  (plus:HI (plus:HI (match_operand:HI 3 "ix86_carry_flag_operator" "")
+			    (match_operand:HI 1 "nonimmediate_operand" "%0,0"))
+		   (match_operand:HI 2 "general_operand" "ri,rm")))
+   (clobber (reg:CC 17))]
+  "ix86_binary_operator_ok (PLUS, HImode, operands)"
+  "adc{w}\t{%2, %0|%0, %2}"
+  [(set_attr "type" "alu")
+   (set_attr "pent_pair" "pu")
+   (set_attr "mode" "HI")
+   (set_attr "ppro_uops" "few")])
+
+(define_insn "addsi3_carry"
   [(set (match_operand:SI 0 "nonimmediate_operand" "=rm,r")
-	  (plus:SI (plus:SI (ltu:SI (reg:CC 17) (const_int 0))
+	  (plus:SI (plus:SI (match_operand:SI 3 "ix86_carry_flag_operator" "")
 			    (match_operand:SI 1 "nonimmediate_operand" "%0,0"))
 		   (match_operand:SI 2 "general_operand" "ri,rm")))
    (clobber (reg:CC 17))]
@@ -4755,7 +5226,7 @@
 (define_insn "*addsi3_carry_zext"
   [(set (match_operand:DI 0 "register_operand" "=r")
 	  (zero_extend:DI 
-	    (plus:SI (plus:SI (ltu:SI (reg:CC 17) (const_int 0))
+	    (plus:SI (plus:SI (match_operand:SI 3 "ix86_carry_flag_operator" "")
 			      (match_operand:SI 1 "nonimmediate_operand" "%0"))
 		     (match_operand:SI 2 "general_operand" "rim"))))
    (clobber (reg:CC 17))]
@@ -5088,7 +5559,7 @@
       if (! rtx_equal_p (operands[0], operands[1]))
 	abort ();
       /* ???? We ought to handle there the 32bit case too
-	 - do we need new constrant?  */
+	 - do we need new constraint?  */
       /* Make things pretty and `subl $4,%eax' rather than `addl $-4, %eax'.
 	 Exceptions: -128 encodes smaller than 128, so swap sign and op.  */
       if (GET_CODE (operands[2]) == CONST_INT
@@ -5138,7 +5609,7 @@
       if (! rtx_equal_p (operands[0], operands[1]))
 	abort ();
       /* ???? We ought to handle there the 32bit case too
-	 - do we need new constrant?  */
+	 - do we need new constraint?  */
       /* Make things pretty and `subl $4,%eax' rather than `addl $-4, %eax'.
 	 Exceptions: -128 encodes smaller than 128, so swap sign and op.  */
       if (GET_CODE (operands[2]) == CONST_INT
@@ -5398,7 +5869,7 @@
 	  (plus:SI (match_operand:SI 1 "register_operand" "")
 		   (match_operand:SI 2 "nonmemory_operand" ""))))
    (clobber (reg:CC 17))]
-  "reload_completed
+  "TARGET_64BIT && reload_completed
    && true_regnum (operands[0]) != true_regnum (operands[1])"
   [(set (match_dup 0)
 	(zero_extend:DI (subreg:SI (plus:DI (match_dup 1) (match_dup 2)) 0)))]
@@ -5588,7 +6059,7 @@
 	(const_string "alu")))
    (set_attr "mode" "SI")])
 
-; For comparisons agains 1, -1 and 128, we may generate better code
+; For comparisons against 1, -1 and 128, we may generate better code
 ; by converting cmp to add, inc or dec as done by peephole2.  This pattern
 ; is matched then.  We can't accept general immediate, because for
 ; case of overflows,  the result is messed up.
@@ -6370,7 +6841,7 @@
 (define_insn "subdi3_carry_rex64"
   [(set (match_operand:DI 0 "nonimmediate_operand" "=rm,r")
 	  (minus:DI (match_operand:DI 1 "nonimmediate_operand" "0,0")
-	    (plus:DI (ltu:DI (reg:CC 17) (const_int 0))
+	    (plus:DI (match_operand:DI 3 "ix86_carry_flag_operator" "")
 	       (match_operand:DI 2 "x86_64_general_operand" "re,rm"))))
    (clobber (reg:CC 17))]
   "TARGET_64BIT && ix86_binary_operator_ok (MINUS, DImode, operands)"
@@ -6416,11 +6887,36 @@
   [(set_attr "type" "alu")
    (set_attr "mode" "DI")])
 
+(define_insn "subqi3_carry"
+  [(set (match_operand:QI 0 "nonimmediate_operand" "=rm,r")
+	  (minus:QI (match_operand:QI 1 "nonimmediate_operand" "0,0")
+	    (plus:QI (match_operand:QI 3 "ix86_carry_flag_operator" "")
+	       (match_operand:QI 2 "general_operand" "ri,rm"))))
+   (clobber (reg:CC 17))]
+  "ix86_binary_operator_ok (MINUS, QImode, operands)"
+  "sbb{b}\t{%2, %0|%0, %2}"
+  [(set_attr "type" "alu")
+   (set_attr "pent_pair" "pu")
+   (set_attr "ppro_uops" "few")
+   (set_attr "mode" "QI")])
+
+(define_insn "subhi3_carry"
+  [(set (match_operand:HI 0 "nonimmediate_operand" "=rm,r")
+	  (minus:HI (match_operand:HI 1 "nonimmediate_operand" "0,0")
+	    (plus:HI (match_operand:HI 3 "ix86_carry_flag_operator" "")
+	       (match_operand:HI 2 "general_operand" "ri,rm"))))
+   (clobber (reg:CC 17))]
+  "ix86_binary_operator_ok (MINUS, HImode, operands)"
+  "sbb{w}\t{%2, %0|%0, %2}"
+  [(set_attr "type" "alu")
+   (set_attr "pent_pair" "pu")
+   (set_attr "ppro_uops" "few")
+   (set_attr "mode" "HI")])
 
 (define_insn "subsi3_carry"
   [(set (match_operand:SI 0 "nonimmediate_operand" "=rm,r")
 	  (minus:SI (match_operand:SI 1 "nonimmediate_operand" "0,0")
-	    (plus:SI (ltu:SI (reg:CC 17) (const_int 0))
+	    (plus:SI (match_operand:SI 3 "ix86_carry_flag_operator" "")
 	       (match_operand:SI 2 "general_operand" "ri,rm"))))
    (clobber (reg:CC 17))]
   "ix86_binary_operator_ok (MINUS, SImode, operands)"
@@ -6434,7 +6930,7 @@
   [(set (match_operand:DI 0 "register_operand" "=rm,r")
 	  (zero_extend:DI
 	    (minus:SI (match_operand:SI 1 "register_operand" "0,0")
-	      (plus:SI (ltu:SI (reg:CC 17) (const_int 0))
+	      (plus:SI (match_operand:SI 3 "ix86_carry_flag_operator" "")
 		 (match_operand:SI 2 "general_operand" "ri,rm")))))
    (clobber (reg:CC 17))]
   "TARGET_64BIT && ix86_binary_operator_ok (MINUS, SImode, operands)"
@@ -6670,7 +7166,7 @@
 
 (define_insn "*muldi3_1_rex64"
   [(set (match_operand:DI 0 "register_operand" "=r,r,r")
-	(mult:DI (match_operand:DI 1 "nonimmediate_operand" "%rm,0,0")
+	(mult:DI (match_operand:DI 1 "nonimmediate_operand" "%rm,rm,0")
 		 (match_operand:DI 2 "x86_64_general_operand" "K,e,mr")))
    (clobber (reg:CC 17))]
   "TARGET_64BIT
@@ -6681,6 +7177,15 @@
    imul{q}\t{%2, %0|%0, %2}"
   [(set_attr "type" "imul")
    (set_attr "prefix_0f" "0,0,1")
+   (set (attr "athlon_decode")
+	(cond [(eq_attr "cpu" "athlon")
+		  (const_string "vector")
+	       (eq_attr "alternative" "1")
+		  (const_string "vector")
+	       (and (eq_attr "alternative" "2")
+		    (match_operand 1 "memory_operand" ""))
+		  (const_string "vector")]
+	      (const_string "direct")))
    (set_attr "mode" "DI")])
 
 (define_expand "mulsi3"
@@ -6693,56 +7198,50 @@
 
 (define_insn "*mulsi3_1"
   [(set (match_operand:SI 0 "register_operand" "=r,r,r")
-	(mult:SI (match_operand:SI 1 "nonimmediate_operand" "%rm,0,0")
+	(mult:SI (match_operand:SI 1 "nonimmediate_operand" "%rm,rm,0")
 		 (match_operand:SI 2 "general_operand" "K,i,mr")))
    (clobber (reg:CC 17))]
   "GET_CODE (operands[1]) != MEM || GET_CODE (operands[2]) != MEM"
-  ; For the {r,0,i} alternative (i.e., register <- register * immediate),
-  ; there are two ways of writing the exact same machine instruction
-  ; in assembly language.  One, for example, is:
-  ;
-  ;   imul $12, %eax
-  ;
-  ; while the other is:
-  ;
-  ;   imul $12, %eax, %eax
-  ;
-  ; The first is simply short-hand for the latter.  But, some assemblers,
-  ; like the SCO OSR5 COFF assembler, don't handle the first form.
   "@
    imul{l}\t{%2, %1, %0|%0, %1, %2}
    imul{l}\t{%2, %1, %0|%0, %1, %2}
    imul{l}\t{%2, %0|%0, %2}"
   [(set_attr "type" "imul")
    (set_attr "prefix_0f" "0,0,1")
+   (set (attr "athlon_decode")
+	(cond [(eq_attr "cpu" "athlon")
+		  (const_string "vector")
+	       (eq_attr "alternative" "1")
+		  (const_string "vector")
+	       (and (eq_attr "alternative" "2")
+		    (match_operand 1 "memory_operand" ""))
+		  (const_string "vector")]
+	      (const_string "direct")))
    (set_attr "mode" "SI")])
 
 (define_insn "*mulsi3_1_zext"
   [(set (match_operand:DI 0 "register_operand" "=r,r,r")
 	(zero_extend:DI
-	  (mult:SI (match_operand:SI 1 "nonimmediate_operand" "%rm,0,0")
+	  (mult:SI (match_operand:SI 1 "nonimmediate_operand" "%rm,rm,0")
 		   (match_operand:SI 2 "general_operand" "K,i,mr"))))
    (clobber (reg:CC 17))]
   "TARGET_64BIT
    && (GET_CODE (operands[1]) != MEM || GET_CODE (operands[2]) != MEM)"
-  ; For the {r,0,i} alternative (i.e., register <- register * immediate),
-  ; there are two ways of writing the exact same machine instruction
-  ; in assembly language.  One, for example, is:
-  ;
-  ;   imul $12, %eax
-  ;
-  ; while the other is:
-  ;
-  ;   imul $12, %eax, %eax
-  ;
-  ; The first is simply short-hand for the latter.  But, some assemblers,
-  ; like the SCO OSR5 COFF assembler, don't handle the first form.
   "@
    imul{l}\t{%2, %1, %k0|%k0, %1, %2}
    imul{l}\t{%2, %1, %k0|%k0, %1, %2}
    imul{l}\t{%2, %k0|%k0, %2}"
   [(set_attr "type" "imul")
    (set_attr "prefix_0f" "0,0,1")
+   (set (attr "athlon_decode")
+	(cond [(eq_attr "cpu" "athlon")
+		  (const_string "vector")
+	       (eq_attr "alternative" "1")
+		  (const_string "vector")
+	       (and (eq_attr "alternative" "2")
+		    (match_operand 1 "memory_operand" ""))
+		  (const_string "vector")]
+	      (const_string "direct")))
    (set_attr "mode" "SI")])
 
 (define_expand "mulhi3"
@@ -6755,18 +7254,22 @@
 
 (define_insn "*mulhi3_1"
   [(set (match_operand:HI 0 "register_operand" "=r,r,r")
-	(mult:HI (match_operand:HI 1 "nonimmediate_operand" "%rm,0,0")
+	(mult:HI (match_operand:HI 1 "nonimmediate_operand" "%rm,rm,0")
 		 (match_operand:HI 2 "general_operand" "K,i,mr")))
    (clobber (reg:CC 17))]
   "GET_CODE (operands[1]) != MEM || GET_CODE (operands[2]) != MEM"
-  ; %%% There was a note about "Assembler has weird restrictions",
-  ; concerning alternative 1 when op1 == op0.  True?
   "@
    imul{w}\t{%2, %1, %0|%0, %1, %2}
    imul{w}\t{%2, %1, %0|%0, %1, %2}
    imul{w}\t{%2, %0|%0, %2}"
   [(set_attr "type" "imul")
    (set_attr "prefix_0f" "0,0,1")
+   (set (attr "athlon_decode")
+	(cond [(eq_attr "cpu" "athlon")
+		  (const_string "vector")
+	       (eq_attr "alternative" "1,2")
+		  (const_string "vector")]
+	      (const_string "direct")))
    (set_attr "mode" "HI")])
 
 (define_expand "mulqi3"
@@ -6787,6 +7290,10 @@
   "mul{b}\t%2"
   [(set_attr "type" "imul")
    (set_attr "length_immediate" "0")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "direct")))
    (set_attr "mode" "QI")])
 
 (define_expand "umulqihi3"
@@ -6809,6 +7316,10 @@
   "mul{b}\t%2"
   [(set_attr "type" "imul")
    (set_attr "length_immediate" "0")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "direct")))
    (set_attr "mode" "QI")])
 
 (define_expand "mulqihi3"
@@ -6829,6 +7340,10 @@
   "imul{b}\t%2"
   [(set_attr "type" "imul")
    (set_attr "length_immediate" "0")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "direct")))
    (set_attr "mode" "QI")])
 
 (define_expand "umulditi3"
@@ -6852,6 +7367,10 @@
   [(set_attr "type" "imul")
    (set_attr "ppro_uops" "few")
    (set_attr "length_immediate" "0")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "double")))
    (set_attr "mode" "DI")])
 
 ;; We can't use this pattern in 64bit mode, since it results in two separate 32bit registers
@@ -6876,6 +7395,10 @@
   [(set_attr "type" "imul")
    (set_attr "ppro_uops" "few")
    (set_attr "length_immediate" "0")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "double")))
    (set_attr "mode" "SI")])
 
 (define_expand "mulditi3"
@@ -6898,6 +7421,10 @@
   "imul{q}\t%2"
   [(set_attr "type" "imul")
    (set_attr "length_immediate" "0")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "double")))
    (set_attr "mode" "DI")])
 
 (define_expand "mulsidi3"
@@ -6920,6 +7447,10 @@
   "imul{l}\t%2"
   [(set_attr "type" "imul")
    (set_attr "length_immediate" "0")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "double")))
    (set_attr "mode" "SI")])
 
 (define_expand "umuldi3_highpart"
@@ -6953,6 +7484,10 @@
   [(set_attr "type" "imul")
    (set_attr "ppro_uops" "few")
    (set_attr "length_immediate" "0")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "double")))
    (set_attr "mode" "DI")])
 
 (define_expand "umulsi3_highpart"
@@ -6985,6 +7520,10 @@
   [(set_attr "type" "imul")
    (set_attr "ppro_uops" "few")
    (set_attr "length_immediate" "0")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "double")))
    (set_attr "mode" "SI")])
 
 (define_insn "*umulsi3_highpart_zext"
@@ -7004,6 +7543,10 @@
   [(set_attr "type" "imul")
    (set_attr "ppro_uops" "few")
    (set_attr "length_immediate" "0")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "double")))
    (set_attr "mode" "SI")])
 
 (define_expand "smuldi3_highpart"
@@ -7036,6 +7579,10 @@
   "imul{q}\t%2"
   [(set_attr "type" "imul")
    (set_attr "ppro_uops" "few")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "double")))
    (set_attr "mode" "DI")])
 
 (define_expand "smulsi3_highpart"
@@ -7067,6 +7614,10 @@
   "imul{l}\t%2"
   [(set_attr "type" "imul")
    (set_attr "ppro_uops" "few")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "double")))
    (set_attr "mode" "SI")])
 
 (define_insn "*smulsi3_highpart_zext"
@@ -7085,6 +7636,10 @@
   "imul{l}\t%2"
   [(set_attr "type" "imul")
    (set_attr "ppro_uops" "few")
+   (set (attr "athlon_decode")
+     (if_then_else (eq_attr "cpu" "athlon")
+        (const_string "vector")
+        (const_string "double")))
    (set_attr "mode" "SI")])
 
 ;; The patterns that match these are at the end of this file.
@@ -7184,7 +7739,7 @@
   "")
 
 ;; Allow to come the parameter in eax or edx to avoid extra moves.
-;; Penalize eax case sligthly because it results in worse scheduling
+;; Penalize eax case slightly because it results in worse scheduling
 ;; of code.
 (define_insn "*divmoddi4_nocltd_rex64"
   [(set (match_operand:DI 0 "register_operand" "=&a,?a")
@@ -7269,7 +7824,7 @@
   "")
 
 ;; Allow to come the parameter in eax or edx to avoid extra moves.
-;; Penalize eax case sligthly because it results in worse scheduling
+;; Penalize eax case slightly because it results in worse scheduling
 ;; of code.
 (define_insn "*divmodsi4_nocltd"
   [(set (match_operand:SI 0 "register_operand" "=&a,?a")
@@ -7497,10 +8052,11 @@
 (define_insn "*testdi_1_rex64"
   [(set (reg 17)
 	(compare
-	  (and:DI (match_operand:DI 0 "nonimmediate_operand" "%*a,r,*a,r,rm")
-		  (match_operand:DI 1 "x86_64_szext_nonmemory_operand" "Z,Z,e,e,re"))
+	  (and:DI (match_operand:DI 0 "nonimmediate_operand" "%!*a,r,!*a,r,rm")
+		  (match_operand:DI 1 "x86_64_szext_general_operand" "Z,Z,e,e,re"))
 	  (const_int 0)))]
-  "TARGET_64BIT && ix86_match_ccmode (insn, CCNOmode)"
+  "TARGET_64BIT && ix86_match_ccmode (insn, CCNOmode)
+   && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
   "@
    test{l}\t{%k1, %k0|%k0, %k1} 
    test{l}\t{%k1, %k0|%k0, %k1} 
@@ -7515,10 +8071,11 @@
 (define_insn "testsi_1"
   [(set (reg 17)
 	(compare
-	  (and:SI (match_operand:SI 0 "nonimmediate_operand" "%*a,r,rm")
-		  (match_operand:SI 1 "nonmemory_operand" "in,in,rin"))
+	  (and:SI (match_operand:SI 0 "nonimmediate_operand" "%!*a,r,rm")
+		  (match_operand:SI 1 "general_operand" "in,in,rin"))
 	  (const_int 0)))]
-  "ix86_match_ccmode (insn, CCNOmode)"
+  "ix86_match_ccmode (insn, CCNOmode)
+   && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
   "test{l}\t{%1, %0|%0, %1}"
   [(set_attr "type" "test")
    (set_attr "modrm" "0,1,1")
@@ -7536,10 +8093,11 @@
 
 (define_insn "*testhi_1"
   [(set (reg 17)
-        (compare (and:HI (match_operand:HI 0 "nonimmediate_operand" "%*a,r,rm")
-			 (match_operand:HI 1 "nonmemory_operand" "n,n,rn"))
+        (compare (and:HI (match_operand:HI 0 "nonimmediate_operand" "%!*a,r,rm")
+			 (match_operand:HI 1 "general_operand" "n,n,rn"))
 		 (const_int 0)))]
-  "ix86_match_ccmode (insn, CCNOmode)"
+  "ix86_match_ccmode (insn, CCNOmode)
+   && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
   "test{w}\t{%1, %0|%0, %1}"
   [(set_attr "type" "test")
    (set_attr "modrm" "0,1,1")
@@ -7556,10 +8114,11 @@
 
 (define_insn "*testqi_1"
   [(set (reg 17)
-        (compare (and:QI (match_operand:QI 0 "nonimmediate_operand" "%*a,q,qm,r")
-			 (match_operand:QI 1 "nonmemory_operand" "n,n,qn,n"))
+        (compare (and:QI (match_operand:QI 0 "nonimmediate_operand" "%!*a,q,qm,r")
+			 (match_operand:QI 1 "general_operand" "n,n,qn,n"))
 		 (const_int 0)))]
-  "ix86_match_ccmode (insn, CCNOmode)"
+  "ix86_match_ccmode (insn, CCNOmode)
+   && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
 {
   if (which_alternative == 3)
     {
@@ -7614,9 +8173,10 @@
 	      (const_int 8)
 	      (const_int 8))
 	    (zero_extend:SI
-	      (match_operand:QI 1 "nonimmediate_operand" "Qm")))
+	      (match_operand:QI 1 "general_operand" "Qm")))
 	  (const_int 0)))]
-  "!TARGET_64BIT && ix86_match_ccmode (insn, CCNOmode)"
+  "!TARGET_64BIT && ix86_match_ccmode (insn, CCNOmode)
+   && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
   "test{b}\t{%1, %h0|%h0, %1}"
   [(set_attr "type" "test")
    (set_attr "mode" "QI")])
@@ -7742,7 +8302,7 @@
 ;; Convert HImode/SImode test instructions with immediate to QImode ones.
 ;; i386 does not allow to encode test with 8bit sign extended immediate, so
 ;; this is relatively important trick.
-;; Do the converison only post-reload to avoid limiting of the register class
+;; Do the conversion only post-reload to avoid limiting of the register class
 ;; to QI regs.
 (define_split
   [(set (reg 17)
@@ -8206,7 +8766,7 @@
 
 ;; Convert wide AND instructions with immediate operand to shorter QImode
 ;; equivalents when possible.
-;; Don't do the splitting with memory operands, since it intoduces risc
+;; Don't do the splitting with memory operands, since it introduces risk
 ;; of memory mismatch stalls.  We may want to do the splitting for optimizing
 ;; for size, but that can (should?) be handled by generic code instead.
 (define_split
@@ -9260,12 +9820,15 @@
 	     in register.  */
 	  rtx reg = gen_reg_rtx (SFmode);
 	  rtx dest = operands[0];
+	  rtx imm = gen_lowpart (SFmode, gen_int_mode (0x80000000, SImode));
 
 	  operands[1] = force_reg (SFmode, operands[1]);
 	  operands[0] = force_reg (SFmode, operands[0]);
-	  emit_move_insn (reg,
-			  gen_lowpart (SFmode,
-				       gen_int_mode (0x80000000, SImode)));
+	  reg = force_reg (V4SFmode,
+			   gen_rtx_CONST_VECTOR (V4SFmode,
+			     gen_rtvec (4, imm, CONST0_RTX (SFmode),
+					CONST0_RTX (SFmode),
+					CONST0_RTX (SFmode))));
 	  emit_insn (gen_negsf2_ifs (operands[0], operands[1], reg));
 	  if (dest != operands[0])
 	    emit_move_insn (dest, operands[0]);
@@ -9284,7 +9847,7 @@
 (define_insn "negsf2_ifs"
   [(set (match_operand:SF 0 "nonimmediate_operand" "=x#fr,x#fr,f#xr,rm#xf")
 	(neg:SF (match_operand:SF 1 "nonimmediate_operand" "0,x#fr,0,0")))
-   (use (match_operand:SF 2 "nonmemory_operand" "x,0#x,*g#x,*g#x"))
+   (use (match_operand:V4SF 2 "nonimmediate_operand" "xm,0,xm*r,xm*r"))
    (clobber (reg:CC 17))]
   "TARGET_SSE
    && (reload_in_progress || reload_completed
@@ -9305,7 +9868,7 @@
 (define_split
   [(set (match_operand:SF 0 "register_operand" "")
 	(neg:SF (match_operand:SF 1 "register_operand" "")))
-   (use (match_operand:SF 2 "" ""))
+   (use (match_operand:V4SF 2 "" ""))
    (clobber (reg:CC 17))]
   "reload_completed && !SSE_REG_P (operands[0])"
   [(parallel [(set (match_dup 0)
@@ -9315,13 +9878,15 @@
 (define_split
   [(set (match_operand:SF 0 "register_operand" "")
 	(neg:SF (match_operand:SF 1 "register_operand" "")))
-   (use (match_operand:SF 2 "register_operand" ""))
+   (use (match_operand:V4SF 2 "nonimmediate_operand" ""))
    (clobber (reg:CC 17))]
   "reload_completed && SSE_REG_P (operands[0])"
   [(set (subreg:TI (match_dup 0) 0)
-	(xor:TI (subreg:TI (match_dup 1) 0)
-		(subreg:TI (match_dup 2) 0)))]
+	(xor:TI (match_dup 1)
+		(match_dup 2)))]
 {
+  operands[1] = simplify_gen_subreg (TImode, operands[1], SFmode, 0);
+  operands[2] = simplify_gen_subreg (TImode, operands[2], V4SFmode, 0);
   if (operands_match_p (operands[0], operands[2]))
     {
       rtx tmp;
@@ -9360,7 +9925,7 @@
   [(parallel [(set (match_dup 0) (xor:SI (match_dup 0) (match_dup 1)))
 	      (clobber (reg:CC 17))])]
   "operands[1] = gen_int_mode (0x80000000, SImode);
-   operands[0] = gen_rtx_REG (SImode, REGNO (operands[0]));")
+   operands[0] = gen_lowpart (SImode, operands[0]);")
 
 (define_split
   [(set (match_operand 0 "memory_operand" "")
@@ -9394,7 +9959,7 @@
 	{
 	  /* Using SSE is tricky, since we need bitwise negation of -0
 	     in register.  */
-	  rtx reg = gen_reg_rtx (DFmode);
+	  rtx reg;
 #if HOST_BITS_PER_WIDE_INT >= 64
 	  rtx imm = gen_int_mode (((HOST_WIDE_INT)1) << 63, DImode);
 #else
@@ -9404,7 +9969,10 @@
 
 	  operands[1] = force_reg (DFmode, operands[1]);
 	  operands[0] = force_reg (DFmode, operands[0]);
-	  emit_move_insn (reg, gen_lowpart (DFmode, imm));
+	  imm = gen_lowpart (DFmode, imm);
+	  reg = force_reg (V2DFmode,
+			   gen_rtx_CONST_VECTOR (V2DFmode,
+			     gen_rtvec (2, imm, CONST0_RTX (DFmode))));
 	  emit_insn (gen_negdf2_ifs (operands[0], operands[1], reg));
 	  if (dest != operands[0])
 	    emit_move_insn (dest, operands[0]);
@@ -9423,7 +9991,7 @@
 (define_insn "negdf2_ifs"
   [(set (match_operand:DF 0 "nonimmediate_operand" "=Y#fr,Y#fr,f#Yr,rm#Yf")
 	(neg:DF (match_operand:DF 1 "nonimmediate_operand" "0,Y#fr,0,0")))
-   (use (match_operand:DF 2 "nonmemory_operand" "Y,0,*g#Y,*g#Y"))
+   (use (match_operand:V2DF 2 "nonimmediate_operand" "Ym,0,Ym*r,Ym*r"))
    (clobber (reg:CC 17))]
   "!TARGET_64BIT && TARGET_SSE2
    && (reload_in_progress || reload_completed
@@ -9433,8 +10001,8 @@
 
 (define_insn "*negdf2_ifs_rex64"
   [(set (match_operand:DF 0 "nonimmediate_operand" "=Y#f,Y#f,fm#Y")
-	(neg:DF (match_operand:DF 1 "nonimmediate_operand" "0,Y#f,0")))
-   (use (match_operand:DF 2 "general_operand" "Y,0,*g#Y*r"))
+	(neg:DF (match_operand:DF 1 "nonimmediate_operand" "0,Y#fr,0")))
+   (use (match_operand:V2DF 2 "nonimmediate_operand" "Ym,0,Ym*r"))
    (clobber (reg:CC 17))]
   "TARGET_64BIT && TARGET_SSE2
    && (reload_in_progress || reload_completed
@@ -9445,7 +10013,7 @@
 (define_split
   [(set (match_operand:DF 0 "memory_operand" "")
 	(neg:DF (match_operand:DF 1 "memory_operand" "")))
-   (use (match_operand:DF 2 "" ""))
+   (use (match_operand:V2DF 2 "" ""))
    (clobber (reg:CC 17))]
   ""
   [(parallel [(set (match_dup 0)
@@ -9455,7 +10023,7 @@
 (define_split
   [(set (match_operand:DF 0 "register_operand" "")
 	(neg:DF (match_operand:DF 1 "register_operand" "")))
-   (use (match_operand:DF 2 "" ""))
+   (use (match_operand:V2DF 2 "" ""))
    (clobber (reg:CC 17))]
   "reload_completed && !SSE_REG_P (operands[0])
    && (!TARGET_64BIT || FP_REG_P (operands[0]))"
@@ -9466,7 +10034,7 @@
 (define_split
   [(set (match_operand:DF 0 "register_operand" "")
 	(neg:DF (match_operand:DF 1 "register_operand" "")))
-   (use (match_operand:DF 2 "" ""))
+   (use (match_operand:V2DF 2 "" ""))
    (clobber (reg:CC 17))]
   "TARGET_64BIT && reload_completed && GENERAL_REG_P (operands[0])"
   [(parallel [(set (match_dup 0)
@@ -9479,13 +10047,19 @@
 (define_split
   [(set (match_operand:DF 0 "register_operand" "")
 	(neg:DF (match_operand:DF 1 "register_operand" "")))
-   (use (match_operand:DF 2 "register_operand" ""))
+   (use (match_operand:V2DF 2 "nonimmediate_operand" ""))
    (clobber (reg:CC 17))]
   "reload_completed && SSE_REG_P (operands[0])"
   [(set (subreg:TI (match_dup 0) 0)
-	(xor:TI (subreg:TI (match_dup 1) 0)
-		(subreg:TI (match_dup 2) 0)))]
+	(xor:TI (match_dup 1)
+		(match_dup 2)))]
 {
+  operands[0] = simplify_gen_subreg (V2DFmode, operands[0], DFmode, 0);
+  operands[1] = simplify_gen_subreg (TImode, operands[1], DFmode, 0);
+  operands[2] = simplify_gen_subreg (TImode, operands[2], V2DFmode, 0);
+  /* Avoid possible reformatting on the operands.  */
+  if (TARGET_SSE_PARTIAL_REGS && !optimize_size)
+    emit_insn (gen_sse2_unpcklpd (operands[0], operands[0], operands[0]));
   if (operands_match_p (operands[0], operands[2]))
     {
       rtx tmp;
@@ -9612,7 +10186,7 @@
    operands[0] = gen_rtx_REG (SImode,
 			      true_regnum (operands[0]) + (TARGET_64BIT ? 1 : 2));")
 
-;; Conditionize these after reload. If they matches before reload, we 
+;; Conditionalize these after reload. If they matches before reload, we 
 ;; lose the clobber and ability to use integer instructions.
 
 (define_insn "*negsf2_1"
@@ -9718,14 +10292,18 @@
 	{
 	  /* Using SSE is tricky, since we need bitwise negation of -0
 	     in register.  */
-	  rtx reg = gen_reg_rtx (SFmode);
+	  rtx reg = gen_reg_rtx (V4SFmode);
 	  rtx dest = operands[0];
+	  rtx imm;
 
 	  operands[1] = force_reg (SFmode, operands[1]);
 	  operands[0] = force_reg (SFmode, operands[0]);
-	  emit_move_insn (reg,
-			  gen_lowpart (SFmode,
-				       gen_int_mode (0x80000000, SImode)));
+	  imm = gen_lowpart (SFmode, gen_int_mode(~0x80000000, SImode));
+	  reg = force_reg (V4SFmode,
+			   gen_rtx_CONST_VECTOR (V4SFmode,
+			   gen_rtvec (4, imm, CONST0_RTX (SFmode),
+				      CONST0_RTX (SFmode),
+				      CONST0_RTX (SFmode))));
 	  emit_insn (gen_abssf2_ifs (operands[0], operands[1], reg));
 	  if (dest != operands[0])
 	    emit_move_insn (dest, operands[0]);
@@ -9742,20 +10320,20 @@
   "#")
 
 (define_insn "abssf2_ifs"
-  [(set (match_operand:SF 0 "nonimmediate_operand" "=x#fr,f#xr,rm#xf")
-	(abs:SF (match_operand:SF 1 "nonimmediate_operand" "x,0,0")))
-   (use (match_operand:SF 2 "nonmemory_operand" "*0#x,*g#x,*g#x"))
+  [(set (match_operand:SF 0 "nonimmediate_operand" "=x#fr,x#fr,f#xr,rm#xf")
+	(abs:SF (match_operand:SF 1 "nonimmediate_operand" "0,x#fr,0,0")))
+   (use (match_operand:V4SF 2 "nonimmediate_operand" "xm,0,xm*r,xm*r"))
    (clobber (reg:CC 17))]
   "TARGET_SSE
    && (reload_in_progress || reload_completed
        || (register_operand (operands[0], VOIDmode)
-	   && register_operand (operands[1], VOIDmode)))"
+	    && register_operand (operands[1], VOIDmode)))"
   "#")
 
 (define_split
   [(set (match_operand:SF 0 "memory_operand" "")
 	(abs:SF (match_operand:SF 1 "memory_operand" "")))
-   (use (match_operand:SF 2 "" ""))
+   (use (match_operand:V4SF 2 "" ""))
    (clobber (reg:CC 17))]
   ""
   [(parallel [(set (match_dup 0)
@@ -9765,7 +10343,7 @@
 (define_split
   [(set (match_operand:SF 0 "register_operand" "")
 	(abs:SF (match_operand:SF 1 "register_operand" "")))
-   (use (match_operand:SF 2 "" ""))
+   (use (match_operand:V4SF 2 "" ""))
    (clobber (reg:CC 17))]
   "reload_completed && !SSE_REG_P (operands[0])"
   [(parallel [(set (match_dup 0)
@@ -9775,12 +10353,23 @@
 (define_split
   [(set (match_operand:SF 0 "register_operand" "")
 	(abs:SF (match_operand:SF 1 "register_operand" "")))
-   (use (match_operand:SF 2 "register_operand" ""))
+   (use (match_operand:V4SF 2 "nonimmediate_operand" ""))
    (clobber (reg:CC 17))]
   "reload_completed && SSE_REG_P (operands[0])"
   [(set (subreg:TI (match_dup 0) 0)
-	(and:TI (not:TI (subreg:TI (match_dup 2) 0))
-		(subreg:TI (match_dup 1) 0)))])
+	(and:TI (match_dup 1)
+		(match_dup 2)))]
+{
+  operands[1] = simplify_gen_subreg (TImode, operands[1], SFmode, 0);
+  operands[2] = simplify_gen_subreg (TImode, operands[2], V4SFmode, 0);
+  if (operands_match_p (operands[0], operands[2]))
+    {
+      rtx tmp;
+      tmp = operands[1];
+      operands[1] = operands[2];
+      operands[2] = tmp;
+    }
+})
 
 ;; Keep 'f' and 'r' in separate alternatives to avoid reload problems
 ;; because of secondary memory needed to reload from class FLOAT_INT_REGS
@@ -9809,7 +10398,7 @@
   [(parallel [(set (match_dup 0) (and:SI (match_dup 0) (match_dup 1)))
 	      (clobber (reg:CC 17))])]
   "operands[1] = gen_int_mode (~0x80000000, SImode);
-   operands[0] = gen_rtx_REG (SImode, REGNO (operands[0]));")
+   operands[0] = gen_lowpart (SImode, operands[0]);")
 
 (define_split
   [(set (match_operand 0 "memory_operand" "")
@@ -9843,17 +10432,22 @@
 	{
 	  /* Using SSE is tricky, since we need bitwise negation of -0
 	     in register.  */
-	  rtx reg = gen_reg_rtx (DFmode);
+	  rtx reg = gen_reg_rtx (V2DFmode);
 #if HOST_BITS_PER_WIDE_INT >= 64
-	  rtx imm = gen_int_mode (((HOST_WIDE_INT)1) << 63, DImode);
+	  rtx imm = gen_int_mode (~(((HOST_WIDE_INT)1) << 63), DImode);
 #else
-	  rtx imm = immed_double_const (0, 0x80000000, DImode);
+	  rtx imm = immed_double_const (~0, ~0x80000000, DImode);
 #endif
 	  rtx dest = operands[0];
 
 	  operands[1] = force_reg (DFmode, operands[1]);
 	  operands[0] = force_reg (DFmode, operands[0]);
-	  emit_move_insn (reg, gen_lowpart (DFmode, imm));
+
+	  /* Produce LONG_DOUBLE with the proper immediate argument.  */
+	  imm = gen_lowpart (DFmode, imm);
+	  reg = force_reg (V2DFmode,
+			   gen_rtx_CONST_VECTOR (V2DFmode,
+			   gen_rtvec (2, imm, CONST0_RTX (DFmode))));
 	  emit_insn (gen_absdf2_ifs (operands[0], operands[1], reg));
 	  if (dest != operands[0])
 	    emit_move_insn (dest, operands[0]);
@@ -9870,9 +10464,9 @@
   "#")
 
 (define_insn "absdf2_ifs"
-  [(set (match_operand:DF 0 "nonimmediate_operand" "=Y#fr,mf#Yr,mr#Yf")
-	(abs:DF (match_operand:DF 1 "nonimmediate_operand" "Y,0,0")))
-   (use (match_operand:DF 2 "nonmemory_operand" "*0#Y,*g#Y,*g#Y"))
+  [(set (match_operand:DF 0 "nonimmediate_operand" "=Y#fr,Y#fr,mf#Yr,mr#Yf")
+	(abs:DF (match_operand:DF 1 "nonimmediate_operand" "0,Y#fr,0,0")))
+   (use (match_operand:V2DF 2 "nonimmediate_operand" "Ym,0,Ym*r,Ym*r"))
    (clobber (reg:CC 17))]
   "!TARGET_64BIT && TARGET_SSE2
    && (reload_in_progress || reload_completed
@@ -9881,9 +10475,9 @@
   "#")
 
 (define_insn "*absdf2_ifs_rex64"
-  [(set (match_operand:DF 0 "nonimmediate_operand" "=Y#fr,mf#Yr")
-	(abs:DF (match_operand:DF 1 "nonimmediate_operand" "Y,0")))
-   (use (match_operand:DF 2 "nonmemory_operand" "*0#Y,*g#Y"))
+  [(set (match_operand:DF 0 "nonimmediate_operand" "=Y#fr,Y#fr,mf#Yr")
+	(abs:DF (match_operand:DF 1 "nonimmediate_operand" "0,Y#fr,0")))
+   (use (match_operand:V2DF 2 "nonimmediate_operand" "Ym,0,Ym*r"))
    (clobber (reg:CC 17))]
   "TARGET_64BIT && TARGET_SSE2
    && (reload_in_progress || reload_completed
@@ -9894,7 +10488,7 @@
 (define_split
   [(set (match_operand:DF 0 "memory_operand" "")
 	(abs:DF (match_operand:DF 1 "memory_operand" "")))
-   (use (match_operand:DF 2 "" ""))
+   (use (match_operand:V2DF 2 "" ""))
    (clobber (reg:CC 17))]
   ""
   [(parallel [(set (match_dup 0)
@@ -9904,7 +10498,7 @@
 (define_split
   [(set (match_operand:DF 0 "register_operand" "")
 	(abs:DF (match_operand:DF 1 "register_operand" "")))
-   (use (match_operand:DF 2 "" ""))
+   (use (match_operand:V2DF 2 "" ""))
    (clobber (reg:CC 17))]
   "reload_completed && !SSE_REG_P (operands[0])"
   [(parallel [(set (match_dup 0)
@@ -9914,12 +10508,27 @@
 (define_split
   [(set (match_operand:DF 0 "register_operand" "")
 	(abs:DF (match_operand:DF 1 "register_operand" "")))
-   (use (match_operand:DF 2 "register_operand" ""))
+   (use (match_operand:V2DF 2 "nonimmediate_operand" ""))
    (clobber (reg:CC 17))]
   "reload_completed && SSE_REG_P (operands[0])"
   [(set (subreg:TI (match_dup 0) 0)
-	(and:TI (not:TI (subreg:TI (match_dup 2) 0))
-		(subreg:TI (match_dup 1) 0)))])
+	(and:TI (match_dup 1)
+		(match_dup 2)))]
+{
+  operands[0] = simplify_gen_subreg (V2DFmode, operands[0], DFmode, 0);
+  operands[1] = simplify_gen_subreg (TImode, operands[1], DFmode, 0);
+  operands[2] = simplify_gen_subreg (TImode, operands[2], V2DFmode, 0);
+  /* Avoid possible reformatting on the operands.  */
+  if (TARGET_SSE_PARTIAL_REGS && !optimize_size)
+    emit_insn (gen_sse2_unpcklpd (operands[0], operands[0], operands[0]));
+  if (operands_match_p (operands[0], operands[2]))
+    {
+      rtx tmp;
+      tmp = operands[1];
+      operands[1] = operands[2];
+      operands[2] = tmp;
+    }
+})
 
 
 ;; Keep 'f' and 'r' in separate alternatives to avoid reload problems
@@ -10692,9 +11301,11 @@
 	(zero_extend:DI (ashift (match_operand 1 "register_operand" "")
 				(match_operand:QI 2 "const_int_operand" ""))))
    (clobber (reg:CC 17))]
-  "reload_completed
+  "TARGET_64BIT && reload_completed
    && true_regnum (operands[0]) != true_regnum (operands[1])"
-  [(set (match_dup 0) (zero_extend:DI (subreg:SI (mult:SI (match_dup 1) (match_dup 2)) 0)))]
+  [(set (match_dup 0) (zero_extend:DI
+			(subreg:SI (mult:SI (match_dup 1)
+					    (match_dup 2)) 0)))]
 {
   operands[1] = gen_lowpart (Pmode, operands[1]);
   operands[2] = gen_int_mode (1 << INTVAL (operands[2]), Pmode);
@@ -12643,7 +13254,7 @@
 ;; The SSE store flag instructions saves 0 or 0xffffffff to the result.
 ;; subsequent logical operations are used to imitate conditional moves.
 ;; 0xffffffff is NaN, but not in normalized form, so we can't represent
-;; it directly.  Futher holding this value in pseudo register might bring
+;; it directly.  Further holding this value in pseudo register might bring
 ;; problem in implicit normalization in spill code.
 ;; So we don't define FLOAT_STORE_FLAG_VALUE and create these
 ;; instructions after reload by splitting the conditional move patterns.
@@ -13316,7 +13927,7 @@
 	(match_dup 2))]
 {
   operands[4] = gen_rtx_REG (GET_MODE (operands[0]), 17);
-  operands[5] = gen_rtx_REG (QImode, REGNO (operands[3]));
+  operands[5] = gen_lowpart (QImode, operands[3]);
   ix86_expand_clear (operands[3]);
 })
 
@@ -13338,7 +13949,7 @@
 	(match_dup 2))]
 {
   operands[4] = gen_rtx_REG (GET_MODE (operands[0]), 17);
-  operands[5] = gen_rtx_REG (QImode, REGNO (operands[3]));
+  operands[5] = gen_lowpart (QImode, operands[3]);
   ix86_expand_clear (operands[3]);
 })
 
@@ -13556,6 +14167,19 @@
    (set_attr "length_immediate" "0")
    (set_attr "modrm" "0")])
 
+;; Used by x86_machine_dependent_reorg to avoid penalty on single byte RET
+;; instruction Athlon and K8 have.
+
+(define_insn "return_internal_long"
+  [(return)
+   (unspec [(const_int 0)] UNSPEC_REP)]
+  "reload_completed"
+  "nop {;} ret"
+  [(set_attr "length" "1")
+   (set_attr "length_immediate" "0")
+   (set_attr "prefix_rep" "1")
+   (set_attr "modrm" "0")])
+
 (define_insn "return_pop_internal"
   [(return)
    (use (match_operand:SI 0 "const_int_operand" ""))]
@@ -13582,6 +14206,20 @@
    (set_attr "modrm" "0")
    (set_attr "ppro_uops" "one")])
 
+;; UNSPEC_VOLATILE is considered to use and clobber all hard registers and
+;; all of memory.  This blocks insns from being moved across this point.
+
+(define_insn "align"
+  [(unspec_volatile [(match_operand 0 "" "")] UNSPECV_ALIGN)]
+  ""
+{
+#ifdef HAVE_GAS_MAX_SKIP_P2ALIGN
+  return ".p2align\t4,,%c0";
+#endif
+}
+  [(set_attr "length" "16")])
+
+
 (define_expand "prologue"
   [(const_int 1)]
   ""
@@ -13652,11 +14290,7 @@
    (clobber (mem:BLK (scratch)))]
   "!TARGET_64BIT"
   "leave"
-  [(set_attr "length_immediate" "0")
-   (set_attr "length" "1")
-   (set_attr "modrm" "0")
-   (set_attr "athlon_decode" "vector")
-   (set_attr "ppro_uops" "few")])
+  [(set_attr "type" "leave")])
 
 (define_insn "leave_rex64"
   [(set (reg:DI 7) (plus:DI (reg:DI 6) (const_int 8)))
@@ -13664,11 +14298,7 @@
    (clobber (mem:BLK (scratch)))]
   "TARGET_64BIT"
   "leave"
-  [(set_attr "length_immediate" "0")
-   (set_attr "length" "1")
-   (set_attr "modrm" "0")
-   (set_attr "athlon_decode" "vector")
-   (set_attr "ppro_uops" "few")])
+  [(set_attr "type" "leave")])
 
 (define_expand "ffssi2"
   [(set (match_operand:SI 0 "nonimmediate_operand" "") 
@@ -14302,6 +14932,24 @@
               (const_string "fop")))
    (set_attr "mode" "SF")])
 
+(define_insn "*fop_df_6"
+  [(set (match_operand:DF 0 "register_operand" "=f,f")
+	(match_operator:DF 3 "binary_fp_operator"
+	  [(float_extend:DF
+	    (match_operand:SF 1 "register_operand" "0,f"))
+	   (float_extend:DF
+	    (match_operand:SF 2 "nonimmediate_operand" "fm,0"))]))]
+  "TARGET_80387 && !(TARGET_SSE2 && TARGET_SSE_MATH)"
+  "* return output_387_binary_op (insn, operands);"
+  [(set (attr "type") 
+        (cond [(match_operand:DF 3 "mult_operator" "") 
+                 (const_string "fmul")
+               (match_operand:DF 3 "div_operator" "") 
+                 (const_string "fdiv")
+              ]
+              (const_string "fop")))
+   (set_attr "mode" "SF")])
+
 (define_insn "*fop_xf_1"
   [(set (match_operand:XF 0 "register_operand" "=f,f")
 	(match_operator:XF 3 "binary_fp_operator"
@@ -14411,7 +15059,7 @@
 (define_insn "*fop_xf_4"
   [(set (match_operand:XF 0 "register_operand" "=f,f")
 	(match_operator:XF 3 "binary_fp_operator"
-	   [(float_extend:XF (match_operand:SF 1 "nonimmediate_operand" "fm,0"))
+	   [(float_extend:XF (match_operand 1 "nonimmediate_operand" "fm,0"))
 	    (match_operand:XF 2 "register_operand" "0,f")]))]
   "!TARGET_64BIT && TARGET_80387"
   "* return output_387_binary_op (insn, operands);"
@@ -14427,7 +15075,7 @@
 (define_insn "*fop_tf_4"
   [(set (match_operand:TF 0 "register_operand" "=f,f")
 	(match_operator:TF 3 "binary_fp_operator"
-	   [(float_extend:TF (match_operand:SF 1 "nonimmediate_operand" "fm,0"))
+	   [(float_extend:TF (match_operand 1 "nonimmediate_operand" "fm,0"))
 	    (match_operand:TF 2 "register_operand" "0,f")]))]
   "TARGET_80387"
   "* return output_387_binary_op (insn, operands);"
@@ -14445,7 +15093,7 @@
 	(match_operator:XF 3 "binary_fp_operator"
 	  [(match_operand:XF 1 "register_operand" "0,f")
 	   (float_extend:XF
-	    (match_operand:SF 2 "nonimmediate_operand" "fm,0"))]))]
+	    (match_operand 2 "nonimmediate_operand" "fm,0"))]))]
   "!TARGET_64BIT && TARGET_80387"
   "* return output_387_binary_op (insn, operands);"
   [(set (attr "type") 
@@ -14462,7 +15110,7 @@
 	(match_operator:TF 3 "binary_fp_operator"
 	  [(match_operand:TF 1 "register_operand" "0,f")
 	   (float_extend:TF
-	    (match_operand:SF 2 "nonimmediate_operand" "fm,0"))]))]
+	    (match_operand 2 "nonimmediate_operand" "fm,0"))]))]
   "TARGET_80387"
   "* return output_387_binary_op (insn, operands);"
   [(set (attr "type") 
@@ -14477,41 +15125,10 @@
 (define_insn "*fop_xf_6"
   [(set (match_operand:XF 0 "register_operand" "=f,f")
 	(match_operator:XF 3 "binary_fp_operator"
-	   [(float_extend:XF (match_operand:DF 1 "nonimmediate_operand" "fm,0"))
-	    (match_operand:XF 2 "register_operand" "0,f")]))]
-  "!TARGET_64BIT && TARGET_80387"
-  "* return output_387_binary_op (insn, operands);"
-  [(set (attr "type") 
-        (cond [(match_operand:XF 3 "mult_operator" "") 
-                 (const_string "fmul")
-               (match_operand:XF 3 "div_operator" "") 
-                 (const_string "fdiv")
-              ]
-              (const_string "fop")))
-   (set_attr "mode" "DF")])
-
-(define_insn "*fop_tf_6"
-  [(set (match_operand:TF 0 "register_operand" "=f,f")
-	(match_operator:TF 3 "binary_fp_operator"
-	   [(float_extend:TF (match_operand:DF 1 "nonimmediate_operand" "fm,0"))
-	    (match_operand:TF 2 "register_operand" "0,f")]))]
-  "TARGET_80387"
-  "* return output_387_binary_op (insn, operands);"
-  [(set (attr "type") 
-        (cond [(match_operand:TF 3 "mult_operator" "") 
-                 (const_string "fmul")
-               (match_operand:TF 3 "div_operator" "") 
-                 (const_string "fdiv")
-              ]
-              (const_string "fop")))
-   (set_attr "mode" "DF")])
-
-(define_insn "*fop_xf_7"
-  [(set (match_operand:XF 0 "register_operand" "=f,f")
-	(match_operator:XF 3 "binary_fp_operator"
-	  [(match_operand:XF 1 "register_operand" "0,f")
+	  [(float_extend:XF
+	    (match_operand 1 "register_operand" "0,f"))
 	   (float_extend:XF
-	    (match_operand:DF 2 "nonimmediate_operand" "fm,0"))]))]
+	    (match_operand 2 "nonimmediate_operand" "fm,0"))]))]
   "!TARGET_64BIT && TARGET_80387"
   "* return output_387_binary_op (insn, operands);"
   [(set (attr "type") 
@@ -14521,14 +15138,15 @@
                  (const_string "fdiv")
               ]
               (const_string "fop")))
-   (set_attr "mode" "DF")])
+   (set_attr "mode" "SF")])
 
-(define_insn "*fop_tf_7"
+(define_insn "*fop_tf_6"
   [(set (match_operand:TF 0 "register_operand" "=f,f")
 	(match_operator:TF 3 "binary_fp_operator"
-	  [(match_operand:TF 1 "register_operand" "0,f")
+	  [(float_extend:TF
+	    (match_operand 1 "register_operand" "0,f"))
 	   (float_extend:TF
-	    (match_operand:DF 2 "nonimmediate_operand" "fm,0"))]))]
+	    (match_operand 2 "nonimmediate_operand" "fm,0"))]))]
   "TARGET_80387"
   "* return output_387_binary_op (insn, operands);"
   [(set (attr "type") 
@@ -14538,7 +15156,7 @@
                  (const_string "fdiv")
               ]
               (const_string "fop")))
-   (set_attr "mode" "DF")])
+   (set_attr "mode" "SF")])
 
 (define_split
   [(set (match_operand 0 "register_operand" "")
@@ -15565,6 +16183,10 @@
 {
   rtx addr1, addr2, out, outlow, count, countreg, align;
 
+  /* Can't use this if the user has appropriated esi or edi.  */
+  if (global_regs[4] || global_regs[5])
+    FAIL;
+
   out = operands[0];
   if (GET_CODE (out) != REG)
     out = gen_reg_rtx (SImode);
@@ -15864,7 +16486,7 @@
 
 (define_insn "x86_movdicc_0_m1_rex64"
   [(set (match_operand:DI 0 "register_operand" "=r")
-	(if_then_else:DI (ltu (reg:CC 17) (const_int 0))
+	(if_then_else:DI (match_operand 1 "ix86_carry_flag_operator" "")
 	  (const_int -1)
 	  (const_int 0)))
    (clobber (reg:CC 17))]
@@ -15879,7 +16501,7 @@
    (set_attr "mode" "DI")
    (set_attr "length_immediate" "0")])
 
-(define_insn "*movdicc_c_rex64"
+(define_insn "movdicc_c_rex64"
   [(set (match_operand:DI 0 "register_operand" "=r,r")
 	(if_then_else:DI (match_operator 1 "ix86_comparison_operator" 
 				[(reg 17) (const_int 0)])
@@ -15907,7 +16529,7 @@
 
 (define_insn "x86_movsicc_0_m1"
   [(set (match_operand:SI 0 "register_operand" "=r")
-	(if_then_else:SI (ltu (reg:CC 17) (const_int 0))
+	(if_then_else:SI (match_operand 1 "ix86_carry_flag_operator" "")
 	  (const_int -1)
 	  (const_int 0)))
    (clobber (reg:CC 17))]
@@ -15939,9 +16561,9 @@
 (define_expand "movhicc"
   [(set (match_operand:HI 0 "register_operand" "")
 	(if_then_else:HI (match_operand 1 "comparison_operator" "")
-			 (match_operand:HI 2 "nonimmediate_operand" "")
-			 (match_operand:HI 3 "nonimmediate_operand" "")))]
-  "TARGET_CMOVE && TARGET_HIMODE_MATH"
+			 (match_operand:HI 2 "general_operand" "")
+			 (match_operand:HI 3 "general_operand" "")))]
+  "TARGET_HIMODE_MATH"
   "if (!ix86_expand_int_movcc (operands)) FAIL; DONE;")
 
 (define_insn "*movhicc_noc"
@@ -15958,6 +16580,33 @@
   [(set_attr "type" "icmov")
    (set_attr "mode" "HI")])
 
+(define_expand "movqicc"
+  [(set (match_operand:QI 0 "register_operand" "")
+	(if_then_else:QI (match_operand 1 "comparison_operator" "")
+			 (match_operand:QI 2 "general_operand" "")
+			 (match_operand:QI 3 "general_operand" "")))]
+  "TARGET_QIMODE_MATH"
+  "if (!ix86_expand_int_movcc (operands)) FAIL; DONE;")
+
+(define_insn_and_split "*movqicc_noc"
+  [(set (match_operand:QI 0 "register_operand" "=r,r")
+	(if_then_else:QI (match_operator 1 "ix86_comparison_operator" 
+				[(match_operand 4 "flags_reg_operand" "") (const_int 0)])
+		      (match_operand:QI 2 "register_operand" "r,0")
+		      (match_operand:QI 3 "register_operand" "0,r")))]
+  "TARGET_CMOVE && !TARGET_PARTIAL_REG_STALL"
+  "#"
+  "&& reload_completed"
+  [(set (match_dup 0)
+	(if_then_else:SI (match_op_dup 1 [(match_dup 4) (const_int 0)])
+		      (match_dup 2)
+		      (match_dup 3)))]
+  "operands[0] = gen_lowpart (SImode, operands[0]);
+   operands[2] = gen_lowpart (SImode, operands[2]);
+   operands[3] = gen_lowpart (SImode, operands[3]);"
+  [(set_attr "type" "icmov")
+   (set_attr "mode" "SI")])
+
 (define_expand "movsfcc"
   [(set (match_operand:SF 0 "register_operand" "")
 	(if_then_else:SF (match_operand 1 "comparison_operator" "")
@@ -15967,11 +16616,11 @@
   "if (! ix86_expand_fp_movcc (operands)) FAIL; DONE;")
 
 (define_insn "*movsfcc_1"
-  [(set (match_operand:SF 0 "register_operand" "=f,f,r,r")
+  [(set (match_operand:SF 0 "register_operand" "=f#r,f#r,r#f,r#f")
 	(if_then_else:SF (match_operator 1 "fcmov_comparison_operator" 
 				[(reg 17) (const_int 0)])
-		      (match_operand:SF 2 "nonimmediate_operand" "f,0,rm,0")
-		      (match_operand:SF 3 "nonimmediate_operand" "0,f,0,rm")))]
+		      (match_operand:SF 2 "nonimmediate_operand" "f#r,0,rm#f,0")
+		      (match_operand:SF 3 "nonimmediate_operand" "0,f#r,0,rm#f")))]
   "TARGET_CMOVE
    && (GET_CODE (operands[2]) != MEM || GET_CODE (operands[3]) != MEM)"
   "@
@@ -15991,11 +16640,11 @@
   "if (! ix86_expand_fp_movcc (operands)) FAIL; DONE;")
 
 (define_insn "*movdfcc_1"
-  [(set (match_operand:DF 0 "register_operand" "=f,f,&r,&r")
+  [(set (match_operand:DF 0 "register_operand" "=f#r,f#r,&r#f,&r#f")
 	(if_then_else:DF (match_operator 1 "fcmov_comparison_operator" 
 				[(reg 17) (const_int 0)])
-		      (match_operand:DF 2 "nonimmediate_operand" "f,0,rm,0")
-		      (match_operand:DF 3 "nonimmediate_operand" "0,f,0,rm")))]
+		      (match_operand:DF 2 "nonimmediate_operand" "f#r,0,rm#f,0")
+		      (match_operand:DF 3 "nonimmediate_operand" "0,f#r,0,rm#f")))]
   "!TARGET_64BIT && TARGET_CMOVE
    && (GET_CODE (operands[2]) != MEM || GET_CODE (operands[3]) != MEM)"
   "@
@@ -16007,11 +16656,11 @@
    (set_attr "mode" "DF")])
 
 (define_insn "*movdfcc_1_rex64"
-  [(set (match_operand:DF 0 "register_operand" "=f,f,&r,&r")
+  [(set (match_operand:DF 0 "register_operand" "=f#r,f#r,r#f,r#f")
 	(if_then_else:DF (match_operator 1 "fcmov_comparison_operator" 
 				[(reg 17) (const_int 0)])
-		      (match_operand:DF 2 "nonimmediate_operand" "f,0,rm,0")
-		      (match_operand:DF 3 "nonimmediate_operand" "0,f,0,rm")))]
+		      (match_operand:DF 2 "nonimmediate_operand" "f#r,0#r,rm#f,0#f")
+		      (match_operand:DF 3 "nonimmediate_operand" "0#r,f#r,0#f,rm#f")))]
   "TARGET_64BIT && TARGET_CMOVE
    && (GET_CODE (operands[2]) != MEM || GET_CODE (operands[3]) != MEM)"
   "@
@@ -16025,7 +16674,7 @@
 (define_split
   [(set (match_operand:DF 0 "register_and_not_any_fp_reg_operand" "")
 	(if_then_else:DF (match_operator 1 "fcmov_comparison_operator" 
-				[(match_operand 4 "" "") (const_int 0)])
+				[(match_operand 4 "flags_reg_operand" "") (const_int 0)])
 		      (match_operand:DF 2 "nonimmediate_operand" "")
 		      (match_operand:DF 3 "nonimmediate_operand" "")))]
   "!TARGET_64BIT && reload_completed"
@@ -16133,6 +16782,39 @@
 			 (match_dup 1)
 			 (match_dup 2)))])
 
+;; Conditional addition patterns
+(define_expand "addqicc"
+  [(match_operand:QI 0 "register_operand" "")
+   (match_operand 1 "comparison_operator" "")
+   (match_operand:QI 2 "register_operand" "")
+   (match_operand:QI 3 "const_int_operand" "")]
+  ""
+  "if (!ix86_expand_int_addcc (operands)) FAIL; DONE;")
+
+(define_expand "addhicc"
+  [(match_operand:HI 0 "register_operand" "")
+   (match_operand 1 "comparison_operator" "")
+   (match_operand:HI 2 "register_operand" "")
+   (match_operand:HI 3 "const_int_operand" "")]
+  ""
+  "if (!ix86_expand_int_addcc (operands)) FAIL; DONE;")
+
+(define_expand "addsicc"
+  [(match_operand:SI 0 "register_operand" "")
+   (match_operand 1 "comparison_operator" "")
+   (match_operand:SI 2 "register_operand" "")
+   (match_operand:SI 3 "const_int_operand" "")]
+  ""
+  "if (!ix86_expand_int_addcc (operands)) FAIL; DONE;")
+
+(define_expand "adddicc"
+  [(match_operand:DI 0 "register_operand" "")
+   (match_operand 1 "comparison_operator" "")
+   (match_operand:DI 2 "register_operand" "")
+   (match_operand:DI 3 "const_int_operand" "")]
+  "TARGET_64BIT"
+  "if (!ix86_expand_int_addcc (operands)) FAIL; DONE;")
+
 ;; We can't represent the LT test directly.  Do this by swapping the operands.
 
 (define_split
@@ -16539,6 +17221,12 @@
    (clobber (reg:CC 17))]
   "TARGET_SSE
    && (GET_CODE (operands[2]) != MEM || GET_CODE (operands[3]) != MEM)
+   /* Avoid combine from being smart and converting min/max
+      instruction patterns into conditional moves.  */
+   && ((GET_CODE (operands[1]) != LT && GET_CODE (operands[1]) != GT
+	&& GET_CODE (operands[1]) != UNLE && GET_CODE (operands[1]) != UNGE)
+       || !rtx_equal_p (operands[4], operands[2])
+       || !rtx_equal_p (operands[5], operands[3]))
    && (!TARGET_IEEE_FP
        || (GET_CODE (operands[1]) != EQ && GET_CODE (operands[1]) != NE))"
   "#")
@@ -16566,6 +17254,12 @@
    (clobber (reg:CC 17))]
   "TARGET_SSE2
    && (GET_CODE (operands[2]) != MEM || GET_CODE (operands[3]) != MEM)
+   /* Avoid combine from being smart and converting min/max
+      instruction patterns into conditional moves.  */
+   && ((GET_CODE (operands[1]) != LT && GET_CODE (operands[1]) != GT
+	&& GET_CODE (operands[1]) != UNLE && GET_CODE (operands[1]) != UNGE)
+       || !rtx_equal_p (operands[4], operands[2])
+       || !rtx_equal_p (operands[5], operands[3]))
    && (!TARGET_IEEE_FP
        || (GET_CODE (operands[1]) != EQ && GET_CODE (operands[1]) != NE))"
   "#")
@@ -16604,7 +17298,7 @@
    DONE;
 })
 
-;; Split SSE based conditional move into seqence:
+;; Split SSE based conditional move into sequence:
 ;; cmpCC op0, op4   -  set op0 to 0 or ffffffff depending on the comparison
 ;; and   op2, op0   -  zero op2 if comparison was false
 ;; nand  op0, op3   -  load op3 to op0 if comparison was false
@@ -16627,10 +17321,22 @@
    (set (subreg:TI (match_dup 0) 0) (ior:TI (subreg:TI (match_dup 6) 0)
 					    (subreg:TI (match_dup 7) 0)))]
 {
-  /* If op2 == op3, op3 will be clobbered before it is used.
-     This should be optimized out though.  */
+  if (GET_MODE (operands[2]) == DFmode
+      && TARGET_SSE_PARTIAL_REGS && !optimize_size)
+    {
+      rtx op = simplify_gen_subreg (V2DFmode, operands[2], DFmode, 0);
+      emit_insn (gen_sse2_unpcklpd (op, op, op));
+      op = simplify_gen_subreg (V2DFmode, operands[3], DFmode, 0);
+      emit_insn (gen_sse2_unpcklpd (op, op, op));
+    }
+
+  /* If op2 == op3, op3 would be clobbered before it is used.  */
   if (operands_match_p (operands[2], operands[3]))
-    abort ();
+    {
+      emit_move_insn (operands[0], operands[2]);
+      DONE;
+    }
+
   PUT_MODE (operands[1], GET_MODE (operands[0]));
   if (operands_match_p (operands[0], operands[4]))
     operands[6] = operands[4], operands[7] = operands[2];
@@ -16638,7 +17344,7 @@
     operands[6] = operands[2], operands[7] = operands[4];
 })
 
-;; Special case of conditional move we can handle effectivly.
+;; Special case of conditional move we can handle effectively.
 ;; Do not brother with the integer/floating point case, since these are
 ;; bot considerably slower, unlike in the generic case.
 (define_insn "*sse_movsfcc_const0_1"
@@ -16733,8 +17439,22 @@
        || const0_operand (operands[3], GET_MODE (operands[0])))"
   [(set (match_dup 0) (match_op_dup 1 [(match_dup 0) (match_dup 5)]))
    (set (subreg:TI (match_dup 0) 0) (and:TI (match_dup 6)
-					    (subreg:TI (match_dup 7) 0)))]
+					    (match_dup 7)))]
 {
+  if (TARGET_SSE_PARTIAL_REGS && !optimize_size
+      && GET_MODE (operands[2]) == DFmode)
+    {
+      if (REG_P (operands[2]))
+	{
+	  rtx op = simplify_gen_subreg (V2DFmode, operands[2], DFmode, 0);
+	  emit_insn (gen_sse2_unpcklpd (op, op, op));
+	}
+      if (REG_P (operands[3]))
+	{
+	  rtx op = simplify_gen_subreg (V2DFmode, operands[3], DFmode, 0);
+	  emit_insn (gen_sse2_unpcklpd (op, op, op));
+	}
+    }
   PUT_MODE (operands[1], GET_MODE (operands[0]));
   if (!sse_comparison_operator (operands[1], VOIDmode)
       || !rtx_equal_p (operands[0], operands[4]))
@@ -16757,6 +17477,8 @@
       operands[7] = operands[2];
       operands[6] = gen_rtx_SUBREG (TImode, operands[0], 0);
     }
+  operands[7] = simplify_gen_subreg (TImode, operands[7],
+  				     GET_MODE (operands[7]), 0);
 })
 
 (define_expand "allocate_stack_worker"
@@ -17020,7 +17742,7 @@
   [(parallel [(set (match_dup 2) (const_int 0))
 	      (clobber (reg:CC 17))])
    (set (match_dup 0) (match_dup 1))]
-  "operands[2] = gen_rtx_REG (SImode, true_regnum (operands[1]));")
+  "operands[2] = gen_lowpart (SImode, operands[1]);")
 
 (define_peephole2
   [(match_scratch:QI 1 "q")
@@ -17034,7 +17756,7 @@
   [(parallel [(set (match_dup 2) (const_int 0))
 	      (clobber (reg:CC 17))])
    (set (match_dup 0) (match_dup 1))]
-  "operands[2] = gen_rtx_REG (SImode, true_regnum (operands[1]));")
+  "operands[2] = gen_lowpart (SImode, operands[1]);")
 
 (define_peephole2
   [(match_scratch:SI 2 "r")
@@ -17080,7 +17802,7 @@
 
 ;; NOT is not pairable on Pentium, while XOR is, but one byte longer. 
 ;; Don't split NOTs with a displacement operand, because resulting XOR
-;; will not be pariable anyway.
+;; will not be pairable anyway.
 ;;
 ;; On AMD K6, NOT is vector decoded with memory operand that can not be
 ;; represented using a modRM byte.  The XOR replacement is long decoded,
@@ -17289,8 +18011,8 @@
    && peep2_regno_dead_p (0, FLAGS_REG)"
   [(parallel [(set (match_dup 0) (const_int 0))
 	      (clobber (reg:CC 17))])]
-  "operands[0] = gen_rtx_REG (GET_MODE (operands[0]) == DImode ? DImode : SImode,
-			      true_regnum (operands[0]));")
+  "operands[0] = gen_lowpart (GET_MODE (operands[0]) == DImode ? DImode : SImode,
+			      operands[0]);")
 
 (define_peephole2
   [(set (strict_low_part (match_operand 0 "register_operand" ""))
@@ -17313,8 +18035,8 @@
    && peep2_regno_dead_p (0, FLAGS_REG)"
   [(parallel [(set (match_dup 0) (const_int -1))
 	      (clobber (reg:CC 17))])]
-  "operands[0] = gen_rtx_REG (GET_MODE (operands[0]) == DImode ? DImode : SImode,
-			      true_regnum (operands[0]));")
+  "operands[0] = gen_lowpart (GET_MODE (operands[0]) == DImode ? DImode : SImode,
+			      operands[0]);")
 
 ;; Attempt to convert simple leas to adds. These can be created by
 ;; move expanders.
@@ -17688,6 +18410,102 @@
 	      (set (reg:DI 7) (plus:DI (reg:DI 7) (const_int 8)))])]
   "")
 
+;; Imul $32bit_imm, mem, reg is vector decoded, while
+;; imul $32bit_imm, reg, reg is direct decoded.
+(define_peephole2
+  [(match_scratch:DI 3 "r")
+   (parallel [(set (match_operand:DI 0 "register_operand" "")
+		   (mult:DI (match_operand:DI 1 "memory_operand" "")
+			    (match_operand:DI 2 "immediate_operand" "")))
+	      (clobber (reg:CC 17))])]
+  "TARGET_K8 && !optimize_size
+   && (GET_CODE (operands[2]) != CONST_INT
+       || !CONST_OK_FOR_LETTER_P (INTVAL (operands[2]), 'K'))"
+  [(set (match_dup 3) (match_dup 1))
+   (parallel [(set (match_dup 0) (mult:DI (match_dup 3) (match_dup 2)))
+	      (clobber (reg:CC 17))])]
+"")
+
+(define_peephole2
+  [(match_scratch:SI 3 "r")
+   (parallel [(set (match_operand:SI 0 "register_operand" "")
+		   (mult:SI (match_operand:SI 1 "memory_operand" "")
+			    (match_operand:SI 2 "immediate_operand" "")))
+	      (clobber (reg:CC 17))])]
+  "TARGET_K8 && !optimize_size
+   && (GET_CODE (operands[2]) != CONST_INT
+       || !CONST_OK_FOR_LETTER_P (INTVAL (operands[2]), 'K'))"
+  [(set (match_dup 3) (match_dup 1))
+   (parallel [(set (match_dup 0) (mult:SI (match_dup 3) (match_dup 2)))
+	      (clobber (reg:CC 17))])]
+"")
+
+(define_peephole2
+  [(match_scratch:SI 3 "r")
+   (parallel [(set (match_operand:DI 0 "register_operand" "")
+		   (zero_extend:DI
+		     (mult:SI (match_operand:SI 1 "memory_operand" "")
+			      (match_operand:SI 2 "immediate_operand" ""))))
+	      (clobber (reg:CC 17))])]
+  "TARGET_K8 && !optimize_size
+   && (GET_CODE (operands[2]) != CONST_INT
+       || !CONST_OK_FOR_LETTER_P (INTVAL (operands[2]), 'K'))"
+  [(set (match_dup 3) (match_dup 1))
+   (parallel [(set (match_dup 0) (zero_extend:DI (mult:SI (match_dup 3) (match_dup 2))))
+	      (clobber (reg:CC 17))])]
+"")
+
+;; imul $8/16bit_imm, regmem, reg is vector decoded.
+;; Convert it into imul reg, reg
+;; It would be better to force assembler to encode instruction using long
+;; immediate, but there is apparently no way to do so.
+(define_peephole2
+  [(parallel [(set (match_operand:DI 0 "register_operand" "")
+		   (mult:DI (match_operand:DI 1 "nonimmediate_operand" "")
+			    (match_operand:DI 2 "const_int_operand" "")))
+	      (clobber (reg:CC 17))])
+   (match_scratch:DI 3 "r")]
+  "TARGET_K8 && !optimize_size
+   && CONST_OK_FOR_LETTER_P (INTVAL (operands[2]), 'K')"
+  [(set (match_dup 3) (match_dup 2))
+   (parallel [(set (match_dup 0) (mult:DI (match_dup 0) (match_dup 3)))
+	      (clobber (reg:CC 17))])]
+{
+  if (!rtx_equal_p (operands[0], operands[1]))
+    emit_move_insn (operands[0], operands[1]);
+})
+
+(define_peephole2
+  [(parallel [(set (match_operand:SI 0 "register_operand" "")
+		   (mult:SI (match_operand:SI 1 "nonimmediate_operand" "")
+			    (match_operand:SI 2 "const_int_operand" "")))
+	      (clobber (reg:CC 17))])
+   (match_scratch:SI 3 "r")]
+  "TARGET_K8 && !optimize_size
+   && CONST_OK_FOR_LETTER_P (INTVAL (operands[2]), 'K')"
+  [(set (match_dup 3) (match_dup 2))
+   (parallel [(set (match_dup 0) (mult:SI (match_dup 0) (match_dup 3)))
+	      (clobber (reg:CC 17))])]
+{
+  if (!rtx_equal_p (operands[0], operands[1]))
+    emit_move_insn (operands[0], operands[1]);
+})
+
+(define_peephole2
+  [(parallel [(set (match_operand:HI 0 "register_operand" "")
+		   (mult:HI (match_operand:HI 1 "nonimmediate_operand" "")
+			    (match_operand:HI 2 "immediate_operand" "")))
+	      (clobber (reg:CC 17))])
+   (match_scratch:HI 3 "r")]
+  "TARGET_K8 && !optimize_size"
+  [(set (match_dup 3) (match_dup 2))
+   (parallel [(set (match_dup 0) (mult:HI (match_dup 0) (match_dup 3)))
+	      (clobber (reg:CC 17))])]
+{
+  if (!rtx_equal_p (operands[0], operands[1]))
+    emit_move_insn (operands[0], operands[1]);
+})
+
 ;; Call-value patterns last so that the wildcard operand does not
 ;; disrupt insn-recog's switch tables.
 
@@ -17850,37 +18668,120 @@
   [(set (match_operand:V4SF 0 "nonimmediate_operand" "=x,x,m")
 	(match_operand:V4SF 1 "vector_move_operand" "C,xm,x"))]
   "TARGET_SSE"
-  ;; @@@ let's try to use movaps here.
   "@
-   xorps\t%0, %0
-   movaps\t{%1, %0|%0, %1}
-   movaps\t{%1, %0|%0, %1}"
+    xorps\t%0, %0
+    movaps\t{%1, %0|%0, %1}
+    movaps\t{%1, %0|%0, %1}"
   [(set_attr "type" "ssemov")
    (set_attr "mode" "V4SF")])
 
+(define_split
+  [(set (match_operand:V4SF 0 "register_operand" "")
+	(match_operand:V4SF 1 "zero_extended_scalar_load_operand" ""))]
+  "TARGET_SSE"
+  [(set (match_dup 0)
+	(vec_merge:V4SF
+	 (vec_duplicate:V4SF (match_dup 1))
+	 (match_dup 2)
+	 (const_int 1)))]
+{
+  operands[1] = simplify_gen_subreg (SFmode, operands[1], V4SFmode, 0);
+  operands[2] = CONST0_RTX (V4SFmode);
+})
+
 (define_insn "movv4si_internal"
   [(set (match_operand:V4SI 0 "nonimmediate_operand" "=x,x,m")
 	(match_operand:V4SI 1 "vector_move_operand" "C,xm,x"))]
   "TARGET_SSE"
-  ;; @@@ let's try to use movaps here.
-  "@
-   xorps\t%0, %0
-   movaps\t{%1, %0|%0, %1}
-   movaps\t{%1, %0|%0, %1}"
+{
+  switch (which_alternative)
+    {
+    case 0:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "xorps\t%0, %0";
+      else
+	return "pxor\t%0, %0";
+    case 1:
+    case 2:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "movaps\t{%1, %0|%0, %1}";
+      else
+	return "movdqa\t{%1, %0|%0, %1}";
+    default:
+      abort ();
+    }
+}
   [(set_attr "type" "ssemov")
-   (set_attr "mode" "V4SF")])
+   (set (attr "mode")
+        (cond [(eq_attr "alternative" "0,1")
+		 (if_then_else
+		   (ne (symbol_ref "optimize_size")
+		       (const_int 0))
+		   (const_string "V4SF")
+		   (const_string "TI"))
+	       (eq_attr "alternative" "2")
+		 (if_then_else
+		   (ior (ne (symbol_ref "TARGET_SSE_TYPELESS_STORES")
+			    (const_int 0))
+			(ne (symbol_ref "optimize_size")
+			    (const_int 0)))
+		   (const_string "V4SF")
+		   (const_string "TI"))]
+	       (const_string "TI")))])
 
 (define_insn "movv2di_internal"
   [(set (match_operand:V2DI 0 "nonimmediate_operand" "=x,x,m")
 	(match_operand:V2DI 1 "vector_move_operand" "C,xm,x"))]
-  "TARGET_SSE"
-  ;; @@@ let's try to use movaps here.
-  "@
-   pxor\t%0, %0
-   movdqa\t{%1, %0|%0, %1} 
-   movdqa\t{%1, %0|%0, %1}"
+  "TARGET_SSE2"
+{
+  switch (which_alternative)
+    {
+    case 0:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "xorps\t%0, %0";
+      else
+	return "pxor\t%0, %0";
+    case 1:
+    case 2:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "movaps\t{%1, %0|%0, %1}";
+      else
+	return "movdqa\t{%1, %0|%0, %1}";
+    default:
+      abort ();
+    }
+}
   [(set_attr "type" "ssemov")
-   (set_attr "mode" "V4SF")])
+   (set (attr "mode")
+        (cond [(eq_attr "alternative" "0,1")
+		 (if_then_else
+		   (ne (symbol_ref "optimize_size")
+		       (const_int 0))
+		   (const_string "V4SF")
+		   (const_string "TI"))
+	       (eq_attr "alternative" "2")
+		 (if_then_else
+		   (ior (ne (symbol_ref "TARGET_SSE_TYPELESS_STORES")
+			    (const_int 0))
+			(ne (symbol_ref "optimize_size")
+			    (const_int 0)))
+		   (const_string "V4SF")
+		   (const_string "TI"))]
+	       (const_string "TI")))])
+
+(define_split
+  [(set (match_operand:V2DF 0 "register_operand" "")
+	(match_operand:V2DF 1 "zero_extended_scalar_load_operand" ""))]
+  "TARGET_SSE2"
+  [(set (match_dup 0)
+	(vec_merge:V2DF
+	 (vec_duplicate:V2DF (match_dup 1))
+	 (match_dup 2)
+	 (const_int 1)))]
+{
+  operands[1] = simplify_gen_subreg (DFmode, operands[1], V2DFmode, 0);
+  operands[2] = CONST0_RTX (V2DFmode);
+})
 
 (define_insn "movv8qi_internal"
   [(set (match_operand:V8QI 0 "nonimmediate_operand" "=y,y,m")
@@ -17947,36 +18848,123 @@
 	(match_operand:V2DF 1 "vector_move_operand" "C,xm,x"))]
   "TARGET_SSE2
    && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
-  "@
-   xorpd\t%0, %0
-   movapd\t{%1, %0|%0, %1}
-   movapd\t{%1, %0|%0, %1}"
+{
+  switch (which_alternative)
+    {
+    case 0:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "xorps\t%0, %0";
+      else
+	return "xorpd\t%0, %0";
+    case 1:
+    case 2:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "movaps\t{%1, %0|%0, %1}";
+      else
+	return "movapd\t{%1, %0|%0, %1}";
+    default:
+      abort ();
+    }
+}
   [(set_attr "type" "ssemov")
-   (set_attr "mode" "V2DF")])
+   (set (attr "mode")
+        (cond [(eq_attr "alternative" "0,1")
+		 (if_then_else
+		   (ne (symbol_ref "optimize_size")
+		       (const_int 0))
+		   (const_string "V4SF")
+		   (const_string "V2DF"))
+	       (eq_attr "alternative" "2")
+		 (if_then_else
+		   (ior (ne (symbol_ref "TARGET_SSE_TYPELESS_STORES")
+			    (const_int 0))
+			(ne (symbol_ref "optimize_size")
+			    (const_int 0)))
+		   (const_string "V4SF")
+		   (const_string "V2DF"))]
+	       (const_string "V2DF")))])
 
 (define_insn "movv8hi_internal"
   [(set (match_operand:V8HI 0 "nonimmediate_operand" "=x,x,m")
 	(match_operand:V8HI 1 "vector_move_operand" "C,xm,x"))]
   "TARGET_SSE2
    && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
-  "@
-   xorps\t%0, %0
-   movaps\t{%1, %0|%0, %1}
-   movaps\t{%1, %0|%0, %1}"
+{
+  switch (which_alternative)
+    {
+    case 0:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "xorps\t%0, %0";
+      else
+	return "pxor\t%0, %0";
+    case 1:
+    case 2:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "movaps\t{%1, %0|%0, %1}";
+      else
+	return "movdqa\t{%1, %0|%0, %1}";
+    default:
+      abort ();
+    }
+}
   [(set_attr "type" "ssemov")
-   (set_attr "mode" "V4SF")])
+   (set (attr "mode")
+        (cond [(eq_attr "alternative" "0,1")
+		 (if_then_else
+		   (ne (symbol_ref "optimize_size")
+		       (const_int 0))
+		   (const_string "V4SF")
+		   (const_string "TI"))
+	       (eq_attr "alternative" "2")
+		 (if_then_else
+		   (ior (ne (symbol_ref "TARGET_SSE_TYPELESS_STORES")
+			    (const_int 0))
+			(ne (symbol_ref "optimize_size")
+			    (const_int 0)))
+		   (const_string "V4SF")
+		   (const_string "TI"))]
+	       (const_string "TI")))])
 
 (define_insn "movv16qi_internal"
   [(set (match_operand:V16QI 0 "nonimmediate_operand" "=x,x,m")
-	(match_operand:V16QI 1 "vector_move_operand" "C,xm,x"))]
+	(match_operand:V16QI 1 "nonimmediate_operand" "C,xm,x"))]
   "TARGET_SSE2
    && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
-  "@
-   xorps\t%0, %0
-   movaps\t{%1, %0|%0, %1}
-   movaps\t{%1, %0|%0, %1}"
+{
+  switch (which_alternative)
+    {
+    case 0:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "xorps\t%0, %0";
+      else
+	return "pxor\t%0, %0";
+    case 1:
+    case 2:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "movaps\t{%1, %0|%0, %1}";
+      else
+	return "movdqa\t{%1, %0|%0, %1}";
+    default:
+      abort ();
+    }
+}
   [(set_attr "type" "ssemov")
-   (set_attr "mode" "V4SF")])
+   (set (attr "mode")
+        (cond [(eq_attr "alternative" "0,1")
+		 (if_then_else
+		   (ne (symbol_ref "optimize_size")
+		       (const_int 0))
+		   (const_string "V4SF")
+		   (const_string "TI"))
+	       (eq_attr "alternative" "2")
+		 (if_then_else
+		   (ior (ne (symbol_ref "TARGET_SSE_TYPELESS_STORES")
+			    (const_int 0))
+			(ne (symbol_ref "optimize_size")
+			    (const_int 0)))
+		   (const_string "V4SF")
+		   (const_string "TI"))]
+	       (const_string "TI")))])
 
 (define_expand "movv2df"
   [(set (match_operand:V2DF 0 "nonimmediate_operand" "")
@@ -18068,6 +19056,12 @@
   DONE;
 })
 
+(define_insn "*pushti"
+  [(set (match_operand:TI 0 "push_operand" "=<")
+	(match_operand:TI 1 "register_operand" "x"))]
+  "TARGET_SSE"
+  "#")
+
 (define_insn "*pushv2df"
   [(set (match_operand:V2DF 0 "push_operand" "=<")
 	(match_operand:V2DF 1 "register_operand" "x"))]
@@ -18151,152 +19145,88 @@
    operands[3] = GEN_INT (-GET_MODE_SIZE (GET_MODE (operands[0])));")
 
 
-(define_insn_and_split "*pushti"
-  [(set (match_operand:TI 0 "push_operand" "=<")
-	(match_operand:TI 1 "nonmemory_operand" "x"))]
-  "TARGET_SSE"
-  "#"
-  ""
-  [(set (reg:SI 7) (plus:SI (reg:SI 7) (const_int -16)))
-   (set (mem:TI (reg:SI 7)) (match_dup 1))]
-  ""
-  [(set_attr "type" "multi")])
-
-(define_insn_and_split "*pushv2df"
-  [(set (match_operand:V2DF 0 "push_operand" "=<")
-	(match_operand:V2DF 1 "nonmemory_operand" "x"))]
-  "TARGET_SSE2"
-  "#"
-  ""
-  [(set (reg:SI 7) (plus:SI (reg:SI 7) (const_int -16)))
-   (set (mem:V2DF (reg:SI 7)) (match_dup 1))]
-  ""
-  [(set_attr "type" "multi")])
-
-(define_insn_and_split "*pushv2di"
-  [(set (match_operand:V2DI 0 "push_operand" "=<")
-	(match_operand:V2DI 1 "nonmemory_operand" "x"))]
-  "TARGET_SSE2"
-  "#"
-  ""
-  [(set (reg:SI 7) (plus:SI (reg:SI 7) (const_int -16)))
-   (set (mem:V2DI (reg:SI 7)) (match_dup 1))]
-  ""
-  [(set_attr "type" "multi")])
-
-(define_insn_and_split "*pushv8hi"
-  [(set (match_operand:V8HI 0 "push_operand" "=<")
-	(match_operand:V8HI 1 "nonmemory_operand" "x"))]
-  "TARGET_SSE2"
-  "#"
-  ""
-  [(set (reg:SI 7) (plus:SI (reg:SI 7) (const_int -16)))
-   (set (mem:V8HI (reg:SI 7)) (match_dup 1))]
-  ""
-  [(set_attr "type" "multi")])
-
-(define_insn_and_split "*pushv16qi"
-  [(set (match_operand:V16QI 0 "push_operand" "=<")
-	(match_operand:V16QI 1 "nonmemory_operand" "x"))]
-  "TARGET_SSE2"
-  "#"
-  ""
-  [(set (reg:SI 7) (plus:SI (reg:SI 7) (const_int -16)))
-   (set (mem:V16QI (reg:SI 7)) (match_dup 1))]
-  ""
-  [(set_attr "type" "multi")])
-
-(define_insn_and_split "*pushv4sf"
-  [(set (match_operand:V4SF 0 "push_operand" "=<")
-	(match_operand:V4SF 1 "nonmemory_operand" "x"))]
-  "TARGET_SSE"
-  "#"
-  ""
-  [(set (reg:SI 7) (plus:SI (reg:SI 7) (const_int -16)))
-   (set (mem:V4SF (reg:SI 7)) (match_dup 1))]
-  ""
-  [(set_attr "type" "multi")])
-
-(define_insn_and_split "*pushv4si"
-  [(set (match_operand:V4SI 0 "push_operand" "=<")
-	(match_operand:V4SI 1 "nonmemory_operand" "x"))]
-  "TARGET_SSE"
-  "#"
-  ""
-  [(set (reg:SI 7) (plus:SI (reg:SI 7) (const_int -16)))
-   (set (mem:V4SI (reg:SI 7)) (match_dup 1))]
-  ""
-  [(set_attr "type" "multi")])
-
-(define_insn_and_split "*pushv2si"
-  [(set (match_operand:V2SI 0 "push_operand" "=<")
-	(match_operand:V2SI 1 "nonmemory_operand" "y"))]
-  "TARGET_MMX"
-  "#"
-  ""
-  [(set (reg:SI 7) (plus:SI (reg:SI 7) (const_int -8)))
-   (set (mem:V2SI (reg:SI 7)) (match_dup 1))]
-  ""
-  [(set_attr "type" "mmx")])
-
-(define_insn_and_split "*pushv4hi"
-  [(set (match_operand:V4HI 0 "push_operand" "=<")
-	(match_operand:V4HI 1 "nonmemory_operand" "y"))]
-  "TARGET_MMX"
-  "#"
-  ""
-  [(set (reg:SI 7) (plus:SI (reg:SI 7) (const_int -8)))
-   (set (mem:V4HI (reg:SI 7)) (match_dup 1))]
-  ""
-  [(set_attr "type" "mmx")])
-
-(define_insn_and_split "*pushv8qi"
-  [(set (match_operand:V8QI 0 "push_operand" "=<")
-	(match_operand:V8QI 1 "nonmemory_operand" "y"))]
-  "TARGET_MMX"
-  "#"
-  ""
-  [(set (reg:SI 7) (plus:SI (reg:SI 7) (const_int -8)))
-   (set (mem:V8QI (reg:SI 7)) (match_dup 1))]
-  ""
-  [(set_attr "type" "mmx")])
-
-(define_insn_and_split "*pushv2sf"
-  [(set (match_operand:V2SF 0 "push_operand" "=<")
-	(match_operand:V2SF 1 "nonmemory_operand" "y"))]
-  "TARGET_3DNOW"
-  "#"
-  ""
-  [(set (reg:SI 7) (plus:SI (reg:SI 7) (const_int -8)))
-   (set (mem:V2SF (reg:SI 7)) (match_dup 1))]
-  ""
-  [(set_attr "type" "mmx")])
-
 (define_insn "movti_internal"
   [(set (match_operand:TI 0 "nonimmediate_operand" "=x,x,m")
 	(match_operand:TI 1 "vector_move_operand" "C,xm,x"))]
   "TARGET_SSE && !TARGET_64BIT
    && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
-  "@
-   xorps\t%0, %0
-   movaps\t{%1, %0|%0, %1}
-   movaps\t{%1, %0|%0, %1}"
+{
+  switch (which_alternative)
+    {
+    case 0:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "xorps\t%0, %0";
+      else
+	return "pxor\t%0, %0";
+    case 1:
+    case 2:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "movaps\t{%1, %0|%0, %1}";
+      else
+	return "movdqa\t{%1, %0|%0, %1}";
+    default:
+      abort ();
+    }
+}
   [(set_attr "type" "ssemov,ssemov,ssemov")
-   (set_attr "mode" "V4SF")])
+   (set (attr "mode")
+        (cond [(eq_attr "alternative" "0,1")
+		 (if_then_else
+		   (ne (symbol_ref "optimize_size")
+		       (const_int 0))
+		   (const_string "V4SF")
+		   (const_string "TI"))
+	       (eq_attr "alternative" "2")
+		 (if_then_else
+		   (ne (symbol_ref "optimize_size")
+		       (const_int 0))
+		   (const_string "V4SF")
+		   (const_string "TI"))]
+	       (const_string "TI")))])
 
 (define_insn "*movti_rex64"
-  [(set (match_operand:TI 0 "nonimmediate_operand" "=r,o,x,mx,x")
-	(match_operand:TI 1 "general_operand" "riFo,riF,C,x,m"))]
+  [(set (match_operand:TI 0 "nonimmediate_operand" "=r,o,x,x,xm")
+	(match_operand:TI 1 "general_operand" "riFo,riF,C,xm,x"))]
   "TARGET_64BIT
    && (GET_CODE (operands[0]) != MEM || GET_CODE (operands[1]) != MEM)"
-  "@
-   #
-   #
-   xorps\t%0, %0
-   movaps\\t{%1, %0|%0, %1}
-   movaps\\t{%1, %0|%0, %1}"
+{
+  switch (which_alternative)
+    {
+    case 0:
+    case 1:
+      return "#";
+    case 2:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "xorps\t%0, %0";
+      else
+	return "pxor\t%0, %0";
+    case 3:
+    case 4:
+      if (get_attr_mode (insn) == MODE_V4SF)
+	return "movaps\t{%1, %0|%0, %1}";
+      else
+	return "movdqa\t{%1, %0|%0, %1}";
+    default:
+      abort ();
+    }
+}
   [(set_attr "type" "*,*,ssemov,ssemov,ssemov")
-   (set_attr "mode" "V4SF")])
+   (set (attr "mode")
+        (cond [(eq_attr "alternative" "2,3")
+		 (if_then_else
+		   (ne (symbol_ref "optimize_size")
+		       (const_int 0))
+		   (const_string "V4SF")
+		   (const_string "TI"))
+	       (eq_attr "alternative" "4")
+		 (if_then_else
+		   (ior (ne (symbol_ref "TARGET_SSE_TYPELESS_STORES")
+			    (const_int 0))
+			(ne (symbol_ref "optimize_size")
+			    (const_int 0)))
+		   (const_string "V4SF")
+		   (const_string "TI"))]
+	       (const_string "DI")))])
 
 (define_split
   [(set (match_operand:TI 0 "nonimmediate_operand" "")
@@ -18685,14 +19615,14 @@
 ;; on integral types.  We deal with this by representing the floating point
 ;; logical as logical on arguments casted to TImode as this is what hardware
 ;; really does.  Unfortunately hardware requires the type information to be
-;; present and thus we must avoid subregs from being simplified and elliminated
+;; present and thus we must avoid subregs from being simplified and eliminated
 ;; in later compilation phases.
 ;;
 ;; We have following variants from each instruction:
 ;; sse_andsf3 - the operation taking V4SF vector operands
 ;;              and doing TImode cast on them
 ;; *sse_andsf3_memory - the operation taking one memory operand casted to
-;;                      TImode, since backend insist on elliminating casts
+;;                      TImode, since backend insist on eliminating casts
 ;;                      on memory operands
 ;; sse_andti3_sf_1 - the operation taking SF scalar operands.
 ;;                   We can not accept memory operand here as instruction reads
@@ -18700,7 +19630,7 @@
 ;;		     scalar float operations that expands to logicals (fabs)
 ;; sse_andti3_sf_2 - the operation taking SF scalar input and TImode
 ;;		     memory operand.  Eventually combine can be able
-;;		     to synthetize these using splitter.
+;;		     to synthesize these using splitter.
 ;; sse2_anddf3, *sse2_anddf3_memory
 ;;              
 ;; 
@@ -19010,12 +19940,26 @@
 ;; this insn.
 (define_insn "sse_clrv4sf"
   [(set (match_operand:V4SF 0 "register_operand" "=x")
-        (unspec:V4SF [(const_int 0)] UNSPEC_NOP))]
+	(match_operand:V4SF 1 "const0_operand" "X"))]
   "TARGET_SSE"
-  "xorps\t{%0, %0|%0, %0}"
+{
+  if (get_attr_mode (insn) == MODE_TI)
+    return "pxor\t{%0, %0|%0, %0}";
+  else
+    return "xorps\t{%0, %0|%0, %0}";
+}
   [(set_attr "type" "sselog")
    (set_attr "memory" "none")
-   (set_attr "mode" "V4SF")])
+   (set (attr "mode")
+	(if_then_else
+	   (and (and (ne (symbol_ref "TARGET_SSE_LOAD0_BY_PXOR")
+			 (const_int 0))
+		     (ne (symbol_ref "TARGET_SSE2")
+			 (const_int 0)))
+		(eq (symbol_ref "optimize_size")
+		    (const_int 0)))
+	 (const_string "TI")
+	 (const_string "V4SF")))])
 
 ;; Use xor, but don't show input operands so they aren't live before
 ;; this insn.
@@ -19098,7 +20042,7 @@
 		       (parallel [(const_int 0)]))))]
   "TARGET_SSE"
   "comiss\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecmp")
+  [(set_attr "type" "ssecomi")
    (set_attr "mode" "SF")])
 
 (define_insn "sse_ucomi"
@@ -19111,7 +20055,7 @@
 			(parallel [(const_int 0)]))))]
   "TARGET_SSE"
   "ucomiss\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecmp")
+  [(set_attr "type" "ssecomi")
    (set_attr "mode" "SF")])
 
 
@@ -19236,15 +20180,16 @@
    (set_attr "mode" "SF")])
 
 (define_insn "cvtsi2ss"
-  [(set (match_operand:V4SF 0 "register_operand" "=x")
+  [(set (match_operand:V4SF 0 "register_operand" "=x,x")
 	(vec_merge:V4SF
-	 (match_operand:V4SF 1 "register_operand" "0")
+	 (match_operand:V4SF 1 "register_operand" "0,0")
 	 (vec_duplicate:V4SF
-	  (float:SF (match_operand:SI 2 "nonimmediate_operand" "rm")))
+	  (float:SF (match_operand:SI 2 "nonimmediate_operand" "r,rm")))
 	 (const_int 14)))]
   "TARGET_SSE"
   "cvtsi2ss\t{%2, %0|%0, %2}"
-  [(set_attr "type" "ssecvt")
+  [(set_attr "type" "sseicvt")
+   (set_attr "athlon_decode" "vector,double")
    (set_attr "mode" "SF")])
 
 (define_insn "cvtsi2ssq"
@@ -19256,18 +20201,19 @@
 	 (const_int 14)))]
   "TARGET_SSE && TARGET_64BIT"
   "cvtsi2ssq\t{%2, %0|%0, %2}"
-  [(set_attr "type" "ssecvt")
-   (set_attr "athlon_decode" "vector,vector")
+  [(set_attr "type" "sseicvt")
+   (set_attr "athlon_decode" "vector,double")
    (set_attr "mode" "SF")])
 
 (define_insn "cvtss2si"
-  [(set (match_operand:SI 0 "register_operand" "=r")
+  [(set (match_operand:SI 0 "register_operand" "=r,r")
 	(vec_select:SI
-	 (fix:V4SI (match_operand:V4SF 1 "nonimmediate_operand" "xm"))
+	 (fix:V4SI (match_operand:V4SF 1 "nonimmediate_operand" "x,m"))
 	 (parallel [(const_int 0)])))]
   "TARGET_SSE"
   "cvtss2si\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecvt")
+  [(set_attr "type" "sseicvt")
+   (set_attr "athlon_decode" "double,vector")
    (set_attr "mode" "SF")])
 
 (define_insn "cvtss2siq"
@@ -19278,19 +20224,20 @@
   "TARGET_SSE"
   "cvtss2siq\t{%1, %0|%0, %1}"
   [(set_attr "type" "ssecvt")
-   (set_attr "athlon_decode" "vector,vector")
+   (set_attr "athlon_decode" "double,vector")
    (set_attr "mode" "SF")])
 
 (define_insn "cvttss2si"
-  [(set (match_operand:SI 0 "register_operand" "=r")
+  [(set (match_operand:SI 0 "register_operand" "=r,r")
 	(vec_select:SI
-	 (unspec:V4SI [(match_operand:V4SF 1 "nonimmediate_operand" "xm")]
+	 (unspec:V4SI [(match_operand:V4SF 1 "nonimmediate_operand" "x,xm")]
 		      UNSPEC_FIX)
 	 (parallel [(const_int 0)])))]
   "TARGET_SSE"
   "cvttss2si\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecvt")
-   (set_attr "mode" "SF")])
+  [(set_attr "type" "sseicvt")
+   (set_attr "mode" "SF")
+   (set_attr "athlon_decode" "double,vector")])
 
 (define_insn "cvttss2siq"
   [(set (match_operand:DI 0 "register_operand" "=r,r")
@@ -19302,7 +20249,7 @@
   "cvttss2siq\t{%1, %0|%0, %1}"
   [(set_attr "type" "ssecvt")
    (set_attr "mode" "SF")
-   (set_attr "athlon_decode" "vector,vector")])
+   (set_attr "athlon_decode" "double,vector")])
 
 
 ;; MMX insns
@@ -20096,7 +21043,7 @@
       output_asm_insn (\"movaps\\t{%5, %4|%4, %5}\", operands);
     }
   ASM_OUTPUT_INTERNAL_LABEL (asm_out_file, \"L\",
-			     CODE_LABEL_NUMBER (operands[3]));
+  			     CODE_LABEL_NUMBER (operands[3]));
   RET;
 }
   "
@@ -20325,7 +21272,7 @@
   [(set_attr "type" "mmxshft")
    (set_attr "mode" "TI")])
 
-;; 3DNow reciprical and sqrt
+;; 3DNow reciprocal and sqrt
  
 (define_insn "pfrcpv2sf2"
   [(set (match_operand:V2SF 0 "register_operand" "=y")
@@ -20719,7 +21666,7 @@
 		       (parallel [(const_int 0)]))))]
   "TARGET_SSE2"
   "comisd\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecmp")
+  [(set_attr "type" "ssecomi")
    (set_attr "mode" "DF")])
 
 (define_insn "sse2_ucomi"
@@ -20732,7 +21679,7 @@
 			 (parallel [(const_int 0)]))))]
   "TARGET_SSE2"
   "ucomisd\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecmp")
+  [(set_attr "type" "ssecomi")
    (set_attr "mode" "DF")])
 
 ;; SSE Strange Moves.
@@ -20901,7 +21848,7 @@
 			       (parallel [(const_int 0)]))))]
   "TARGET_SSE2"
   "cvtsd2si\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecvt")
+  [(set_attr "type" "sseicvt")
    (set_attr "mode" "SI")])
 
 (define_insn "cvtsd2siq"
@@ -20914,13 +21861,14 @@
    (set_attr "mode" "SI")])
 
 (define_insn "cvttsd2si"
-  [(set (match_operand:SI 0 "register_operand" "=r")
-	(unspec:SI [(vec_select:DF (match_operand:V2DF 1 "register_operand" "xm")
+  [(set (match_operand:SI 0 "register_operand" "=r,r")
+	(unspec:SI [(vec_select:DF (match_operand:V2DF 1 "register_operand" "x,xm")
 				   (parallel [(const_int 0)]))] UNSPEC_FIX))]
   "TARGET_SSE2"
   "cvttsd2si\t{%1, %0|%0, %1}"
-  [(set_attr "type" "ssecvt")
-   (set_attr "mode" "SI")])
+  [(set_attr "type" "sseicvt")
+   (set_attr "mode" "SI")
+   (set_attr "athlon_decode" "double,vector")])
 
 (define_insn "cvttsd2siq"
   [(set (match_operand:DI 0 "register_operand" "=r,r")
@@ -20930,19 +21878,20 @@
   "cvttsd2siq\t{%1, %0|%0, %1}"
   [(set_attr "type" "ssecvt")
    (set_attr "mode" "DI")
-   (set_attr "athlon_decode" "vector,vector")])
+   (set_attr "athlon_decode" "double,vector")])
 
 (define_insn "cvtsi2sd"
-  [(set (match_operand:V2DF 0 "register_operand" "=x")
-	(vec_merge:V2DF (match_operand:V2DF 1 "register_operand" "0")
+  [(set (match_operand:V2DF 0 "register_operand" "=x,x")
+	(vec_merge:V2DF (match_operand:V2DF 1 "register_operand" "0,0")
 	 		(vec_duplicate:V2DF
 			  (float:DF
-			    (match_operand:SI 2 "nonimmediate_operand" "rm")))
+			    (match_operand:SI 2 "nonimmediate_operand" "r,rm")))
 			(const_int 2)))]
   "TARGET_SSE2"
   "cvtsi2sd\t{%2, %0|%0, %2}"
-  [(set_attr "type" "ssecvt")
-   (set_attr "mode" "DF")])
+  [(set_attr "type" "sseicvt")
+   (set_attr "mode" "DF")
+   (set_attr "athlon_decode" "double,direct")])
 
 (define_insn "cvtsi2sdq"
   [(set (match_operand:V2DF 0 "register_operand" "=x,x")
@@ -20955,20 +21904,21 @@
   "cvtsi2sdq\t{%2, %0|%0, %2}"
   [(set_attr "type" "ssecvt")
    (set_attr "mode" "DF")
-   (set_attr "athlon_decode" "vector,direct")])
+   (set_attr "athlon_decode" "double,direct")])
 
 ;; Conversions between SF and DF
 
 (define_insn "cvtsd2ss"
-  [(set (match_operand:V4SF 0 "register_operand" "=x")
-	(vec_merge:V4SF (match_operand:V4SF 1 "register_operand" "0")
+  [(set (match_operand:V4SF 0 "register_operand" "=x,x")
+	(vec_merge:V4SF (match_operand:V4SF 1 "register_operand" "0,0")
 	 		(vec_duplicate:V4SF
 			  (float_truncate:V2SF
-			    (match_operand:V2DF 2 "register_operand" "xm")))
+			    (match_operand:V2DF 2 "nonimmediate_operand" "x,xm")))
 			(const_int 14)))]
   "TARGET_SSE2"
   "cvtsd2ss\t{%2, %0|%0, %2}"
   [(set_attr "type" "ssecvt")
+   (set_attr "athlon_decode" "vector,double")
    (set_attr "mode" "SF")])
 
 (define_insn "cvtss2sd"
@@ -20976,7 +21926,7 @@
 	(vec_merge:V2DF (match_operand:V2DF 1 "register_operand" "0")
 	 		(float_extend:V2DF
 			  (vec_select:V2SF
-			    (match_operand:V4SF 2 "register_operand" "xm")
+			    (match_operand:V4SF 2 "nonimmediate_operand" "xm")
 			    (parallel [(const_int 0)
 				       (const_int 1)])))
 			(const_int 2)))]
@@ -21252,10 +22202,20 @@
 (define_insn "sse2_clrti"
   [(set (match_operand:TI 0 "register_operand" "=x") (const_int 0))]
   "TARGET_SSE2"
-  "pxor\t{%0, %0|%0, %0}"
-  [(set_attr "type" "sseiadd")
+{
+  if (get_attr_mode (insn) == MODE_TI)
+    return "pxor\t%0, %0";
+  else
+    return "xorps\t%0, %0";
+}
+  [(set_attr "type" "ssemov")
    (set_attr "memory" "none")
-   (set_attr "mode" "TI")])
+   (set (attr "mode")
+	      (if_then_else
+		(ne (symbol_ref "optimize_size")
+		    (const_int 0))
+		(const_string "V4SF")
+		(const_string "TI")))])
 
 ;; MMX unsigned averages/sum of absolute differences
 
@@ -21635,10 +22595,10 @@
 (define_insn "sse2_unpckhpd"
   [(set (match_operand:V2DF 0 "register_operand" "=x")
 	(vec_concat:V2DF
-	 (vec_select:V2DF (match_operand:V2DF 1 "register_operand" "0")
-			  (parallel [(const_int 1)]))
-	 (vec_select:V2DF (match_operand:V2DF 2 "register_operand" "x")
-			  (parallel [(const_int 0)]))))]
+	 (vec_select:DF (match_operand:V2DF 1 "register_operand" "0")
+			(parallel [(const_int 1)]))
+	 (vec_select:DF (match_operand:V2DF 2 "register_operand" "x")
+			(parallel [(const_int 0)]))))]
   "TARGET_SSE2"
   "unpckhpd\t{%2, %0|%0, %2}"
   [(set_attr "type" "ssecvt")
@@ -21647,10 +22607,10 @@
 (define_insn "sse2_unpcklpd"
   [(set (match_operand:V2DF 0 "register_operand" "=x")
 	(vec_concat:V2DF
-	 (vec_select:V2DF (match_operand:V2DF 1 "register_operand" "0")
-			  (parallel [(const_int 0)]))
-	 (vec_select:V2DF (match_operand:V2DF 2 "register_operand" "x")
-			  (parallel [(const_int 1)]))))]
+	 (vec_select:DF (match_operand:V2DF 1 "register_operand" "0")
+			(parallel [(const_int 0)]))
+	 (vec_select:DF (match_operand:V2DF 2 "register_operand" "x")
+			(parallel [(const_int 1)]))))]
   "TARGET_SSE2"
   "unpcklpd\t{%2, %0|%0, %2}"
   [(set_attr "type" "ssecvt")
--- gcc-3.3.1/gcc/config/ia64/ia64.c.hammer-branch	2003-07-30 09:37:51.000000000 +0200
+++ gcc-3.3.1/gcc/config/ia64/ia64.c	2003-08-05 18:22:46.000000000 +0200
@@ -1559,7 +1559,7 @@ ia64_split_call (retval, addr, retaddr, 
 
       is_desc = true;
       addr = scratch_b;
-    }
+	    }
 
   if (sibcall_p)
     insn = gen_sibcall_nogp (addr);
--- gcc-3.3.1/gcc/cp/Make-lang.in.hammer-branch	2003-06-23 22:31:53.000000000 +0200
+++ gcc-3.3.1/gcc/cp/Make-lang.in	2003-08-05 18:22:46.000000000 +0200
@@ -220,6 +220,10 @@ c++.stage3: stage3-start
 	-mv cp/*$(objext) stage3/cp
 c++.stage4: stage4-start
 	-mv cp/*$(objext) stage4/cp
+c++.stageprofile: stageprofile-start
+	-mv cp/*$(objext) stageprofile/cp
+c++.stagefeedback: stagefeedback-start
+	-mv cp/*$(objext) stagefeedback/cp
 
 #
 # .o: .h dependencies.
--- gcc-3.3.1/gcc/doc/install.texi.hammer-branch	2003-07-30 09:37:53.000000000 +0200
+++ gcc-3.3.1/gcc/doc/install.texi	2003-08-05 18:22:46.000000000 +0200
@@ -1161,6 +1161,24 @@ following commands (assuming @command{ma
 Currently, when compiling the Ada front end, you cannot use the parallel
 build feature described in the previous section.
 
+@section Building with profile feedback
+
+It is possible to use profile feedback to optimize the compiler itself.  This
+should result in a faster compiler binary.  Experiments done on x86 using gcc
+3.3 showed approximately 7 percent speedup on compiling C programs.  To
+bootstrap compiler with profile feedback, use @code{make profiledbootstrap}.
+
+When @samp{make profiledbootstrap} is run, it will first build a @code{stage1}
+compiler.  This compiler is used to build a @code{stageprofile} compiler
+instrumented to collect execution counts of instruction and branch
+probabilities.  Then runtime libraries are compiled with profile collected.
+Finally a @code{stagefeedback} compiler is built using the information collected.
+
+Unlike @samp{make bootstrap} several additional restrictions apply.  The
+compiler used to build @code{stage1} needs to support a 64-bit integral type.
+It is recommended to only use GCC for this.  Also parallel make is currently
+not supported since collisions in profile collecting may occur.
+
 @html
 <hr />
 <p>
--- gcc-3.3.1/gcc/doc/invoke.texi.hammer-branch	2003-07-23 18:14:20.000000000 +0200
+++ gcc-3.3.1/gcc/doc/invoke.texi	2003-08-05 18:22:46.000000000 +0200
@@ -3173,6 +3173,9 @@ with @samp{r}.
 @item y
 @opindex dy
 Dump debugging information during parsing, to standard error.
+@item Z
+@opindex dZ
+Dump after web construction pass, to @file{@var{file}.10.web}.
 @end table
 
 @item -fdump-unnumbered
@@ -3432,7 +3435,8 @@ invoking @option{-O2} on programs that u
 @item -O3
 @opindex O3
 Optimize yet more.  @option{-O3} turns on all optimizations specified by
-@option{-O2} and also turns on the @option{-finline-functions} and
+@option{-O2} and also turns on the @option{-finline-functions}, @option{-fweb},
+@option{-funit-at-a-time}, @option{-ftracer}, @option{-funswitch-loops} and
 @option{-frename-registers} options.
 
 @item -O0
@@ -4063,6 +4067,15 @@ will most benefit processors with lots o
 make debugging impossible, since variables will no longer stay in
 a ``home register''.
 
+@item -fweb
+@opindex fweb
+Constructs webs as commonly used for register allocation purposes and assign
+each web individual pseudo register.  This allows our register allocation pass
+to operate on pseudos directly, but also strengthens several other optimization
+passes, such as CSE, loop optimizer and trivial dead code remover.  It can,
+however, make debugging impossible, since variables will no longer stay in a
+``home register''.
+
 Enabled at levels @option{-O3}.
 
 @item -fno-cprop-registers
--- gcc-3.3.1/gcc/doc/passes.texi.hammer-branch	2003-03-25 14:31:27.000000000 +0100
+++ gcc-3.3.1/gcc/doc/passes.texi	2003-08-05 18:22:46.000000000 +0200
@@ -370,6 +370,18 @@ The option @option{-dL} causes a debuggi
 this pass.  This dump file's name is made by appending @samp{.loop} to
 the input file name.
 
+@cindex web construction
+@item
+Simple optimization pass that splits indepdendent uses of each pseudo
+increasing effectivity of other optimizations.  This can improve effect of the
+other transformation, such as CSE or register allocation.
+Its source files are @file{web.c}.
+
+@opindex dZ
+The option @option{-dZ} causes a debugging dump of the RTL code after
+this pass.  This dump file's name is made by appending @samp{.web} to
+the input file name.
+
 @item
 @opindex frerun-cse-after-loop
 If @option{-frerun-cse-after-loop} was enabled, a second common
--- gcc-3.3.1/gcc/doc/tm.texi.hammer-branch	2003-06-09 23:37:53.000000000 +0200
+++ gcc-3.3.1/gcc/doc/tm.texi	2003-08-05 18:22:46.000000000 +0200
@@ -4724,6 +4724,15 @@ Define this macro if GCC should generate
 (and System V) library functions @code{memcpy}, @code{memmove} and
 @code{memset} rather than the BSD functions @code{bcopy} and @code{bzero}.
 
+@findex TARGET_C99_FUNCTIONS
+@cindex C99 math functions, implicit usage
+@item TARGET_C99_FUNCTIONS
+When this macro is nonzero, GCC will implicitly optimize @code{sin} calls into
+@code{sinf} and similary for other functions defined by C99 standard.  The
+default is nonzero that should be proper value for most modern systems, however
+number of existing systems lacks support for these functions in the runtime so
+they needs this macro to be redefined to 0.
+
 @findex LIBGCC_NEEDS_DOUBLE
 @item LIBGCC_NEEDS_DOUBLE
 Define this macro if @code{float} arguments cannot be passed to library
--- gcc-3.3.1/gcc/f/Make-lang.in.hammer-branch	2003-07-16 10:27:42.000000000 +0200
+++ gcc-3.3.1/gcc/f/Make-lang.in	2003-08-05 18:22:46.000000000 +0200
@@ -63,7 +63,8 @@ F77 f77: f771$(exeext)
   f77.install-common f77.install-info f77.install-man \
   f77.uninstall f77.mostlyclean f77.clean f77.distclean \
   f77.extraclean f77.maintainer-clean f77.rebuilt \
-  f77.stage1 f77.stage2 f77.stage3 f77.stage4
+  f77.stage1 f77.stage2 f77.stage3 f77.stage4 \
+  f77.stageprofile f77.stagefeedback
 
 g77spec.o: $(srcdir)/f/g77spec.c $(SYSTEM_H) $(GCC_H) \
 	$(CONFIG_H)
@@ -344,6 +345,11 @@ f77.stage3: stage3-start
 f77.stage4: stage4-start
 	-mv -f $(G77STAGESTUFF) stage4/f
 
+f77.stageprofile: stageprofile-start
+	-mv -f $(G77STAGESTUFF) stageprofile/f
+
+f77.stagefeedback: stageprofile-start
+	-mv -f $(G77STAGESTUFF) stagefeedback/f
 #
 # .o: .h dependencies.
 
--- gcc-3.3.1/gcc/java/Make-lang.in.hammer-branch	2003-07-16 10:27:46.000000000 +0200
+++ gcc-3.3.1/gcc/java/Make-lang.in	2003-08-05 18:22:46.000000000 +0200
@@ -256,6 +256,10 @@ java.stage3: stage3-start
 	-mv java/*$(objext) stage3/java
 java.stage4: stage4-start
 	-mv java/*$(objext) stage4/java
+java.stageprofile: stageprofile-start
+	-mv java/*$(objext) stageprofile/java
+java.stagefeedback: stageprofile-start
+	-mv java/*$(objext) stagefeedback/java
 
 #
 # .o:.h dependencies.
--- gcc-3.3.1/gcc/java/builtins.c.hammer-branch	2003-03-04 15:37:22.000000000 +0100
+++ gcc-3.3.1/gcc/java/builtins.c	2003-08-05 18:22:46.000000000 +0200
@@ -72,7 +72,7 @@ static tree build_function_call_expr PAR
 static void define_builtin PARAMS ((enum built_in_function,
 				    const char *,
 				    enum built_in_class,
-				    tree, int));
+				    tree, int, int));
 static tree define_builtin_type PARAMS ((int, int, int, int, int));
 
 
@@ -193,12 +193,13 @@ sqrt_builtin (method_return_type, method
 
 /* Define a single builtin.  */
 static void
-define_builtin (val, name, class, type, fallback_p)
+define_builtin (val, name, class, type, fallback_p, implicit)
      enum built_in_function val;
      const char *name;
      enum built_in_class class;
      tree type;
      int fallback_p;
+     int implicit;
 {
   tree decl;
 
@@ -218,6 +219,8 @@ define_builtin (val, name, class, type, 
   DECL_BUILT_IN_CLASS (decl) = class;
   DECL_FUNCTION_CODE (decl) = val;
   built_in_decls[val] = decl;
+  if (implicit)
+    implicit_built_in_decls[val] = decl;
 }
 
 /* Compute the type for a builtin.  */
@@ -318,8 +321,8 @@ initialize_builtins ()
 #include "builtin-types.def"
 
 #define DEF_BUILTIN(ENUM, NAME, CLASS, TYPE, LIBTYPE, BOTH_P, \
-                    FALLBACK_P, NONANSI_P, ATTRS) \
-  define_builtin (ENUM, NAME, CLASS, builtin_types[TYPE], FALLBACK_P);
+                    FALLBACK_P, NONANSI_P, ATTRS, IMPLICIT) \
+  define_builtin (ENUM, NAME, CLASS, builtin_types[TYPE], FALLBACK_P, IMPLICIT);
 #include "builtins.def"
 }
 
--- gcc-3.3.1/gcc/objc/Make-lang.in.hammer-branch	2002-08-08 11:10:39.000000000 +0200
+++ gcc-3.3.1/gcc/objc/Make-lang.in	2003-08-05 18:22:46.000000000 +0200
@@ -158,3 +158,9 @@ objc.stage3: stage3-start
 objc.stage4: stage4-start
 	-mv objc/*$(objext) stage4/objc
 	-mv cc1obj$(exeext) stage4
+objc.stageprofile: stageprofile-start
+	-mv objc/*$(objext) stageprofile/objc
+	-mv cc1obj$(exeext) stageprofile
+objc.stagefeedback: stagefeedback-start
+	-mv objc/*$(objext) stagefeedback/objc
+	-mv cc1obj$(exeext) stagefeedback
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/990208-1.c.hammer-branch	2000-05-12 18:51:20.000000000 +0200
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/990208-1.c	2003-08-05 18:22:46.000000000 +0200
@@ -14,16 +14,19 @@ static __inline__ void doit(void **pptr,
   }
 }
 
+__attribute__ ((noinline))
 static void f(int cond)
 {
   doit (&ptr1, cond);
 }
 
+__attribute__ ((noinline))
 static void g(int cond)
 {
   doit (&ptr2, cond);
 }
 
+__attribute__ ((noinline))
 static void bar(void);
 
 int main()
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/eeprof-1.c.hammer-branch	2000-06-29 05:10:00.000000000 +0200
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/eeprof-1.c	2003-08-05 18:22:46.000000000 +0200
@@ -5,13 +5,16 @@ int entry_calls, exit_calls;
 void (*last_fn_entered)();
 void (*last_fn_exited)();
 
+__attribute__ ((noinline))
 int main () NOCHK;
 
+__attribute__ ((noinline))
 void foo ()
 {
   ASSERT (last_fn_entered == foo);
 }
 
+__attribute__ ((noinline))
 static void foo2 ()
 {
   ASSERT (entry_calls == 1 && exit_calls == 0);
@@ -22,6 +25,7 @@ static void foo2 ()
   ASSERT (last_fn_exited == foo);
 }
 
+__attribute__ ((noinline))
 void nfoo (void) NOCHK;
 void nfoo ()
 {
@@ -55,11 +59,13 @@ int main ()
 void __cyg_profile_func_enter (void (*fn)(), void (*parent)()) NOCHK;
 void __cyg_profile_func_exit (void (*fn)(), void (*parent)()) NOCHK;
 
+__attribute__ ((noinline))
 void __cyg_profile_func_enter (void (*fn)(), void (*parent)())
 {
   entry_calls++;
   last_fn_entered = fn;
 }
+__attribute__ ((noinline))
 void __cyg_profile_func_exit (void (*fn)(), void (*parent)())
 {
   exit_calls++;
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/stdio-opt-1.c.hammer-branch	2003-03-24 02:26:29.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/stdio-opt-1.c	2003-08-05 18:22:46.000000000 +0200
@@ -58,6 +58,7 @@ int main()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static int
 fputs(const char *string, FILE *stream)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/stdio-opt-2.c.hammer-branch	2001-01-28 02:27:25.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/stdio-opt-2.c	2003-08-05 18:22:46.000000000 +0200
@@ -45,6 +45,7 @@ int main()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static int
 printf (const char *string, ...)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/stdio-opt-3.c.hammer-branch	2001-01-08 00:15:47.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/stdio-opt-3.c	2003-08-05 18:22:46.000000000 +0200
@@ -57,6 +57,7 @@ int main()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static int
 fprintf (FILE *stream, const char *string, ...)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-1.c.hammer-branch	2000-12-27 16:29:52.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-1.c	2003-08-05 18:22:46.000000000 +0200
@@ -37,6 +37,7 @@ int main()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static char *
 strstr(const char *s1, const char *s2)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-10.c.hammer-branch	2000-12-27 16:29:52.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-10.c	2003-08-05 18:22:46.000000000 +0200
@@ -76,6 +76,7 @@ int main ()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static char *
 strncat (char *s1, const char *s2, size_t n)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-11.c.hammer-branch	2000-12-27 16:29:52.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-11.c	2003-08-05 18:22:46.000000000 +0200
@@ -58,6 +58,7 @@ int main ()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static size_t
 strspn (const char *s1, const char *s2)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-12.c.hammer-branch	2000-12-27 16:29:52.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-12.c	2003-08-05 18:22:46.000000000 +0200
@@ -58,6 +58,7 @@ int main ()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static size_t
 strcspn (const char *s1, const char *s2)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-13.c.hammer-branch	2001-11-15 00:37:31.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-13.c	2003-08-05 18:22:46.000000000 +0200
@@ -49,6 +49,7 @@ main ()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static size_t
 strlen (const char *s)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-14.c.hammer-branch	2001-12-13 01:43:51.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-14.c	2003-08-05 18:22:46.000000000 +0200
@@ -25,12 +25,14 @@ main ()
    something else.  So any remaining calls to the original function
    should abort.  */
 
+__attribute__ ((noinline))
 static void *
 memset (void *s, int c, size_t n)
 {
   abort ();
 }
 
+__attribute__ ((noinline))
 static void *
 memcpy (void *dest, const void *src, size_t n)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-15.c.hammer-branch	2001-12-13 01:43:51.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-15.c	2003-08-05 18:22:46.000000000 +0200
@@ -37,6 +37,7 @@ main ()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static int
 memcmp (const char *p1, const char *p2, size_t len)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-16.c.hammer-branch	2001-12-28 00:34:31.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-16.c	2003-08-05 18:22:46.000000000 +0200
@@ -28,6 +28,7 @@ main (int argc)
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static int
 memcmp (const void *s1, const void *s2, size_t len)
 {
@@ -36,6 +37,7 @@ memcmp (const void *s1, const void *s2, 
 #else
 /* When not optimizing, the above tests may generate references to
    the function link_error, but should never actually call it.  */
+__attribute__ ((noinline))
 static void
 link_error ()
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-17.c.hammer-branch	2002-04-23 12:16:48.000000000 +0200
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-17.c	2003-08-05 18:22:46.000000000 +0200
@@ -40,6 +40,7 @@ main (int argc)
 /* When optimizing, most of the above cases should be transformed into
    something else.  So any remaining calls to the original function
    for short lengths should abort.  */
+__attribute__ ((noinline))
 static void *
 memset (void *dst, int c, size_t len)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-2.c.hammer-branch	2000-12-27 16:29:52.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-2.c	2003-08-05 18:22:46.000000000 +0200
@@ -46,6 +46,7 @@ int main()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static char *
 strpbrk(const char *s1, const char *s2)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-3.c.hammer-branch	2000-12-27 16:29:52.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-3.c	2003-08-05 18:22:46.000000000 +0200
@@ -104,18 +104,21 @@ rindex (const char *s, int c)
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static __SIZE_TYPE__
 strlen (const char *s)
 {
   abort ();
 }
 
+__attribute__ ((noinline))
 static int
 strcmp (const char *s1, const char *s2)
 {
   abort ();
 }
 
+__attribute__ ((noinline))
 static char *
 strrchr (const char *s, int c)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-4.c.hammer-branch	2000-12-27 16:29:52.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-4.c	2003-08-05 18:22:46.000000000 +0200
@@ -53,6 +53,7 @@ index (const char *s, int c)
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static char *
 strchr (const char *s, int c)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-6.c.hammer-branch	2000-12-27 16:29:52.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-6.c	2003-08-05 18:22:46.000000000 +0200
@@ -45,6 +45,7 @@ int main()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static char *
 strcpy (char *d, const char *s)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-7.c.hammer-branch	2000-12-27 16:29:52.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-7.c	2003-08-05 18:22:46.000000000 +0200
@@ -69,6 +69,7 @@ int main ()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static char *
 strncpy(char *s1, const char *s2, size_t n)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-8.c.hammer-branch	2002-06-27 20:23:33.000000000 +0200
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-8.c	2003-08-05 18:22:46.000000000 +0200
@@ -231,6 +231,7 @@ int main ()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static int
 strncmp(const char *s1, const char *s2, size_t n)
 {
--- gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-9.c.hammer-branch	2000-12-27 16:29:52.000000000 +0100
+++ gcc-3.3.1/gcc/testsuite/gcc.c-torture/execute/string-opt-9.c	2003-08-05 18:22:46.000000000 +0200
@@ -46,6 +46,7 @@ int main ()
 /* When optimizing, all the above cases should be transformed into
    something else.  So any remaining calls to the original function
    should abort.  */
+__attribute__ ((noinline))
 static char *
 strcat (char *s1, const char *s2)
 {
--- gcc-3.3.1/gcc/treelang/Make-lang.in.hammer-branch	2003-07-08 15:28:41.000000000 +0200
+++ gcc-3.3.1/gcc/treelang/Make-lang.in	2003-08-05 18:22:46.000000000 +0200
@@ -255,6 +255,10 @@ treelang.stage3: stage3-start
 	-mv treelang/*$(objext) stage3/treelang
 treelang.stage4: stage4-start
 	-mv treelang/*$(objext) stage4/treelang
+treelang.stageprofile: stageprofile-start
+	-mv treelang/*$(objext) stageprofile/treelang
+treelang.stagefeedback: stagefeedback-start
+	-mv treelang/*$(objext) stagefeedback/treelang
 #
 # Maintenance hooks:
 
--- gcc-3.3.1/gcc/ChangeLog.hammer.hammer-branch	2003-08-05 18:22:46.000000000 +0200
+++ gcc-3.3.1/gcc/ChangeLog.hammer	2003-08-05 18:22:46.000000000 +0200
@@ -0,0 +1,1380 @@
+Tue May 20 12:12:42 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (gcc_3_3_release)
+
+Sat May 10 21:17:37 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* fold-const.c (fold):  Fix fp compare optimization.
+
+Thu May  1 11:39:07 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20030430)
+
+Sun Apr 20 18:25:48 CEST 2003  Jan Hubicka  <jH@suse.cz>
+
+	Backport Richard Henderson's from the mainline:
+
+        * emit-rtl.c (try_split): Handle 1-1 splits of call insns properly.
+
+        * config/ia64/ia64.c (TARGET_FUNCTION_OK_FOR_SIBCALL): New.
+        (ia64_gp_save_reg): Remove.
+        (struct ia64_frame_info): Move to the beginning of the file;
+        add reg_save_gp.
+        (ia64_expand_call): Rearrange for new call patterns.
+        (ia64_reload_gp): New.
+        (ia64_split_call): New.
+        (ia64_compute_frame_size): Allocate reg_save_gp.
+        (ia64_expand_prologue): Save reg_save_gp.
+        (ia64_expand_epilogue): Don't restore gp.
+        (ia64_hard_regno_rename_ok): Remove R4 hack.
+        (ia64_function_ok_for_sibcall): New.
+        (ia64_output_mi_thunk): Set reload_completed, no_new_pseudos;
+        call try_split on sibcall pattern.
+        * config/ia64/ia64-protos.h: Update.
+        * config/ia64/ia64.md (call_nogp, call_value_nogp, sibcall_nogp):
+        Rename from nopic versions.  Confiscate 2nd argument to call as
+        a marker.
+        (call_pic, call_value_pic, sibcall_pic): Remove.
+        (call_gp, call_value_gp, sibcall_gp): New.
+        (builtin_setjmp_setup): Remove.
+        (builtin_setjmp_receiver): Call ia64_reload_gp.
+
+Sun Apr 20 17:42:23 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20030421)
+
+Mon Apr 14 22:40:51 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20030414)
+
+Fri Apr 11 23:58:08 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20030411)
+
+Mon Apr 10 21:19:38 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* profile.c (compute_branch_probabilities): Be even more permisive.
+
+Mon Apr  7 21:49:58 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20030407)
+
+Mon Apr  7 21:19:38 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* profile.c (compute_branch_probabilities): Accept negative counts after calls.
+	(branch_prob): Do not add extra edges for setjmp.
+
+Sun Apr  6 19:26:32 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Makefile.in:  Use --static when bulding profilestage
+	* config/i386/i386.c: Avoid warnings.
+
+Thu Apr  3 09:47:28 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* cfgrtl.c (update_bb_for_insn): Do not set block for barriers.
+
+Wed Apr  2 23:57:03 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* install.tex: Document profiledbootstrap.
+
+	* Makefile.in (profiledbootstrap): New target
+
+	* Makefile.in (clean, distclean): Kill new stages
+	(POSTSTAGE1_FLAGS_TO_PASS): Break from ...
+	(STAGE2_FLAGS_TO_PASS): ... this one.
+	(STAGEPROFILE_FLAGS_TO_PASS, STAGEFEEDBACK_FLAGS_TO_PASS): New.
+	(stage[2-4]_build): Add POSTSTAGE1_FLAGS_TO_PASS.
+	(stageprofile_build, stageprofile_copy, stagefeedback_build,
+	stagefeedback_copy): New.
+	(restageprofile, restagefeedback, stageprofile-start, 
+	stageprofile, stagefeedback-start): Likewise.
+
+	* Make-lang.in:  Add support for stageprofile and stagefeedback
+
+Wed Apr  2 23:54:55 CEST 2003  Jan Hubicka  <jh@suse.cz>
+
+	* i386.md:  Temporarily replace rep ; ret by nop ; ret.
+
+Fri Mar 28 12:37:47 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20030328)
+
+Wed Mar 26 00:37:13 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* i386.md: New splitters to canonicalize order of arguments for AND.
+
+Tue Mar 25 23:13:49 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* i386.md (test patterns):  Allow memory operand in operand1.
+
+Tue Mar 25 20:57:40 CET 2003  Jan Hubicka <jh@suse.cz>
+
+	* i386.h (RTX_COST): Fix for -fpic.
+	* i386.c (k8_avoid_jump_misspredicts): Fix.
+
+Mon Mar 24 00:07:19 CET 2003  Jan Hubicka  <jh@suse.cz.
+
+	* i386.md (align): New pattern.
+	* i386.c (min_insn_size, k8_avoid_jump_misspredicts): New functions.
+
+Fri Mar 21 19:35:07 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from mainline:
+	Richard Henderson
+		* Makefile.in (cfglayout.o): Depend on TARGET_H.
+		* cfglayout.c: Include target.h.
+		(cfg_layout_can_duplicate_bb_p): Check targetm.cannot_copy_insn_p.
+		* target-def.h (TARGET_CANNOT_COPY_INSN_P): New.
+		* target.h (struct gcc_target): Add cannot_copy_insn_p.
+
+		* config/alpha/alpha.c (alpha_cannot_copy_insn_p): New.
+		(TARGET_CANNOT_COPY_INSN_P): New.
+		(override_options): Revert 2003-02-08 hack.
+
+Thu Mar 20 13:36:00 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* callgraph.c (known_fns): make extern.
+	* function.c (known_fns): make global; fix GTY marker.
+
+Tue Mar 18 21:05:45 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from mainline (hammer-3_3-merge-20030318)
+
+Mon Mar 17 15:02:17 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* i386.md (cvtts?2si peep2): New.
+
+Wed Mar 12 09:40:47 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge form mainline (hammer-3_3-merge-20030212)
+
+Mon Mar 10 15:43:30 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* i386.c (x86_machine_dependent_reorg):  Replace rep instead of padding.
+	* i386.md (UNSPEC_REP): New constant.
+	(return_internal_long): New pattern.
+
+Mon Mar 10 00:13:18 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge form mainline (hammer-3_3-merge-20030309)
+
+Sun Mar  9 12:08:18 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* toplev.c (rest_of_compilation):  Do split insns after post-reload
+	copy propagation.
+
+	* toplev.c (rest_of_compilation): re-enable insn splitting after
+	reg-stack.
+
+	* callgraph.c (known_fns): Declare.
+	(cgraph_node): Add the decl into varray.
+	* function.c (known_fns): New static variable.
+
+Fri Mar  7 12:55:23 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* i386.c (ix86_expand_vector_move): Do not crash when offloading
+	to memory in PIC mode.
+
+2003-03-05  Michael Matz  <matz@suse.de>
+
+	* unwind.h: Add the GPL exception.
+	* Makefile.in (USER_H): Add unwind.h.
+
+Tue Mar  4 23:45:34 CET 2003  Jan Hubicka   <jh@suse.cz>
+
+	Merge from 3.3-branch (hammer-3_3-merge-20030304)
+
+Mon Mar  3 16:35:42 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	Merge from mainline:
+	2003-02-24  Jeff Law  <law@redhat.com>
+
+		* i386.md (testdi_1_rex64): Discourage reload from using the %eax
+		alternative.
+		(testsi_1, testhi_1, testqi_1): Likewise.
+
+	2003-02-22  Josef Zlomek  <zlomekj@suse.cz>
+
+		* config/i386/i386.md: Use gen_lowpart instead of gen_rtx_REG
+		for copying a register.
+
+	Wed Feb 19 17:22:51 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (movsfcc_1, movdfcc_1): Fix constrains.
+
+2003-03-02  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+	* profile.c (compute_value_histograms): Fix.
+	* toplev.c (dump_file): Choose usable flag for vpt dump.
+
+Thu Feb 27 12:31:33 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20030226)
+
+Wed Feb 26 19:20:55 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* optabs.c (expand_fix): Do not widen the input operand.
+
+Mon Feb 24 22:14:07 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* combine.c (simplify_shift_const):  Simplify few special cases
+	into constants.
+
+Sat Feb 22 23:04:10 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* i386.md (movabs*): Restrict to first alternative
+	* i386.c (x86_64_movabs_operand): Prohibit non-immediate operands.
+
+Sat Feb 22 22:14:09 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20030222)
+
+Sat Feb 22 20:12:57 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* expmed.c (expand_divmod): Undo sign extensions for unsigned operands
+
+Fri Feb 21 23:42:59 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* fold-const.c (fold): Fix typo.
+
+2003-02-22  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+	* gcov-io.h (GCOV_TAG_SAME_VALUE_HISTOGRAMS): New section.
+	(gcov_read_counter): Add parameter.
+	(merger_function, same_value_histograms_merger, profile_merger_for_tag):
+	New.
+	(gcov_read_summary): Add parameter to gcov_read_counter call.
+	* gcov.c (read_count_file): Ditto.
+	* libgcc.c (gcov_exit): Support profile merging hooks.
+	* profile.c (same_value_histograms_label): New.
+	(get_exec_counts): Add parameter to gcov_read_counter call.
+	(get_histogram_counts): Support profile merging hooks.
+	(instrument_values, compute_value_histograms): Place same value
+	histograms into own section.
+	(branch_prob): Call allocate_reg_info after find_values_to_profile.
+	(init_branch_prob): Create same_value_histograms_label.
+	(label_for_tag): Return the label.
+	* profile.h (MAX_COUNTER_SECTIONS): Increase.
+
+	* loop.c (loop_optimize): Avoid warning.
+	* regclass.c (cannot_change_mode_set_regs): Ditto.
+
+Thu Feb 20 16:27:56 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* c-typeck.c (build_c_cast):  Fold constant variables into
+	initial values.
+
+Thu Feb 20 16:25:49 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* expr.c (emit_group_store):  Fix crash when converting single
+	register into complex register.
+
+Thu Feb 20 14:36:55 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* gcc.c-torture/execute/20020720-1.x: XFAIL for x86-64.
+
+Wed Feb 19 17:18:06 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* i386.md (movsfcc_1, movdfcc_1): Fix constrains.
+
+Wed Feb 19 17:09:48 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* toplev.c (rest_of_compilation): Do unswitching before tracing.
+
+Wed Feb 19 14:41:02 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* gcc.c-torture/execute/990208-1.c:  Add noinline attributes as needed.
+	* gcc.c-torture/execute/eeprof-1.c:  Likewise.
+	* gcc.c-torture/execute/stdio-opt-*.c: Likewise.
+	* gcc.c-torture/execute/string-opt-*.c: Likewise.
+
+Wed Feb 19 14:25:48 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* i386.md (cosxf2):  fix conditional.
+
+Sun Feb 16 22:38:39 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* toplev.c: Rename -funit-at-time to -funit-at-a-time
+	* doc/invoke.texi: Likewise.
+
+Sun Feb 16 19:55:18 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3 branch (hammer-3_3-merge-20030216)
+
+2003-02-13  Josef Zlomek  <zlomekj@suse.cz>
+
+	* cfgcleanup.c (outgoing_edges_match): When there is single outgoing
+	edge and block ends with a jump insn it must be simple jump.
+
+Wed Feb 12 16:58:29 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* predict.c (estimate_probability):  Fix roundoff error.
+
+Tue Feb 11 19:07:08 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* combine.c (combine_simplify_rtx): Fix folding of
+	nested float_truncates.
+
+Tue Feb 11 10:04:33 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* predict.c (choose_function_section): Choose sections correctly.
+
+Mon Feb 10 21:18:38 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* c-decl.c (c_expand_deferred_function):  Optimize inlining.
+
+Mon Feb 10 11:38:17 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* combine.c (combine_simplify_rtx): Synchronize with mainline.
+
+Sun Feb  9 19:21:35 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* combine.c (combine_simplify_rtx): Simplify using
+	(float_truncate (float x)) is (float x)
+	(float_extend (float_extend x)) is (float_extend x).
+
+	* i386.md (floathi*): Deal with SSE.
+
+Sun Feb  9 17:57:46 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* toplev.c (parse_options_and_default_flags):  Enable -funswitch-loops
+	at -O3.
+
+Sat Feb  8 02:03:22 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* i386.md (ahi?v*3): Set third operand type to TImode.
+	* i386.c (ix86_expand_binop_builtin): Extend operand when needed.
+
+	* simplify-rtx.c (simplify_subreg): Fix conversion from vector into
+	integer mode.
+
+	* rtl.def (VEC_MERGE, VEC_SELECT, VEC_CONCAT, VEC_DUPLICATE):
+	Change code so they are arithmetic expressions now.
+	* simplify-rtx.c (simplify_unary_operation, simplify_binary_operation,
+	simplify_ternary_operation): Deal with VEC_* expressions.
+
+	* i386.md (vmaskcmp, pinsrw, movd patterns):  Fix RTL representation.
+
+Fri Feb  7 22:24:20 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* i386.md (sse2_nandv2di3):  Fix.
+
+	* i386.md (movdi_rex64_1): Fix mmx<->int move opcode.
+
+Fri Feb  7 12:59:54 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* passes.texi (web construction):  Document.
+	* invoke.texi (-O3): Document that -fweb is enabled.
+
+	* combine.c (combine_simplify_rtx):  Use reversed_comparison_code_parts.
+
+	* toplev.c (rest_of_compilation): Move -fweb past new loop.
+	(parse_options_and_default_flags):  Enable -fweb only at -O3.
+
+	Merge from mainline
+
+	Thu Feb  6 00:18:38 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (x86_inter_unit_moves): New variable.
+		(ix86_secondary_memory_needed): Fix 64bit case, honor
+		TARGET_INTER_UNIT_MOVES
+		* i386.h (x86_inter_unit_moves): Declare.
+		(TARGET_INTER_UNIT_MOVES): New macro.
+		* i386.md (movsi_1): Cleanup constraints; disable
+		when not doing inter-unit moves.
+		(movsi_1_nointernunit): New.
+		(movdi_1_rex64): Fix constraints; deal with SSE->GPR moves.
+		(movdi_1_rex64_nointerunit): New.
+		(mivsf_1): disable when not doing inter-unit moves.
+		(movsf_1_nointerunit): New.
+
+	Mon Feb  3 16:01:17 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (ix86_expand_int_movcc):  Fix setcc sign bit case.
+
+	Tue Jan 28 12:15:13 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (ix86_carry_flag_operator):  New predicate.
+		(fcmov_operator): Fix whitespace.
+		(ix86_expand_carry_flag_compare):  Deal with floating point.
+		(ix86_expand_int_movcc): Deal with fp; update insn expansion
+		(ix86_expand_int_addcc): Likewise.
+		(ix86_expand_strlensi_unroll_1): likewsie.
+		* i386.h (PREDICATE_CODES): Add ix86_carry_flag_operator.
+		* i386.md (add?i_carry_rex64): Use new predicate.
+		(sub?i3_carry_rex64): Likewise.
+		(x86_mov?icc_0_m1*): Likewise.
+
+	Thu Jan  9 00:57:15 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c  (ix86_expand_int_addcc): Fix thinko.
+
+	Sat Jan 25 15:55:08 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (ix86_expand_movstr, ix86_expand_clrstr): Consistently
+		do libcall for large blocks.
+		* i386.md (comi patterns): Set type to ssecomi.
+		(sse2_unpck?pd): Fix mode of vec_select.
+
+	2003-01-26  Kazu Hirata  <kazu@cs.umass.edu>
+
+		* config/i386/i386-modes.def: Fix comment typos.
+		* config/i386/i386.c: Likewise.
+		* config/i386/i386.md: Likewise.
+
+	Wed Feb  5 23:12:57 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+		* i386-protos.h (x86_emit_floatuns):  Declare.
+		* i386.c (x86_emit_floatuns): New global function.
+		* i386.md (floatunssisf2, floatunsdisf2,
+		floatunsdidf2):  New patterns.
+
+	Wed Jan  8 12:10:57 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (adddi3_carry_rex64, subdi3_carry_rex64): Name pattern.
+		(addhi3_carry, addqi3_carry, subhi3_carry, subqi3_carry): New patterns.
+		(add??cc): New expanders.
+		* i386.c (expand_int_addcc): New function.
+		* i386-protos.h (expand_int_addcc): Declare.
+
+	Sat Dec 14 20:43:41 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (flags_reg_operand):  New function.
+		* i386.h (PREDICATE_CODES): Add flags_reg_operand.
+		* i386.md (cmov splitter, movqicc): Use new predicate.
+
+	Sat Dec 14 17:03:17 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (movqicc splitter): Fix template.
+
+	Thu Dec  5 14:10:15 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (ix86_expand_prologue):  Add comment, do not use
+		fast prologues for cold and normal functions.
+
+	Thu Dec  5 00:52:37 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (x86_rep_movl_optimal): New variable.
+		(ix86_expand_movstr, ix86_expand_clrstr): Use TARGET_REP_MOVL_OPTIMAL
+		* i386.h (TARGET_REP_MOVL_OPTIMAL): New macro.
+
+		* i386.md (negsf2_ifs, negdf2_ifs, negdf2_ifs_rex64, abssf2_ifs,
+		absdf2_ifs, absdf2_ifs_rex64): Fix constraints.
+		neg?f2_ifs, abs?f2_ifs splitters): Refuse memory operand; do not
+		generate unnecesary subregs.
+
+	Wed Dec  4 11:53:07 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (ix86_expand_int_movcc): Force operand into register for QImode
+		condtiional moves.
+
+Fri Feb  7 02:35:26 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Makefile.in (web.o): New.
+	* web.c: New file.
+	* rtl.h (web_main): Declare.
+	* timervar.def (TV_WEB): New.
+	* toplev.c (dump_file_index, dump_file_info): Add DFI_web.
+	(flag_web): New static variable.
+	(lang_independent_ptions): Add "web".
+	(rest_of_compilation): Call web_main.
+	(parse_options_and_default_flags): Add flag_web.
+	* invoke.texi (-fweb): Document.
+
+Wed Feb  5 15:50:47 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from mainline (hammer-3_3-merge-20030205)
+
+Wed Feb  5 11:08:53 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Makefile.in (CRTSTUFF_CFLAGS): Add -fno-unit-at-time
+	(OBJS): Add callgraph.o
+	(callgraph.o): New.
+	* c-decl.c (expand_body_1): Break out from ...
+	(expand_body): This one;  change calling convention
+	(finish_function): Move some of expand_body logic here.
+	(c_expand_deferred_function): Update call of expand_body
+	(c_expand_stmt): Use c_expand_body_1.
+	* c-lang.c (LANG_HOOKS_CALLGRAPH_EXPAND_FUNCTION): Define.
+	* c-objc-commin.c (c_objc_common_finish_file): Use callgraph code.
+	* c-tree.h (c_expand_body): Declare.
+	* callgraph.c: New file.
+	* flags.h (flag_unit_at_time): Declare.
+	* langhooks.h (LANG_HOOKS_CALLGRAPH_LOWER_FUNCTION,
+	LANG_HOOKS_CALLGRAPH_EXPAND_FUNCTION,
+	LANG_HOOKS_CALLGRAPH_INITIALIZER): New macros.
+	* langhooks.h (struct lang_hooks_for_callgraph): New.
+	(struct lang_hooks): Add callgraph field.
+	* toplev.c (flag_unit_at_time): New.
+	(lang_independent_options): Add flag_unit_at_time.
+	(parse_options_and_default_flags): Set flag_unit_at_time for -O3.
+	(process_options): Disable unit-at-time mode for frontends not
+	supporting callgraph.
+	* tree-inline.c (typedef struct inline_data): Add "decl"
+	(inlinable_function_p): New argument nolimit.
+	(tree_inlinable_function_p): Likewise.
+	(expand_call_inline): Update callgraph.
+	(optimize_inline_calls): Set id.decl.
+	* tree-inline.h (tree_inlinable_function_p): Update prototype./
+	* tree.h (cgraph_finalize_function, cgraph_finalize_compilation_unit,
+	cgraph_create_edges, dump_cgraph, cgraph_optimize, cgraph_remove_call
+	cgraph_calls_p): Declare.
+	* i386.c (x86_optimize_local_function,
+	TARGET_CGRAPH_OPTIMIZE_LOCAL_FUNCTION): New.
+	* target-def.h (TARGET_CGRAPH_OPTIMIZE_LOCAL_FUNCTION, TARGET_CGRAPH):
+	New.
+	* target.h (struct gcc_target): Add new cgraph familly.
+
+Wed Feb  5 11:05:24 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* emit-rtl.c (emit_copy_of_insn_after): Copy INSN_CODE cache too.
+
+Mon Feb  3 15:58:28 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* i386.c (ix86_expand_int_movcc):  Fix setcc sign bit case.
+
+Mon Feb  3 01:46:53 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20030203)
+
+Sat Feb  1 20:31:24 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* reload.c:  Revert Urlich's patch.
+
+Sat Feb  1 13:20:59 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20030201)
+
+Fri Jan 31 22:40:59 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20030131)
+
+Tue Jan 28 19:47:13 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* pa.c (attr_length_millicode_call, attr_length_call): Do not crash
+	when INSN_ADDRESSES are not computed.
+
+Sun Jan 26 15:44:39 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* convert.c:  Fix merge differences.
+
+2003-01-25  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+	* config/i386/i386-protos.h (function_arg_pass_by_reference): Declare.
+	* config/i386/i386.h (FUNCTION_ARG_PASS_BY_REFERENCE): Use it.
+	* config/i386/i386.c (function_arg_pass_by_reference): New.
+	(ix86_va_arg): Support arguments passed by reference.
+
+Thu Jan 23 16:52:55 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20032301 tag.)
+
+2003-01-16  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+	* cfgloop.c (flow_loops_find): Fix handling of abnormal edges.
+
+Sun Jan 12 15:17:43 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3-branch (hammer-3_3-merge-20031201 tag.)
+
+Sat Jan 11 21:06:53 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* convert.c (strip_float_extensions):  Look for narrowest type handling
+	FP constants.
+
+	* fold-const.c (fold):  Fold (double)float1 CMP (double)float2 into
+	float1 CMP float2.
+
+	* convert.c (convert_to_real): Fold - and abs only when profitable.
+	* fold-const.c (fold): Fold truncates in - and abs.
+
+Sat Jan 11 12:21:45 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* builtins.c (fold_trunc_transparent_mathfn): New static function
+	(fold_builtin): Fold floor/trunc/round/ceil/nearbyint.
+	* convert.c (convert_to_real): Fix.
+
+Thu Jan  9 14:17:47 CET 2003  Josef Zlomek <zlomekj@suse.cz>
+
+	Merge from rtlopt branch:
+	
+	2003-01-02  Josef Zlomek <zlomekj@suse.cz>
+
+	* bb-reorder.c (copy_bb_p): Parameter code_may_grow was added.
+	(rotate_loop): Avoid uncond jump to cond jump.
+	(find_traces_1_round): copy_bb_p has one more parameter.
+	(connect_traces): Likewise.
+	(copy_bb): Allow inserting a block into a trace.
+
+	2002-12-30  Josef Zlomek <zlomekj@suse.cz>
+
+	* bb-reorder.c (connect_traces): Get rid of warning.
+
+Tue Jan  7 18:19:28 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* convert.c (convert_to_real):  Do conversion of floor and friends.
+
+Tue Jan  7 18:03:29 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	* builtins.c (DEF_BUILTIN): Accept 10 arguments.
+	(implicit_built_in_decls): New global array.
+	(mathfn_built_in): New global function.
+	(fold_trunc_transparent_mathfn): New static function
+	(expand_builtin_strstr, expand_bultin_strchr,
+	expand_builtin_strpbrk, expand_builtin_strcpy,
+	expand_builtin_strncpy, expand_bultin_strcmp,
+	expand_bultin_strncat, expand_builtin_fputs): Use
+	implicint_built_in_decls.
+	(fold_builtin): Fold floor/trunc/round/ceil/nearbyint.
+	* builtins.def: Fix comments.
+	(DEF_GCC_BUILTIN, DEF_FALLBACK_BUILTIN, DEF_EXT_FALLBACK_BUILTIN,
+	DEF_LIB_BUILTIN, DEF_LIB_ALWAYS_BUILTIN, DEF_EXT_LIB_BUILTIN,
+	DEF_C99_BULTIN, DEF_FRONT_END_LIB_BUILTIN,
+	DEF_EXT_FRONT_END_LIB_BUILTIN): Pass implicit as needed.
+	(DEF_C99_C90RES_BULTIN): New.
+	(*f, *l builtins): Update.
+	* c-common.c (DEF_BUILTIN): Initialize implicit array.
+	(c_expand_builtin_printf, c_expand_builtin_fprintf): Update.
+	* convert.c (strip_float_extensions): New global function.
+	* tree.h (DEF_BUILTIN): Accept 10 arguments.
+	(implicit_built_in_decls, mathfn_built_in, strip_float_extension):
+	Declare.
+	* java/builtins.c (define_builtin): Handle implicit.
+	(DEF_BUILTIN): Update.
+	* tm.texi (TARGET_C99_FUNCTIONS): Document.
+	* defaults.h (TARGET_C99_FUNCTIONS): Default to 1.
+
+Tue Jan  7 17:48:24 CET 2003  Jan Hubicka  <jh@suse.cz>
+
+	Merge from mainline:
+	Fri Nov  8 13:01:42 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* builtins.c (expand_builtin_mathfn): Handle floor/ceil/trunc/round/nearbyint
+	(expand_builtin): Likewise.
+	* builtins.def: Add
+	__builtin_floor, __builtin_floorf, __builtin_floorl
+	__builtin_ceil, __builtin_ceilf, __builtin_ceill
+	__builtin_round, __builtin_roundf, __builtin_roundl
+	__builtin_trunc, __builtin_truncf, __builtin_truncl
+	__builtin_nearbyint, __builtin_nearbyintf, __builtin_nearbyintl.
+	* genopinit.c (optabs): Initialize the new optabs.
+	* optab.c (init_optabs): Likewise.
+	* optabs.h (optab_index): Add OTI_floor, OTI_ceil, OTI_trunc,
+	OTI_round, OTI_nearbyint.
+	(floor_optab, ceil_optab, trunc_optab, round_optab, nearbyint_optab): New.
+	* doc/md.texi: Document new named patterns.
+	* doc/extend.texi (builtin functions)  Document
+	floor, floorf, floorl, ceil, ceilf,
+	ceill, round, roundf, roundl, trunc,
+	truncf, truncl, nearbyint, nearbyintf, nearbyintl.
+
+Tue Dec 31 21:03:03 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3 branch (hammer-3_3-merge-20023112)
+
+Tue Dec 31 11:39:47 CET 2002  Josef Zlomek <zlomekj@suse.cz>
+
+	* jump.c (next_nonnote_insn_in_loop): New function.
+	(copy_loop_headers): Use next_nonnote_insn_in_loop instead of
+	next_nonnote_insn.
+	(duplicate_loop_exit_test). Likewise.
+
+Sat Dec 28 17:08:26 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* params.def:  Early merge from mainline.
+	* toplev.c: Enable tracer by default on -O3.
+
+Sun Dec 15 01:46:56 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* loop-unswitch.c (unswitch_loop): Initialize variable.
+
+2002-12-16  Josef Zlomek  <zlomj9am@artax.karlin.mff.cuni.cz>
+
+	* bb-reorder.c (bb_to_key): Fix accessing index -1 of array.
+	(connect_traces): Fix several bugs.
+
+Sun Dec 15 18:22:59 CET 2002  Josef Zlomek <zlomj9am@artax.karlin.mff.cuni.cz>
+
+	* bb-reorder.c: Update comments, fix coding style.
+
+Sat Dec 14 23:00:47 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from 3.3 branch (hammer-3_3-20021412 tag)
+
+Tue Dec 10 20:34:42 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* cfg.c (dump_flow_info): Move regalloc and reg-scan data dump to ...
+	* local-alloc.c (dump_local_alloc): ... here
+
+2002-12-08  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+	* cfgloopanal.c (mark_irreducible_loops): Fix.
+	* unroll.c (unroll_loop_runtime_iterations,
+	unroll_loop_constant_iterations, peel_loop_completely): Removed the
+	call of mark_irreducible_loops.
+
+Sun Dec  8 15:00:03 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* alias.c (memory_modified_1): New static function.
+	(memory_modified): New static varaible.
+	(memory_modified_in_insn_p): New global function.
+	* rtl.h (memory_modified_in_insn_p): Declare.
+	* rtlanal.c (modified_between_p, modified_in_p): Be smart about memory
+	references.
+
+Sat Dec  7 18:14:54 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* i386.c (ix86_expand_int_movcc):  Use force_operand instead of constructing
+	insn directly.
+
+Sat Dec  7 16:37:13 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* Merge from mainline (hammer-3_3-20020712 tag)
+
+Sat Dec  7 15:45:14 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	Hack to compile gcc:
+	* unroll.c (unroll_loop_runtime_iterations,
+	unroll_loop_constant_iterations, peel_loop_completely):Call
+	mark_irreducible_loops.
+
+Sat Dec  7 15:27:30 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* cfglayout.c (cfg_layout_split_block): New function.
+	* cfglayout.h (cfg_layout_split_block): Declare.
+	* cfgloopmanip.c (create_preheader): Use it.
+
+	* calls.c (expand_call): Rename __bb_fork_func to __gcov_flush.
+
+2002-12-03  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+	Merge from rtlopt branch:
+
+	2002-09-11  Nathan Sidwell  <nathan@codesourcery.com>
+
+		Reimplement gcov format.
+		* gcov-io.h: Replace.
+		* gcov.c: Reimplement.
+		* gcov-iov.c: New file.
+		* gcov-dump.c: New file.
+		* libgcc2.c (L_bb): Replace with ...
+		(L_gcov): ... this.
+		(struct bb_function_info, struct bb): Remove.
+		(inhibit_libc): Never inhibit.
+		(gcov_list, gcov_crc): New static variables.
+		(gcov_version_mismatch): New static function.
+		(__bb_exit_func): Renamed to ...
+		(__gcov_exit): ... here. Made static. Reimplement.
+		(__gcov_init_func): Rename to ...
+		(__gcov_init): ... here. Check version, update crc.
+		(__bb_fork_func): Rename to ...
+		(__gcov_flush): ... here.
+		* libgcc2.h (struct bb, __bb_exit_func, __bb_init_func,
+		__bb_fork_func, gcov_type, __bb_find_arc_counters): Remove.
+		* calls.c (expand_call): Call __gcov_flush.
+		* profile.c (bb_file, last_bb_file_name): Remove.
+		(bbg_file_name): New global variable.
+		(output_gcov_string): Remove.
+		(get_exec_counts): Reimplement.
+		(branch_prob): Reimplement gcov file writing.
+		(init_branch_prob): Create bbg_file_name, don't create
+		bb_file_name.
+		(end_branch_prob): Adjust. Don't remove counter file when
+		instrumenting ourselves.
+		(create_profiler): Adjust.
+		* doc/gcov.texi (Gcov Data Files): Remove detailed specification,
+		point to gcov-io.h.
+		* Makefile.in (LANGUAGES): Add gcov-dump.
+		(coverageexts): Remove .bb.
+		(STAGESTUFF): Add gcov-dump.
+		(LIB2FUNCS_ST): Replace _bb with _gcov.
+		(profile.o): Depend on gcov-iov.h.
+		(final.o): Don't depend on profile.h, gcov.h.
+		(gcov.o): Depend on gcov-iov.h.
+		(gcov-iov.o): New target.
+		(gcov-iov): New target.
+		(gcov-iov.h): New target.
+		(gcov-dump.o): New target.
+		(GCOV_DUMP_OBJS): New variable.
+		(gcov-dump): New target.
+		(distclean): Remove coverageexts.
+		(stage1): Remove coverageexts.
+
+	2002-11-21  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* Makefile.in (vpt.o): New file.
+		(toplev.o, profile.o): Add vpt.h dependecy.
+		* vpt.h: New.
+		* vpt.c: New.
+		* combine.c (distribute_notes): Handle REG_VALUE_HISTOGRAM note.
+		* flags.h (flag_value_histograms): Declare.
+		* gcov-io.h (GCOV_TAG_VALUE_HISTOGRAMS): New.
+		(struct function_info): Add n_value_histogram_counters field.
+		(struct gcov_info): Add value_counts and n_value_counts fields.
+		* libgcc2.c (gcov_exit, __gcov_flush): Write out value histograms.
+		* profile.c: Include vpt.h.
+		(struct function_list): Add value_counters field.
+		(value_histograms_label): New.
+		(gen_loop_profiler): Deleted.
+		(gen_interval_profiler, gen_range_profiler, gen_pow2_profiler,
+		gen_one_value_profiler): New.
+		(get_histogram_counts, instrument_loops, compute_loop_histograms,
+		init_branch_prob): Modified.
+		(index_counts_file): Fix mistakes.
+		(instrument_values, compute_value_histograms): New.
+		(branch_prob): Call them.
+		(create_profiler): Emit requiered structures.
+		* profile.h (struct profile_info): Add count_value_counters,
+		count_value_counters_now and have_value_histograms fields.
+		* rtl.c (reg_note_name): Add REG_VALUE_HISTOGRAM.
+		* rtl.h (enum reg_note): Ditto.
+		* toplev.c: Include vpt.h.
+		(enum dump_file_index, dump_file): Add vpt dump file.
+		(flag_value_histograms, flag_value_profile_transformations): New.
+		(lang_independent_options): Add them.
+		(rest_of_compilation): Add vpt pass.
+
+	2002-11-23  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* vpt.c (find_values_to_profile): Move liveness analysis ...
+		* toplev.c (rest_of_compilation): ... here.  Also do cleanup
+		afterwards.
+
+	2002-11-27  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* Makefile.in (loop-init.o): Add gcov-io.h dependency.
+		* gcov-io.h (struct counter_section, struct counter_section_data): New.
+		(struct function_info): n_arc_counts, n_loop_histogram_counters and
+		n_value_histogram_counters fields merged into new n_counter_sections and
+		counter_sections fields.
+		(struct gcov_info): arc_counts, n_arc_counts, histogram_counts,
+		n_histogram_counts, value_counts and n_value_counts merged into
+		n_counter_sections and counter_sections fields.
+		* libgcc2.c (gcov_exit, __gcov_flush): Rewritten to enable adding
+		new section types.
+		* loop-init.c: Include gcov-io.h.
+		(loop_optimizer_init): Modified.
+		* profile.c (struct function_list): count_edges, histogram_counters
+		and value_counters merged to n_counter_sections and counter_sections
+		fields.
+		(set_purpose, label_for_tag, build_counter_section_fields,
+		build_counter_section_value, build_counter_section_data_fields,
+		build_counter_section_data_value, build_function_info_fields,
+		build_function_info_value, build_gcov_info_fields,
+		build_gcov_info_value, find_counters_section): New.
+		(instrument_edges, instrument_loops, instrument_values,
+		get_histogram_counts, compute_loop_histograms, compute_value_histograms,
+		compute_branch_probabilities, branch_prob, create_profiler): Modified
+		to enable adding new section types.
+		* profile.h (MAX_COUNTER_SECTIONS): New.
+		(struct section_info): New.
+		(struct profile_info): count_instrumented_edges, count_histogram_counters,
+		count_value_counters, count_edges_instrumented_now,
+		count_histogram_counters_now, count_value_counters_now,
+		have_loop_histograms, have_value_histograms fields merged into
+		new n_sections and section_info fields.
+		(find_counters_section): Declare.
+		* vpt.c (insn_values_to_profile, gen_mod_subtract): Fix.
+
+Wed Dec  4 23:42:16 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* i386.c: Fix merge problems.
+	* i386.md: Likewise.
+
+Wed Dec  4 11:47:43 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* i386.c (ix86_expand_int_movcc): Force operand into register for QImode
+	condtiional moves.
+
+2002-12-03  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+	Merge from rtlopt branch:
+	2002-11-29  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* cfgloopanal.c (simple_loop_p): Count desc->n_branches.
+		* cfgloop.h (struct loop_desc): Add n_branches field.
+		* loop-unroll.c (decide_peel_simple, decide_unroll_stupid): Do not
+		unroll/peel loops containing branches.
+
+	2002-11-10  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* loop-unroll.c (decide_unrolling_and_peeling): Fix.
+
+	2002-11-10  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* loop-unroll.c (decide_unroll_constant_iterations,
+		decide_unroll_runtime_iterations, decide_peel_simple,
+		decide_unroll_stupid): Fix.
+
+	2002-11-09  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* cfgloop.c (flow_loop_dump, flow_loops_free, flow_loop_exit_edges_find,
+		get_loop_body, cancel_loop, verify_loop_structure):
+		Use unsigned variables where appropriate.
+		* cfgloopanal.c (simple_loop_p, mark_irreducible_loops,
+		num_loop_insns, expected_loop_iterations): Ditto.
+		* cfgloopmanip.c (record_exit_edges, duplicate_loop_to_header_edge,
+		loopify, fix_loop_placement, can_duplicate_loop_p, create_preheaders,
+		force_single_succ_latches): Ditto.
+		* loop-unswitch.c (may_unswitch_on_p, unswitch_single_loop): Ditto.
+		* predict.c (estimate_probability, estimate_loops_at_level): Ditto.
+		* loop-unroll.c (unroll_or_peel_loop): Removed.
+		(peel_loop_simple, peel_loop_completely, unroll_loop_stupid,
+		unroll_loop_constant_iterations, unroll_loop_runtime_iterations,
+		unroll_and_peel_loops): Modified.
+		(peel_loops_completely, decide_unrolling_and_peeling,
+		decide_peel_simple, decide_peel_once_rolling, decide_peel_completely,
+		decide_unroll_stupid, decide_unroll_constant_iterations,
+		decide_unroll_runtime_iterations): New.
+		* cfgloop.h (enum lpt_dec, struct lpt_decision): New.
+		(struct loop): Add lpt_decision, simple, desc, has_desc and ninsns
+		fields.
+		(expected_loop_iterations, duplicate_loop_to_header_edge): Declaration
+		changed.
+		(UAP_PEEL_COMPLETELY, UAP_PEEL_ONCE_ROLLING): Removed.
+
+	2002-11-01  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* cfgloopmanip.c (duplicate_loop_to_header_edge): Fix count &
+		frequencies updating.
+		* loop-unroll.c (unroll_loop_runtime_iterations): Fix edge
+		probabilities.
+
+	2002-11-01  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* loop-unroll.c (unroll_loop_runtime_iterations): New loop
+		preconditioning.
+		* params.def (PARAM_MAX_PEELED_INSNS,
+		PARAM_MAX_COMPLETELY_PEELED_INSNS): Increase default parameter values.
+		* toplev.c (dump_file): Sync with dump_file_index.
+
+	2002-10-27  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* cfgloopanal.c (count_loop_iterations): Undo my previous fix.
+		* loop-unroll.c (unroll_loop_runtime_iterations): Better
+		preconditioning for preincrement loops.
+
+	2002-10-27  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* cfgloopanal.c (count_loop_iterations): Use stride instead of
+		const1_rtx.
+		* loop-unroll.c (unroll_loop_runtime_iterations): Revert the wrong
+		fix, fix misleading comment.
+		(unroll_or_peel_loop): Add debugging info.
+
+	Sat Oct 26 00:04:54 CEST 2002  Jan Hubicka  <jh@suse.cz>
+
+		* loop-unroll.c (unroll_loop_runtime_iterations):  Compute edge probabilities properly.
+
+	Fri Oct 25 23:31:53 CEST 2002  Jan Hubicka  <jh@suse.cz>
+
+		* loop-unroll.c (unroll_loop_runtime_iterations): Fix.
+
+	Wed Oct 23 16:46:38 CEST 2002  Jan Hubicka  <jh@suse.cz>
+
+		* loop-unroll.c (unroll_or_peel_loop): Do complete unrolling
+		when -funroll-loop is specified.
+
+	2002-10-20  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* cfgloopmanip.c (fix_bb_placement, fix_bb_placements): New.
+		(remove_path): Use them.
+		(record_exit_edges): Prototype changed.
+
+	2002-10-20  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* params.def (PARAM_MAX_COMPLETELY_PEEL_TIMES): Fix spelling typo
+		in comment.
+
+	2002-10-19  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* loop-unroll.c (unroll_or_peel_loop): Modify loop peeling decision.
+		* params.def (PARAM_MAX_COMPLETELY_PEELED_INSNS,
+		PARAM_MAX_COMPLETELY_PEEL_TIMES, PARAM_MAX_ONCE_PEELED_INSNS): New.
+		* cfgloop.h (UAP_UNROLL_ALL, UAP_PEEL_COMPLETELY,
+		UAP_PEEL_ONCE_ROLLING): New.
+
+	2002-10-19  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>
+
+		* Makefile.in (cfgloopanal.o): New.
+		(doloop.o, predict.o, toplev.o, loop.o, unroll.o, cfgloop.o): Include
+		cfgloop.h
+		* basic-block.h (loop, loops): Forward declare.
+		(flow_loops_find, flow_loops_update, flow_loops_free, flow_loops_dump,
+		flow_loop_dump, flow_loop_scan, flow_loop_tree_node_add,
+		flow_loop_tree_node_remove, LOOP_*, flow_loop_outside_edge_p,
+		flow_loop_nested_p, flow_bb_inside_loop_p, get_loop_body,
+		dfs_enumerate_from, loop_preheader_edge, loop_latch_edge,
+		add_bb_to_loop, remove_bb_from_loops, find_common_loop,
+		verify_loop_structure, VLS_*): Move to ...
+		(BB_IRREDUCIBLE): New flag.
+		* cfgloop.h: ... here; new file;
+		(struct loop_desc, simple_loop_p, count_iterations): Declare.
+		* cfgloop.c: Include cfgloop.h
+		* doloop.c: Likewise.
+		* predict.c: Likewise.
+		(estimate_probability): Predict number of iterations.
+		* loop.c: Likewise.
+		(strength_reduce): kill PRED_LOOP_ITERATIONS prediction
+		* toplev.c: Include cfgloop.h
+		* unroll.c: Likewise.
+		* cfgloopanal.c: New file.
+
+		* Makefile.in (cfgloopmanip.o): New.
+		* cfglayout.c (cfg_layout_redirect_edge): Return exit status.
+		(cfg_layout_duplicate_bb): Set copy field.
+		* cfglayout.h (cfg_layout_redirect_edge): Declaration changed.
+		(struct reorder_block_def): Added new fields.
+		* cfgloop.c (cancel_loop, cancel_loop_tree): New functions.
+		* cfgloop.h: Reformated.
+		(struct loop): Added new field.
+		(cancel_loop, cancel_loop_tree, can_duplicate_loop_p,
+		duplicate_loop_to_header_edge, loopify, remove_path, split_loop_bb):
+		Declare.
+		* cfgloopmanip.c: New.
+
+		* Makefile.in (loop-unswitch.o, loop-init.o): New.
+		(cfglayout.o): Add dependency on cfgloop.h.
+		(cfgloopmanip.o): Add dependency on output.h.
+		* basic-block.h (BB_SUPERBLOCK): New flag.
+		* cfglayout.h (cfg_layout_initialize): Declaration changed.
+		* cfglayout.c (cfgloop.h): Include.
+		(cleanup_unconditional_jumps, cfg_layout_initialize): Update loop data.
+		(break_superblocks): New.
+		(cfg_layout_finalize): Call it.
+		* bb_reorder.c (reorder_basic_blocks): Add parameter to
+		cfg_layout_initialize call.
+		* cfgloop.c (verify_loop_structure): Add irreducible loop info
+		verification.
+		* cfgloop.h (VLS_EXPECR_PREHEADERS, VLS_EXPECT_SIMPLE_LATCHES):
+		Merged to enum together with...
+		(VLS_EXPECT_MARKED_IRREDUCIBLE_LOOPS, VLS_FOR_LOOP): New flags.
+		(CP_SIMPLE_PREHEADERS, CP_INSIDE_CFGLAYOUT): New flags.
+		(num_loop_insns, loop_split_edge_with, fix_loop_placement,
+		create_preheaders, force_single_succ_latches, expected_loop_iterations,
+		loop_optimizer_init, loop_optimizer_finalize, unswitch_loops): Declare.
+		* cfgloopanal.c (num_loop_insns, expected_loop_iterations):
+		New functions.
+		* cfgloopmanip.c (output.h): Include.
+		(fix_loop_placement): Declaration moved to cfgloop.h.
+		(create_preheader, create_preheaders, force_sinle_succ_latches,
+		loop_split_edge_with): New functions.
+		* loop-init.c: New.
+		* loop-unswitch: New.
+		* params.def (PARAM_MAX_UNSWITCH_INSNS, PARAM_MAX_UNSWITCH_LEVEL): New.
+		* toplev.c (DFI_loop2): New.
+		(flag_unswitch_loops): New.
+		(rest_of_compilation): Add loop unswitching pass.
+		(parse_options_and_default_flags): Enable it at -O3.
+
+		* Makefile.in (loop-unroll.o): New.
+		* cfgloop.h (UAP_PEEL, UAP_UNROLL, UAP_UNROLL_ALL): New flags.
+		(unroll_and_peel_loops): Declare.
+		* loop-unroll.c: New.
+		* params.def (PARAM_MAX_UNROLL_TIMES, PARAM_MAX_PEELED_INSNS,
+		PARAM_MAX_PEEL_TIMES): New.
+		* toplev.c (flag_old_unroll_loops, flag_old_unroll_all_loops): Used
+		instead of ...
+		(flag_unroll_loops, flag_unroll_all_loops): Used for new loop optimizer.
+		(flag_peel_loops): New.
+		(parse_options_and_default_flags): Turn them on with -O3.
+		(process_options): Modified.
+		(rest_of_compilation): Add unrolling/peeling pass.
+
+Mon Dec  2 22:24:16 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* cselib.c (cselib_current_insn_in_libcall): New static variable.
+	(new_elt_loc_list, cselib_process_insn, cselib_init): Keep track on whether
+	we are inside libcall.
+	* cselib.h (elt_loc_list): Add in_libcall.
+	* gcse.c (do_local_cprop): Do not copy propagate using insns
+	in libcalls.
+
+Mon Dec  2 20:26:21 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* unroll.c (copy_loop_body): Copy CONST_OR_PURE_CALL_P.
+
+Mon Dec  2 19:21:37 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* i386.c (ix86_expand_int_movcc):  Avoid overflow.
+
+Mon Dec  2 18:06:41 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* cfgrtl.c (force_nonfallthru_and_redirect):  Allow abnormal edge
+	to be forced into nonfallthru.
+
+Fri Nov 29 00:06:17 CET 2002  JAn Hubicka  <jh@suse.cz>
+
+	Merge from BIB branch:
+	Thu Nov 28 23:56:24 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (ix86_expand_int_movcc): Add copy_rtx to avoid invalid RTX
+		sharing when operand is SUBREG.
+
+	Thu Nov 28 08:57:26 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* athlon.md (athlon-decodev): New reservation unit.
+		(athlon-direct0): New reservation.
+		(athlon-vector): New use athlon-decodev.
+		(athlon-double, athlon-direct): Better model.
+		(athlon_imul_k8): Use athlon-direct0.
+		(athlon_movlpd_load): New insn reservation.
+
+	Wed Nov 27 20:34:13 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (x86_sse_partial_regs_for_cvtsd2ss): New.
+		* i386.h (x86_sse_partial_regs_for_cvtsd2ss): Declare.
+		(TARGET_SSE_PARTIAL_REGS_FOR_CVTSD2SS): New macro.
+		* i386.md (truncdfsf patterns and splitters): Use
+		TARGET_SSE_PARTIAL_REGS_FOR_CVTSD2SS
+
+	Tue Nov 26 22:43:50 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (ix86_expand_int_movcc):  Do not emit lea for short mode on
+		partial_reg_stall target.
+
+	Tue Nov 26 22:27:47 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (movhicc): Allow general operand.
+		(movqicc): New expander.
+		(movqicc_noc): New pattern.
+		* i386.c (ix86_expand_carry_flag_compare): New function.
+		(ix86_expand_int_movcc): Optimize harder using sbb; support more
+		HImode conversion; support QImode conditional moves
+
+	Tue Nov 26 16:30:59 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (FAST_PROLOGUE_INSN_COUNT): Set to 20.
+		(ix86_expand_prologue): Multiply the count by amount of registers to be
+		pushed.
+
+	Tue Nov 26 15:55:27 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (override_options): Error about wrong -mcpu on x86-64
+		compilation.
+
+	Tue Nov 26 00:14:20 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386-protos.h (x86_extended_QIreg_mentioned_p,
+		x86_extended_reg_mentioned_p): Declare.
+		* i386.c (extended_reg_mentioned_1): New static function.
+		(x86_extended_QIreg_mentioned_p,
+		x86_extended_reg_mentioned_p): New global functions.
+		* i386.h (REX_SSE_REGNO_P): New macro.
+		* i386.md (prefix_rex): New attribute.
+		(length attribute): Add rex.
+
+Thu Nov 28 23:50:23 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	bb-reorder.c:  Merge from rtlopt-branch.
+
+2002-11-27  Andreas Jaeger  <aj@suse.de>
+
+	* version.c: Add -hammer to version_string.
+
+Tue Nov 26 15:42:46 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	Merge from 3.4-BIB:
+	* config.guess:  Default to k8 for k8.
+
+Tue Nov 26 00:24:17 CET 2002  Jan Hubicka  <jh@suse.cz>
+	Merge from 3.4-BIB:
+
+	Mon Nov 25 18:32:37 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (pushsf_rex64): Fix typo.
+
+	Sun Nov 24 10:38:04 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (x86_use_ffreep): New global variable.
+		* i386.h (x86_use_frfeep): Declare
+		(TARGET_USE_FFREEP): New macro
+		* i386.md  (movs?f*): Use freep when asked for.
+		(push?f): Remove dead code.
+
+	Wed Nov 20 19:07:17 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* config.gcc: Add k8 target alias support
+		* i386.c (_cost): Declare costs for various variants of divides and
+		multiplies.
+		(k8_cost): New.
+		(m_K8, m_ATHLON_K8): New macros.
+		(x86_use_leave, x86_push_memory, x86_movx, x86_unroll_strlen,
+		x86_cmove, x86_3dnow_a, x86_deep_branch, x86_use_fiop,
+		x86_promote_QImode, x86_sub_esp_?, x86_add_esp_?,
+		x86_integer_DFmode_moves, x86_partial_reg_dependency,
+		x86_memory_mismatch_stall, x86_accumulate_outgoing_args,
+		x86_prologue_using_move, x86_epilogue_using_move,
+		x86_arch_always_fancy_math_387, x86_sse_partial_regs,
+		x86_sse_typeless_stores): Set for K8
+		(override_options): Add k8 support; fix athlon alignment;
+		complain about non-x86-64 capable CPU being used in x86-64 compilation.
+		(ix86_issue_rate): Set for K8.
+		(ix86_adjust_cost, ia32_use_dfa_pipeline_interface,
+		x86_machine_dependent_reorg): Handle K8 like
+		* i386.h
+		(x86_costs):  Change mult_init and divide into array.
+		(TARGET_K8, TARGET_ATHLON): New macros.
+		(MODE_INDEX): New macro.
+		(RTX_COST): Use new costs.
+		(TARGET_CPU_CPP_BUILTINS):  Define __k8__ and __tune_k8__.
+		(TARGET_CPU_DEFAULT_NAMES): Add k8
+		(TARGET_CPU_DEFAULT_k8): New constant
+		(enum processor_type): Add PROCESSOR_K8.
+		* i386.md (cpu attribute): Add k8.
+
+		* invoke.texi: Document -march=k8.
+
+		* i386.md (type attribute): Add leave
+		(mode attribute): Remove unknownfp.
+		(length_immediate, modrm, memory attributes): Handle leave correctly.
+		(fp comparison patterns): Determine FP mode.
+		(leave, leave_rex64): Remove special cases.
+		* ppro.md (ppro_uops, ppro_p2): Add leave
+		* pentiun.md (pent_pop): Handle leave too.
+		* k6.md (k6_load): Handle leave.
+		* athlon.md (athlon_leave, athlon_pop): Fix.
+		(athlon_decode): Handle leave.
+
+Mon Nov 25 18:00:50 CET 2002  Jan Hubicka  <jh@suse.cz>
+	Merge from mainline (hammer-3_3-merge-20022511 branch)
+
+Mon Nov 18 23:42:28 CET 2002  Jan Hubicka  <jh@suse.cz>
+	Merge from mainline (hammer-3_3-merge-20021811 branch)
+
+	Merge from 3.4-BIB:
+
+	Sun Nov 17 14:01:09 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (negsf2_ifs, negdf2_ifs, negdf2_ifs_rex64, abssf2_ifs,
+		absdf2_ifs, absdf2_ifs_rex64): Fix constraints.
+		(neg?f2_ifs, abs?f2_ifs splitters): Refuse memory operand; do not
+		generate unnecesary subregs.
+
+	Sat Nov 16 16:49:58 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (truncdfsf2_1_sse, truncdfsf2_1_sse_nooverlap, truncdfsf2_2,
+		floats?dff2_i387):
+		Work around regclass stupidity.
+		(truncdfsf_2_1_sse splitter):  Accept !TARGET_PARTIAL_SSE_REGS
+
+	Sat Nov 16 02:17:48 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (fop_df_6): New pattern.
+		(fop_xf_4, fop_xf_5): Handle both SF and DFmode extensions.
+		(fop_xf_6): Rewrite
+		(fop_xf_7): Delete.
+		(fop_tf_4, fop_tf_5): Handle both SF and DFmode extensions.
+		(fop_tf_6): Rewrite
+		(fop_tf_7): Delete.
+
+
+Wed Nov 13 12:08:08 CET 2002  Jan Hubicka  <jh@suse.cz>
+	Merge i386 specific optimizations for 3.4-BIB branch.
+
+	Sat Nov  9 00:10:54 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (x86_machine_dependent_reorg): Fix even more side cases.
+
+	Fri Nov  8 13:33:58 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (sse_loadss, sse2_loadsd): Fix expander.
+
+	Fri Nov  8 13:25:41 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (x86_machine_dependent_reorg): Fix handling of empty functions.
+
+	Fri Nov  8 11:36:11 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (sse_movdfcc, sse_movsfcc): Fix typo in previous patch.
+
+	Thu Nov  7 21:54:22 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (sse_movdfcc, sse_movsfcc): Avoid overactive matching.
+		* i386.c (ix86_expand_fp_movcc): Match the reversed cases.
+
+	Tue Nov  5 14:34:36 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (float_truncate SSE splitter): Ensure that operand is not
+		stack register.
+		(float SSE splitters): Reorder conditional.
+
+	Thu Oct 31 18:20:50 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (sse_loadss, sse_loadsd):  Canonicalize; add expander
+		(movps, movpd splitters): Use canonical form.
+		(movv2di): Fix merge problem.
+
+	Wed Nov  6 17:16:48 CET 2002  Jan Hubicka  <jh@.suse.cz>
+
+		* i386.md (negsf splitter): Accept memory operand in second register.
+		(abssf/absdf splitters): Simplify
+		(sse_loadss, sse_loadsd): Turn into expander.
+
+	Thu Oct 31 16:09:44 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (negdf2_ifs_rex64): Don't allow GPR operand.
+
+	Tue Oct 29 23:28:10 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (negdf splitter): Fix construction of the constant.
+
+	Tue Oct 29 20:47:06 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (negsf, negdf): Reorganize to use vector modes
+		for SSE variants.
+		(abssf, absdf): Use force_reg.
+		(movv4sf, movv2df): New splitters.
+		* i386.h (PREDICATE_CODES): add zero_extended_scalar_load_operand
+		* i386.c (zero_extended_scalar_load_operand
+
+	Wed Oct 23 22:48:44 CEST 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (abs splitters): Do not produce nested subregs.
+
+	Wed Oct 23 12:42:32 CEST 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (movti_rex64): Fix constraints.
+
+	Wed Oct 23 12:01:21 CEST 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.md (abssf,absdf): Use vector operands for SSE
+		(abssf2_ifs, absdf2_ifs, absdf2_ifs_rex64 and splitters): Update for
+		vector operand.
+
+	Wed Oct  9 21:18:43 CEST 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (*_cost): Add branch costs.
+		(override_options): set ix86_branch_cost.
+		(ix86_expand_int_movcc): Use BRANCH_COST.
+		* i386.h (costs): Add branch_cost.
+
+	Tue Oct  8 01:24:19 CEST 2002  Jan Hubicka  <jh@suse.cz>
+
+		* i386.c (x86_sse_partial_reg_dependency, x86_sse_partial_regs,
+		x86_sse_typeless_stores, x86_sse_load0_by_pxor): New global
+		variables.
+		(safe_vector_operand): Update sse_clrv4sf call.
+		(ix86_expand_buildin): Likewise
+		* i386.h (x86_sse_partial_reg_dependency, x86_sse_partial_regs,
+		x86_sse_typeless_stores, x86_sse_load0_by_pxor): Declare.
+		(TARGET_SSE_PARTIAL_REG_DEPENDENCY, TARGET_SSE_PARTIAL_REGS,
+		TARGET_SSE_TYPELESS_STORES, TARGET_SSE_TYPELESS_LOAD0): New
+		macros.
+		* i386.md (movsf*, movdf*, movti, movv4sf, movv2df, movv16qi, movv8hi,
+		movv4si):  Obey the new flags.
+		(floatsi2sf, floatdi2sf, truncatedf2sf): Emit extra load of 0 to avoid
+		reformating penalty.
+		(anddf, cmov patterns): Avoid reformating by first converting.
+		(sse_cvtsd2ss): Fix predicate.
+		(sse2_clrti): Fix mode,
+		(sse_clrv4sf): Avoid unspec.
+
+	Sat Oct  5 22:48:06 CEST 2002  Jan Hubicka  <jh@suse.cz>
+
+		* athlon.md: rewrite to DFA.
+		* i386 (ix86_adjust_cost): Drop memory latency code.
+		(ia32_use_dfa_pipeline_interface): Return true for Athlon.
+Thu Nov  7 11:18:01 CET 2002  Jan Hubicka  <jh@suse.cz>
+
+	* reg-stack.c (compensate_edge): Fix sanity check.
--- gcc-3.3.1/gcc/Makefile.in.hammer-branch	2003-08-05 17:51:54.000000000 +0200
+++ gcc-3.3.1/gcc/Makefile.in	2003-08-05 18:24:01.000000000 +0200
@@ -65,7 +65,7 @@ BOOT_LANGUAGES = c @all_boot_languages@
 # TCFLAGS is used for compilations with the GCC just built.
 XCFLAGS =
 TCFLAGS =
-CFLAGS = -g
+CFLAGS = -g -O2
 STAGE1_CFLAGS = -g @stage1_cflags@
 BOOT_CFLAGS = -g -O2
 
@@ -399,7 +399,7 @@ TARGET_LIBGCC2_CFLAGS =
 # Options to use when compiling crtbegin/end.
 CRTSTUFF_CFLAGS = -O2 $(GCC_CFLAGS) $(INCLUDES) $(MULTILIB_CFLAGS) -g0 \
   -finhibit-size-directive -fno-inline-functions -fno-exceptions \
-  -fno-zero-initialized-in-bss
+  -fno-zero-initialized-in-bss -fno-unit-at-a-time
 
 # Additional sources to handle exceptions; overridden on ia64.
 LIB2ADDEH = $(srcdir)/unwind-dw2.c $(srcdir)/unwind-dw2-fde.c \
@@ -749,6 +749,7 @@ C_OBJS = c-parse.o c-lang.o c-pretty-pri
 OBJS = alias.o bb-reorder.o bitmap.o builtins.o caller-save.o calls.o	   \
  cfg.o cfganal.o cfgbuild.o cfgcleanup.o cfglayout.o cfgloop.o		   \
  cfgrtl.o combine.o conflict.o convert.o cse.o cselib.o dbxout.o	   \
+ cfgloopanal.o cfgloopmanip.o loop-init.o loop-unswitch.o loop-unroll.o	   \
  debug.o df.o diagnostic.o doloop.o dominance.o		                   \
  dwarf2asm.o dwarf2out.o dwarfout.o emit-rtl.o except.o explow.o	   \
  expmed.o expr.o final.o flow.o fold-const.o function.o gcse.o		   \
@@ -763,8 +764,8 @@ OBJS = alias.o bb-reorder.o bitmap.o bui
  sbitmap.o sched-deps.o sched-ebb.o sched-rgn.o sched-vis.o sdbout.o	   \
  sibcall.o simplify-rtx.o ssa.o ssa-ccp.o ssa-dce.o stmt.o		   \
  stor-layout.o stringpool.o timevar.o toplev.o tracer.o tree.o tree-dump.o \
- tree-inline.o unroll.o varasm.o varray.o version.o vmsdbgout.o xcoffout.o \
- et-forest.o $(GGC) $(out_object_file) $(EXTRA_OBJS)
+ tree-inline.o unroll.o varasm.o varray.o version.o vmsdbgout.o vpt.o	   \
+ xcoffout.o et-forest.o callgraph.o web.o $(GGC) $(out_object_file) $(EXTRA_OBJS)
 
 BACKEND = main.o libbackend.a
 
@@ -787,7 +788,7 @@ STAGESTUFF = *$(objext) insn-flags.h ins
  $(EXTRA_PARTS) $(EXTRA_PROGRAMS) gcc-cross$(exeext) cc1obj$(exeext) \
  protoize$(exeext) unprotoize$(exeext) \
  specs collect2$(exeext) $(USE_COLLECT2) \
- gcov$(exeext) *.[0-9][0-9].* *.[si] libcpp.a libbackend.a libgcc.mk \
+ gcov-iov$(exeext) gcov$(exeext) *.[0-9][0-9].* *.[si] libcpp.a libbackend.a libgcc.mk \
  $(LANG_STAGESTUFF)
 
 # Library members defined in libgcc2.c.
@@ -801,7 +802,7 @@ LIB2FUNCS_2 = _floatdixf _fixunsxfsi _fi
     _addvdi3 _subvsi3 _subvdi3 _mulvsi3 _mulvdi3 _negvsi2 _negvdi2 _ctors
 
 # Defined in libgcc2.c, included only in the static library.
-LIB2FUNCS_ST = _eprintf _bb __gcc_bcmp
+LIB2FUNCS_ST = _eprintf _bb __gcc_bcmp _gcov
 
 FPBIT_FUNCS = _pack_sf _unpack_sf _addsub_sf _mul_sf _div_sf \
     _fpcmp_parts_sf _compare_sf _eq_sf _ne_sf _gt_sf _ge_sf \
@@ -1407,8 +1408,8 @@ toplev.o : toplev.c $(CONFIG_H) $(SYSTEM
    debug.h insn-config.h intl.h $(RECOG_H) Makefile toplev.h \
    dwarf2out.h sdbout.h dbxout.h $(EXPR_H) hard-reg-set.h $(BASIC_BLOCK_H) \
    graph.h $(LOOP_H) except.h $(REGS_H) $(TIMEVAR_H) $(lang_options_files) \
-   ssa.h $(PARAMS_H) $(TM_P_H) reload.h dwarf2asm.h $(TARGET_H) \
-   langhooks.h insn-flags.h options.h cfglayout.h real.h
+   ssa.h $(PARAMS_H) $(TM_P_H) reload.h dwarf2asm.h $(TARGET_H) vpt.h \
+   langhooks.h insn-flags.h options.h cfglayout.h cfgloop.h real.h
 	$(CC) $(ALL_CFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \
 	  -DTARGET_NAME=\"$(target_alias)\" \
 	  -c $(srcdir)/toplev.c $(OUTPUT_OPTION)
@@ -1503,6 +1504,8 @@ jump.o : jump.c $(CONFIG_H) $(SYSTEM_H) 
 simplify-rtx.o : simplify-rtx.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(REGS_H) \
    hard-reg-set.h flags.h real.h insn-config.h $(RECOG_H) $(EXPR_H) toplev.h \
    output.h function.h $(GGC_H) $(OBSTACK_H) $(TM_P_H) $(TREE_H)
+callgraph.o : callgraph.c $(CONFIG_H) $(SYSTEM_H) $(TREE_H) \
+    langhooks.h tree-inline.h toplev.h flags.h ggc.h  $(TARGET_H)
 cselib.o : cselib.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(REGS_H) \
    hard-reg-set.h flags.h real.h insn-config.h $(RECOG_H) $(EXPR_H) toplev.h \
    output.h function.h cselib.h $(GGC_H) $(TM_P_H) gt-cselib.h
@@ -1512,6 +1515,9 @@ cse.o : cse.c $(CONFIG_H) $(SYSTEM_H) $(
 gcse.o : gcse.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(REGS_H) hard-reg-set.h \
    flags.h real.h insn-config.h ggc.h $(RECOG_H) $(EXPR_H) $(BASIC_BLOCK_H) \
    function.h output.h toplev.h $(TM_P_H) $(PARAMS_H) except.h gt-gcse.h
+web.o : web.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(REGS_H) hard-reg-set.h \
+   flags.h real.h insn-config.h ggc.h $(RECOG_H) $(EXPR_H) $(BASIC_BLOCK_H) \
+   function.h output.h toplev.h $(TM_P_H) $(PARAMS_H)
 sibcall.o : sibcall.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(REGS_H) function.h \
    hard-reg-set.h flags.h insn-config.h $(RECOG_H) $(BASIC_BLOCK_H)
 resource.o : resource.c $(CONFIG_H) $(RTL_H) hard-reg-set.h $(SYSTEM_H) \
@@ -1535,14 +1541,17 @@ conflict.o : conflict.c $(CONFIG_H) $(SY
    $(RTL_H) hard-reg-set.h $(BASIC_BLOCK_H)
 profile.o : profile.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(TREE_H) flags.h \
    insn-config.h output.h $(REGS_H) $(EXPR_H) function.h \
-   gcov-io.h toplev.h $(GGC_H) hard-reg-set.h $(BASIC_BLOCK_H) $(TARGET_H) \
-   langhooks.h profile.h libfuncs.h gt-profile.h
+   gcov-io.h gcov-iov.h toplev.h $(GGC_H) hard-reg-set.h $(BASIC_BLOCK_H) \
+   $(TARGET_H) langhooks.h profile.h libfuncs.h gt-profile.h cfgloop.h \
+   params.h $(HASHTAB_H) vpt.h
+vpt.o : vpt.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(BASIC_BLOCK_H) profile.h \
+   hard-reg-set.h vpt.h $(EXPR_H) output.h
 loop.o : loop.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) flags.h $(LOOP_H) \
    insn-config.h $(REGS_H) hard-reg-set.h $(RECOG_H) $(EXPR_H) \
    real.h $(PREDICT_H) $(BASIC_BLOCK_H) function.h \
    toplev.h varray.h except.h cselib.h $(OPTABS_H) $(TM_P_H)
 doloop.o : doloop.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) flags.h $(LOOP_H) \
-   $(EXPR_H) hard-reg-set.h $(BASIC_BLOCK_H) $(TM_P_H) toplev.h
+   $(EXPR_H) hard-reg-set.h $(BASIC_BLOCK_H) $(TM_P_H) toplev.h cfgloop.h
 unroll.o : unroll.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) insn-config.h function.h \
    $(INTEGRATE_H) $(REGS_H) $(RECOG_H) flags.h $(EXPR_H) $(LOOP_H) toplev.h \
    hard-reg-set.h varray.h $(BASIC_BLOCK_H) $(TM_P_H) $(PREDICT_H) $(PARAMS_H)
@@ -1564,7 +1573,19 @@ cfgcleanup.o : cfgcleanup.c $(CONFIG_H) 
    $(BASIC_BLOCK_H) hard-reg-set.h output.h flags.h $(RECOG_H) toplev.h \
    $(GGC_H) insn-config.h cselib.h $(TARGET_H) $(TM_P_H) $(PARAMS_H)
 cfgloop.o : cfgloop.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) \
-   $(BASIC_BLOCK_H) hard-reg-set.h
+   $(BASIC_BLOCK_H) hard-reg-set.h cfgloop.h flags.h
+cfgloopanal.o : cfgloopanal.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) \
+   $(BASIC_BLOCK_H) hard-reg-set.h cfgloop.h $(EXPR_H)
+cfgloopmanip.o : cfgloopmanip.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) \
+   $(BASIC_BLOCK_H) hard-reg-set.h cfgloop.h cfglayout.h output.h
+loop-init.o : loop-init.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) gcov-io.h \
+   gcov-iov.h $(BASIC_BLOCK_H) hard-reg-set.h cfgloop.h cfglayout.h profile.h
+loop-unswitch.o : loop-unswitch.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) \
+   $(BASIC_BLOCK_H) hard-reg-set.h cfgloop.h cfglayout.h params.h \
+   output.h $(expr.h)
+loop-unroll.o : loop-unroll.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) \
+   $(BASIC_BLOCK_H) hard-reg-set.h cfgloop.h cfglayout.h params.h \
+   output.h $(expr.h)
 dominance.o : dominance.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) hard-reg-set.h \
    $(BASIC_BLOCK_H) et-forest.h
 et-forest.o : et-forest.c $(CONFIG_H) $(SYSTEM_H) et-forest.h
@@ -1646,16 +1667,17 @@ reg-stack.o : reg-stack.c $(CONFIG_H) $(
 predict.o: predict.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(TREE_H) flags.h \
    insn-config.h $(BASIC_BLOCK_H) $(REGS_H) hard-reg-set.h output.h toplev.h \
    $(RECOG_H) function.h except.h $(EXPR_H) $(TM_P_H) $(PREDICT_H) real.h \
-   $(PARAMS_H) $(TARGET_H)
+   $(PARAMS_H) $(TARGET_H) cfgloop.h
 lists.o: lists.c $(CONFIG_H) $(SYSTEM_H) toplev.h $(RTL_H) $(GGC_H)
 bb-reorder.o : bb-reorder.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(TREE_H) \
-   flags.h $(BASIC_BLOCK_H) hard-reg-set.h output.h cfglayout.h $(TARGET_H)
+   flags.h $(BASIC_BLOCK_H) hard-reg-set.h output.h cfglayout.h $(TARGET_H) \
+   profile.h $(FIBHEAP_H)
 tracer.o : tracer.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(TREE_H) \
    $(BASIC_BLOCK_H) hard-reg-set.h output.h cfglayout.h flags.h \
-   $(PARAMS_H) profile.h
+   $(PARAMS_H) profile.h $(FIBHEAP_H)
 cfglayout.o : cfglayout.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(TREE_H) \
    insn-config.h $(BASIC_BLOCK_H) hard-reg-set.h output.h function.h \
-   cfglayout.h
+   cfglayout.h cfgloop.h $(TARGET_H)
 timevar.o : timevar.c $(CONFIG_H) $(SYSTEM_H) $(TIMEVAR_H) flags.h intl.h
 regrename.o : regrename.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) insn-config.h \
    $(BASIC_BLOCK_H) $(REGS_H) hard-reg-set.h output.h $(RECOG_H) function.h \
@@ -2303,7 +2325,15 @@ test-protoize-simple: ./protoize ./unpro
 	diff $(srcdir)/protoize.c tmp-proto.c | cat
 	-rm -f tmp-proto.[cs] tmp-proto$(objext)
 
-gcov.o: gcov.c gcov-io.h intl.h $(SYSTEM_H) $(CONFIG_H)
+# gcov-iov.c is run on the build machine to generate gcov-iov.h from version.c
+gcov-iov.o: gcov-iov.c version.c $(HCONFIG_H) $(SYSTEM_H)
+	$(CC) -c $(ALL_CFLAGS) $(INCLUDES) $(srcdir)/gcov-iov.c $(OUTPUT_OPTION)
+gcov-iov$(exeext): gcov-iov.o
+	$(CC) $(ALL_CFLAGS) $(LDFLAGS) gcov-iov.o -o $@
+gcov-iov.h: gcov-iov$(build_exeext)
+	./$< > $@
+
+gcov.o: gcov.c gcov-io.h gcov-iov.h intl.h $(SYSTEM_H) $(CONFIG_H)
 
 # Only one of 'gcov' or 'gcov.exe' is actually built, depending
 # upon whether $(exeext) is empty or not.
@@ -2676,7 +2706,7 @@ clean: mostlyclean $(INTL_CLEAN) lang.cl
 	else if [ "x$(MULTILIB_OPTIONS)" != x ] ; then \
 	  rm -rf `echo $(MULTILIB_OPTIONS) | sed -e 's/\// /g'`; \
 	fi ; fi
-	-rm -fr stage1 stage2 stage3 stage4
+	-rm -fr stage1 stage2 stage3 stage4 stageprofile stagefeedback
 # Delete stamps of bootstrap stages
 	-rm -f stage?_*
 	-rm -f clean?_*
@@ -2692,7 +2722,7 @@ distclean: clean $(INTL_DISTCLEAN) lang.
 	-rm -f Make-lang Make-hooks Make-host Make-target
 	-rm -f Makefile *.oaux
 	-rm -f gthr-default.h
-	-rm -f */stage1 */stage2 */stage3 */stage4 */include
+	-rm -f */stage1 */stage2 */stage3 */stage4 */include */stageprofile */stagefeedback
 	-rm -f c-parse.output
 	-rm -f *.asm
 	-rm -f site.exp site.bak testsuite/site.exp testsuite/site.bak
@@ -3316,12 +3346,11 @@ VOL_FILES=`echo $(BACKEND) $(OBJS) $(C_O
 # Flags to pass to stage2 and later recursive makes.  Note that the
 # WARN_CFLAGS setting can't be to the expansion of GCC_WARN_CFLAGS in
 # the context of the stage_x rule.
-STAGE2_FLAGS_TO_PASS = \
+POSTSTAGE1_FLAGS_TO_PASS = \
 	ADAC="\$$(CC)" \
 	AR_FOR_TARGET="$(AR_FOR_TARGET)" \
 	RANLIB_FOR_TARGET="$(RANLIB_FOR_TARGET)" \
 	CFLAGS="$(BOOT_CFLAGS)" \
-	LDFLAGS="$(BOOT_LDFLAGS)" \
 	WARN_CFLAGS="\$$(GCC_WARN_CFLAGS)" \
 	STRICT_WARN="$(STRICT2_WARN)" \
 	libdir=$(libdir) \
@@ -3329,6 +3358,19 @@ STAGE2_FLAGS_TO_PASS = \
 	MAKEOVERRIDES= \
 	OUTPUT_OPTION="-o \$$@"
 
+STAGE2_FLAGS_TO_PASS = \
+	LDFLAGS="$(BOOT_LDFLAGS)" \
+	CFLAGS="$(BOOT_CFLAGS)" 
+
+STAGEPROFILE_FLAGS_TO_PASS = \
+	LDFLAGS="$(BOOT_LDFLAGS) --static" \
+	CFLAGS="$(BOOT_CFLAGS) -fprofile-arcs" 
+
+# Files never linked into the final executable produces warnings about missing
+# profile.
+STAGEFEEDBACK_FLAGS_TO_PASS = \
+	CFLAGS="$(BOOT_CFLAGS) -fbranch-probabilities" 
+
 # Only build the C compiler for stage1, because that is the only one that
 # we can guarantee will build with the native compiler, and also it is the
 # only thing useful for building stage2. STAGE1_CFLAGS (via CFLAGS),
@@ -3351,6 +3393,7 @@ stage1_copy: stage1_build
 stage2_build: stage1_copy
 	$(MAKE) CC="$(STAGE_CC_WRAPPER) stage1/xgcc$(exeext) -Bstage1/ -B$(build_tooldir)/bin/" \
 		 STAGE_PREFIX=stage1/ \
+		 $(POSTSTAGE1_FLAGS_TO_PASS) \
 		 $(STAGE2_FLAGS_TO_PASS)
 	$(STAMP) stage2_build
 	echo stage2_build > stage_last
@@ -3360,9 +3403,36 @@ stage2_copy: stage2_build
 	$(STAMP) stage2_copy
 	echo stage3_build > stage_last
 
+stageprofile_build: stage1_copy
+	$(MAKE) CC="$(STAGE_CC_WRAPPER) stage1/xgcc$(exeext) -Bstage1/ -B$(build_tooldir)/bin/" \
+		 STAGE_PREFIX=stage1/ \
+		 $(POSTSTAGE1_FLAGS_TO_PASS) \
+		 $(STAGEPROFILE_FLAGS_TO_PASS)
+	$(STAMP) stageprofile_build
+	echo stageprofile_build > stage_last
+
+stageprofile_copy: stageprofile_build
+	$(MAKE) stageprofile
+	$(STAMP) stageprofile_copy
+	echo stagefeedback_build > stage_last
+
+stagefeedback_build: stageprofile_copy stage1_copy
+	$(MAKE) CC="$(STAGE_CC_WRAPPER) stage1/xgcc$(exeext) -Bstage1/ -B$(build_tooldir)/bin/" \
+		 STAGE_PREFIX=stage1/ \
+		 $(POSTSTAGE1_FLAGS_TO_PASS) \
+		 $(STAGEFEEDBACK_FLAGS_TO_PASS)
+	$(STAMP) stagefeedback_build
+	echo stagefeedback_build > stage_last
+
+stagefeedback_copy: stagefeedback_build
+	$(MAKE) stagefeedback
+	$(STAMP) stagefeedback_copy
+	echo stagefeedback2_build > stage_last
+
 stage3_build: stage2_copy
 	$(MAKE) CC="$(STAGE_CC_WRAPPER) stage2/xgcc$(exeext) -Bstage2/ -B$(build_tooldir)/bin/" \
 		 STAGE_PREFIX=stage2/ \
+		 $(POSTSTAGE1_FLAGS_TO_PASS) \
 		 $(STAGE2_FLAGS_TO_PASS)
 	$(STAMP) stage3_build
 	echo stage3_build > stage_last
@@ -3376,6 +3446,7 @@ stage3_copy: stage3_build
 stage4_build: stage3_copy
 	$(MAKE) CC="$(STAGE_CC_WRAPPER) stage3/xgcc$(exeext) -Bstage3/ -B$(build_tooldir)/bin/" \
 		 STAGE_PREFIX=stage3/ \
+		 $(POSTSTAGE1_FLAGS_TO_PASS) \
 		 $(STAGE2_FLAGS_TO_PASS)
 	$(STAMP) stage4_build
 	echo stage4_build > stage_last
@@ -3411,7 +3482,7 @@ bootstrap3 bootstrap3-lean: bootstrap
 
 bootstrap4 bootstrap4-lean: stage4_build
 
-unstage1 unstage2 unstage3 unstage4:
+unstage1 unstage2 unstage3 unstage4 unstageprofile unstagefeedback:
 	-set -vx; stage=`echo $@ | sed -e 's/un//'`; \
 	rm -f $$stage/as$(exeext); \
 	rm -f $$stage/ld$(exeext); \
@@ -3441,6 +3512,12 @@ restage3: unstage3
 restage4: unstage4
 	$(MAKE) LANGUAGES="$(LANGUAGES)" stage4_build
 
+restageprofile: unstageprofile
+	$(MAKE) LANGUAGES="$(LANGUAGES)" stageprofile_build
+
+restagefeedback: unstagefeedback
+	$(MAKE) LANGUAGES="$(LANGUAGES)" stagefeedback_build
+
 bubblestrap:
 	if test -f stage3_build; then true; else \
 	  echo; echo You must \"make bootstrap\" first.; \
@@ -3659,6 +3736,63 @@ stage4-start:
 	fi; done
 stage4: force stage4-start lang.stage4
 
+stageprofile-start:
+	-if [ -d stageprofile ] ; then true ; else mkdir stageprofile ; fi
+	$(MAKE) -f libgcc.mk libgcc-stage-start stage=stageprofile
+	-for dir in intl $(SUBDIRS) ; \
+	 do \
+	   if [ -d stageprofile/$$dir ] ; then true ; else mkdir stageprofile/$$dir ; fi ; \
+	 done
+	-mv $(STAGESTUFF) stageprofile
+	-mv intl/*$(objext) stageprofile/intl
+# Copy as/ld if they exist to stage dir, so that running xgcc from the stage
+# dir will work properly.
+	-if [ -f as$(exeext) ] ; then (cd stageprofile && $(LN_S) ../as$(exeext) .) ; else true ; fi
+	-if [ -f ld$(exeext) ] ; then (cd stageprofile && $(LN_S) ../ld$(exeext) .) ; else true ; fi
+	-if [ -f collect-ld$(exeext) ] ; then (cd stageprofile && $(LN_S) ../collect-ld$(exeext) .) ; else true ; fi
+	-rm -f stageprofile/libgcc.a stageprofile/libgcov.a stageprofile/libgcc_eh.a
+	-cp libgcc.a stageprofile
+	-$(RANLIB_FOR_TARGET) stageprofile/libgcc.a
+	-cp libgcov.a stageprofile
+	-$(RANLIB_FOR_TARGET) stageprofile/libgcov.a
+	-if [ -f libgcc_eh.a ] ; then cp libgcc_eh.a stageprofile; \
+	   $(RANLIB_FOR_TARGET) stageprofile/libgcc_eh.a; \
+	fi
+	-for f in .. $(EXTRA_MULTILIB_PARTS); do if [ x$${f} != x.. ]; then \
+	  cp stageprofile/$${f} . ; \
+	else true; \
+	fi; done
+stageprofile: force stageprofile-start lang.stageprofile
+
+stagefeedback-start:
+	-if [ -d stagefeedback ] ; then true ; else mkdir stagefeedback ; fi
+	$(MAKE) -f libgcc.mk libgcc-stage-start stage=stagefeedback
+	-for dir in intl $(SUBDIRS) ; \
+	 do \
+	   if [ -d stagefeedback/$$dir ] ; then true ; else mkdir stagefeedback/$$dir ; fi ; \
+	 done
+	-mv $(STAGESTUFF) stagefeedback
+	-mv intl/*$(objext) stagefeedback/intl
+# Copy as/ld if they exist to stage dir, so that running xgcc from the stage
+# dir will work properly.
+	-if [ -f as$(exeext) ] ; then (cd stagefeedback && $(LN_S) ../as$(exeext) .) ; else true ; fi
+	-if [ -f ld$(exeext) ] ; then (cd stagefeedback && $(LN_S) ../ld$(exeext) .) ; else true ; fi
+	-if [ -f collect-ld$(exeext) ] ; then (cd stagefeedback && $(LN_S) ../collect-ld$(exeext) .) ; else true ; fi
+	-rm -f stagefeedback/libgcc.a stagefeedback/libgcov.a stagefeedback/libgcc_eh.a
+	-rm -f *.da ada/*.da cp/*.da f/*.da java/*.da objc/*.da fixinc/*.da intl po testsuite 2>/dev/null
+	-cp libgcc.a stagefeedback
+	-$(RANLIB_FOR_TARGET) stagefeedback/libgcc.a
+	-cp libgcov.a stagefeedback
+	-$(RANLIB_FOR_TARGET) stagefeedback/libgcov.a
+	-if [ -f libgcc_eh.a ] ; then cp libgcc_eh.a stagefeedback; \
+	   $(RANLIB_FOR_TARGET) stagefeedback/libgcc_eh.a; \
+	fi
+	-for f in .. $(EXTRA_MULTILIB_PARTS); do if [ x$${f} != x.. ]; then \
+	  cp stagefeedback/$${f} . ; \
+	else true; \
+	fi; done
+stagefeedback: force stagefeedback-start lang.stagefeedback
+
 # Copy just the executable files from a particular stage into a subdirectory,
 # and delete the object files.  Use this if you're just verifying a version
 # that is pretty sure to work, and you are short of disk space.
@@ -3677,6 +3811,7 @@ risky-stage4: stage4
 #In GNU Make, ignore whether `stage*' exists.
 .PHONY: stage1 stage2 stage3 stage4 clean maintainer-clean TAGS bootstrap
 .PHONY: risky-stage1 risky-stage2 risky-stage3 risky-stage4
+.PHONY: stagefeedback stageprofile
 
 force:
 
--- gcc-3.3.1/gcc/alias.c.hammer-branch	2003-07-07 09:31:42.000000000 +0200
+++ gcc-3.3.1/gcc/alias.c	2003-08-05 18:22:46.000000000 +0200
@@ -117,6 +117,7 @@ static int nonlocal_referenced_p_1      
 static int nonlocal_referenced_p        PARAMS ((rtx));
 static int nonlocal_set_p_1             PARAMS ((rtx *, void *));
 static int nonlocal_set_p               PARAMS ((rtx));
+static void memory_modified_1		PARAMS ((rtx, rtx, void *));
 
 /* Set up all info needed to perform alias analysis on memory references.  */
 
@@ -2686,6 +2687,35 @@ init_alias_once ()
   alias_sets = splay_tree_new (splay_tree_compare_ints, 0, 0);
 }
 
+/* Set MEMORY_MODIFIED when X modifies DATA (that is assumed
+   to be memory reference.  */
+static bool memory_modified;
+static void
+memory_modified_1 (x, pat, data)
+	rtx x, pat ATTRIBUTE_UNUSED;
+	void *data;
+{
+  if (GET_CODE (x) == MEM)
+    {
+      if (!nonoverlapping_memrefs_p (x, (rtx)data))
+	memory_modified = 1;
+    }
+}
+
+
+/* Return true when INSN possibly modify memory contents of MEM
+   (ie address can be modified).  */
+bool
+memory_modified_in_insn_p (mem, insn)
+     rtx mem, insn;
+{
+  if (!INSN_P (insn))
+    return false;
+  memory_modified = false;
+  note_stores (PATTERN (insn), memory_modified_1, mem);
+  return memory_modified;
+}
+
 /* Initialize the aliasing machinery.  Initialize the REG_KNOWN_VALUE
    array.  */
 
--- gcc-3.3.1/gcc/basic-block.h.hammer-branch	2003-03-25 21:31:38.000000000 +0100
+++ gcc-3.3.1/gcc/basic-block.h	2003-08-05 18:22:46.000000000 +0200
@@ -113,7 +113,11 @@ do {									\
    be done, other than zero the statistics on the first allocation.  */
 #define MAX_REGNO_REG_SET(NUM_REGS, NEW_P, RENUMBER_P)
 
-/* Type we use to hold basic block counters.  Should be at least 64bit.  */
+/* Type we use to hold basic block counters.  Should be at least
+   64bit.  Although a counter cannot be negative, we use a signed
+   type, because erroneous negative counts can be generated when the
+   flow graph is manipulated by various optimizations.  A signed type
+   makes those easy to detect. */
 typedef HOST_WIDEST_INT gcov_type;
 
 /* Control flow edge information.  */
@@ -127,6 +131,9 @@ typedef struct edge_def {
   /* Instructions queued on the edge.  */
   rtx insns;
 
+  /* Histogram of loop whose latch is this edge.  */
+  struct loop_histogram *loop_histogram;
+
   /* Auxiliary info specific to a pass.  */
   void *aux;
 
@@ -149,6 +156,10 @@ typedef struct edge_def {
 
 #define EDGE_COMPLEX	(EDGE_ABNORMAL | EDGE_ABNORMAL_CALL | EDGE_EH)
 
+/* Declared in cfgloop.h.  */
+struct loop;
+struct loops;
+struct loop_histogram;
 
 /* A basic block is a sequence of instructions with only entry and
    only one exit.  If any one of the instructions are executed, they
@@ -236,6 +247,8 @@ typedef struct basic_block_def {
 #define BB_NEW			2
 #define BB_REACHABLE		4
 #define BB_VISITED		8
+#define BB_IRREDUCIBLE_LOOP	16
+#define BB_SUPERBLOCK		32
 
 /* Number of basic blocks in the current function.  */
 
@@ -358,6 +371,9 @@ extern void tidy_fallthru_edges		PARAMS 
 extern void flow_reverse_top_sort_order_compute	PARAMS ((int *));
 extern int flow_depth_first_order_compute	PARAMS ((int *, int *));
 extern void flow_preorder_transversal_compute	PARAMS ((int *));
+extern int dfs_enumerate_from		PARAMS ((basic_block, int,
+						bool (*)(basic_block, void *),
+						basic_block *, int, void *));
 extern void dump_edge_info		PARAMS ((FILE *, edge, int));
 extern void clear_edges			PARAMS ((void));
 extern void mark_critical_edges		PARAMS ((void));
@@ -367,165 +383,6 @@ extern rtx first_insn_after_basic_block_
 
 typedef struct dominance_info *dominance_info;
 
-/* Structure to hold information for each natural loop.  */
-struct loop
-{
-  /* Index into loops array.  */
-  int num;
-
-  /* Basic block of loop header.  */
-  basic_block header;
-
-  /* Basic block of loop latch.  */
-  basic_block latch;
-
-  /* Basic block of loop pre-header or NULL if it does not exist.  */
-  basic_block pre_header;
-
-  /* Array of edges along the pre-header extended basic block trace.
-     The source of the first edge is the root node of pre-header
-     extended basic block, if it exists.  */
-  edge *pre_header_edges;
-
-  /* Number of edges along the pre_header extended basic block trace.  */
-  int num_pre_header_edges;
-
-  /* The first block in the loop.  This is not necessarily the same as
-     the loop header.  */
-  basic_block first;
-
-  /* The last block in the loop.  This is not necessarily the same as
-     the loop latch.  */
-  basic_block last;
-
-  /* Bitmap of blocks contained within the loop.  */
-  sbitmap nodes;
-
-  /* Number of blocks contained within the loop.  */
-  int num_nodes;
-
-  /* Array of edges that enter the loop.  */
-  edge *entry_edges;
-
-  /* Number of edges that enter the loop.  */
-  int num_entries;
-
-  /* Array of edges that exit the loop.  */
-  edge *exit_edges;
-
-  /* Number of edges that exit the loop.  */
-  int num_exits;
-
-  /* Bitmap of blocks that dominate all exits of the loop.  */
-  sbitmap exits_doms;
-
-  /* The loop nesting depth.  */
-  int depth;
-
-  /* Superloops of the loop.  */
-  struct loop **pred;
-
-  /* The height of the loop (enclosed loop levels) within the loop
-     hierarchy tree.  */
-  int level;
-
-  /* The outer (parent) loop or NULL if outermost loop.  */
-  struct loop *outer;
-
-  /* The first inner (child) loop or NULL if innermost loop.  */
-  struct loop *inner;
-
-  /* Link to the next (sibling) loop.  */
-  struct loop *next;
-
-  /* Nonzero if the loop is invalid (e.g., contains setjmp.).  */
-  int invalid;
-
-  /* Auxiliary info specific to a pass.  */
-  void *aux;
-
-  /* The following are currently used by loop.c but they are likely to
-     disappear as loop.c is converted to use the CFG.  */
-
-  /* Nonzero if the loop has a NOTE_INSN_LOOP_VTOP.  */
-  rtx vtop;
-
-  /* Nonzero if the loop has a NOTE_INSN_LOOP_CONT.
-     A continue statement will generate a branch to NEXT_INSN (cont).  */
-  rtx cont;
-
-  /* The NOTE_INSN_LOOP_BEG.  */
-  rtx start;
-
-  /* The NOTE_INSN_LOOP_END.  */
-  rtx end;
-
-  /* For a rotated loop that is entered near the bottom,
-     this is the label at the top.  Otherwise it is zero.  */
-  rtx top;
-
-  /* Place in the loop where control enters.  */
-  rtx scan_start;
-
-  /* The position where to sink insns out of the loop.  */
-  rtx sink;
-
-  /* List of all LABEL_REFs which refer to code labels outside the
-     loop.  Used by routines that need to know all loop exits, such as
-     final_biv_value and final_giv_value.
-
-     This does not include loop exits due to return instructions.
-     This is because all bivs and givs are pseudos, and hence must be
-     dead after a return, so the presense of a return does not affect
-     any of the optimizations that use this info.  It is simpler to
-     just not include return instructions on this list.  */
-  rtx exit_labels;
-
-  /* The number of LABEL_REFs on exit_labels for this loop and all
-     loops nested inside it.  */
-  int exit_count;
-};
-
-
-/* Structure to hold CFG information about natural loops within a function.  */
-struct loops
-{
-  /* Number of natural loops in the function.  */
-  int num;
-
-  /* Maxium nested loop level in the function.  */
-  int levels;
-
-  /* Array of natural loop descriptors (scanning this array in reverse order
-     will find the inner loops before their enclosing outer loops).  */
-  struct loop *array;
-
-  /* The above array is unused in new loop infrastructure and is kept only for
-     purposes of the old loop optimizer.  Instead we store just pointers to
-     loops here.  */
-  struct loop **parray;
-
-  /* Pointer to root of loop heirachy tree.  */
-  struct loop *tree_root;
-
-  /* Information derived from the CFG.  */
-  struct cfg
-  {
-    /* The bitmap vector of dominators or NULL if not computed.  */
-    dominance_info dom;
-
-    /* The ordering of the basic blocks in a depth first search.  */
-    int *dfs_order;
-
-    /* The reverse completion ordering of the basic blocks found in a
-       depth first search.  */
-    int *rc_order;
-  } cfg;
-
-  /* Headers shared by multiple loops that should be merged.  */
-  sbitmap shared_headers;
-};
-
 /* Structure to group all of the information to process IF-THEN and
    IF-THEN-ELSE blocks for the conditional execution support.  This
    needs to be in a public file in case the IFCVT macros call
@@ -553,19 +410,6 @@ typedef struct ce_if_block
 
 } ce_if_block_t;
 
-extern int flow_loops_find PARAMS ((struct loops *, int flags));
-extern int flow_loops_update PARAMS ((struct loops *, int flags));
-extern void flow_loops_free PARAMS ((struct loops *));
-extern void flow_loops_dump PARAMS ((const struct loops *, FILE *,
-				     void (*)(const struct loop *,
-					      FILE *, int), int));
-extern void flow_loop_dump PARAMS ((const struct loop *, FILE *,
-				    void (*)(const struct loop *,
-					     FILE *, int), int));
-extern int flow_loop_scan PARAMS ((struct loops *, struct loop *, int));
-extern void flow_loop_tree_node_add PARAMS ((struct loop *, struct loop *));
-extern void flow_loop_tree_node_remove PARAMS ((struct loop *));
-
 /* This structure maintains an edge list vector.  */
 struct edge_list
 {
@@ -656,15 +500,6 @@ enum update_life_extent
 #define CLEANUP_THREADING	64	/* Do jump threading.  */
 #define CLEANUP_NO_INSN_DEL	128	/* Do not try to delete trivially dead
 					   insns.  */
-/* Flags for loop discovery.  */
-
-#define LOOP_TREE		1	/* Build loop hierarchy tree.  */
-#define LOOP_PRE_HEADER		2	/* Analyse loop pre-header.  */
-#define LOOP_ENTRY_EDGES	4	/* Find entry edges.  */
-#define LOOP_EXIT_EDGES		8	/* Find exit edges.  */
-#define LOOP_EDGES		(LOOP_ENTRY_EDGES | LOOP_EXIT_EDGES)
-#define LOOP_ALL	       15	/* All of the above  */
-
 extern void life_analysis	PARAMS ((rtx, FILE *, int));
 extern int update_life_info	PARAMS ((sbitmap, enum update_life_extent,
 					 int));
@@ -750,26 +585,6 @@ extern void free_aux_for_edges		PARAMS (
    debugger, and it is declared extern so we don't get warnings about
    it being unused.  */
 extern void verify_flow_info		PARAMS ((void));
-extern bool flow_loop_outside_edge_p	PARAMS ((const struct loop *, edge));
-extern bool flow_loop_nested_p		PARAMS ((const struct loop *,
-						 const struct loop *));
-extern bool flow_bb_inside_loop_p	PARAMS ((const struct loop *,
-						 const basic_block));
-extern basic_block *get_loop_body       PARAMS ((const struct loop *));
-extern int dfs_enumerate_from           PARAMS ((basic_block, int,
-				         bool (*)(basic_block, void *),
-					 basic_block *, int, void *));
-
-extern edge loop_preheader_edge PARAMS ((struct loop *));
-extern edge loop_latch_edge PARAMS ((struct loop *));
-
-extern void add_bb_to_loop PARAMS ((basic_block, struct loop *));
-extern void remove_bb_from_loops PARAMS ((basic_block));
-extern struct loop * find_common_loop PARAMS ((struct loop *, struct loop *));
-
-extern void verify_loop_structure PARAMS ((struct loops *, int));
-#define VLS_EXPECT_PREHEADERS 1
-#define VLS_EXPECT_SIMPLE_LATCHES 2
 
 typedef struct conflict_graph_def *conflict_graph;
 
--- gcc-3.3.1/gcc/bb-reorder.c.hammer-branch	2003-03-22 00:56:58.000000000 +0100
+++ gcc-3.3.1/gcc/bb-reorder.c	2003-08-05 18:22:46.000000000 +0200
@@ -1,5 +1,5 @@
 /* Basic block reordering routines for the GNU compiler.
-   Copyright (C) 2000, 2002 Free Software Foundation, Inc.
+   Copyright (C) 2000, 2002, 2003 Free Software Foundation, Inc.
 
    This file is part of GCC.
 
@@ -18,66 +18,51 @@
    Software Foundation, 59 Temple Place - Suite 330, Boston, MA
    02111-1307, USA.  */
 
-/* References:
+/* This (greedy) algorithm constructs traces in several rounds.
+   The construction starts from "seeds".  The seed for the first round
+   is the entry point of function.  When there are more than one seed
+   that one is selected first that has the lowest key in the heap
+   (see function bb_to_key).  Then the algorithm repeatedly adds the most
+   probable successor to the end of a trace.  Finally it connects the traces.
+
+   There are two parameters: Branch Threshold and Exec Threshold.
+   If the edge to a successor of the actual basic block is lower than
+   Branch Threshold or the frequency of the successor is lower than
+   Exec Threshold the successor will be the seed in one of the next rounds.
+   Each round has these parameters lower than the previous one.
+   The last round has to have these parameters set to zero
+   so that the remaining blocks are picked up.
+
+   The algorithm selects the most probable successor from all unvisited
+   successors and successors that have been added to this trace.
+   The other successors (that has not been "sent" to the next round) will be
+   other seeds for this round and the secondary traces will start in them.
+   If the successor has not been visited in this trace it is added to the trace
+   (however, there is some heuristic for simple branches).
+   If the successor has been visited in this trace the loop has been found.
+   If the loop has many iterations the loop is rotated so that the
+   source block of the most probable edge going out from the loop
+   is the last block of the trace.
+   If the loop has few iterations and there is no edge from the last block of
+   the loop going out from loop the loop header is duplicated.
+   Finally, the construction of the trace is terminated.
+
+   When connecting traces it first checks whether there is an edge from the
+   last block of one trace to the first block of another trace.
+   When there are still some unconnected traces it checks whether there exists
+   a basic block BB such that BB is a successor of the last bb of one trace
+   and BB is a predecessor of the first block of another trace. In this case,
+   BB is duplicated and the traces are connected through this duplicate.
+   The rest of traces are simply connected so there will be a jump to the
+   beginning of the rest of trace.
+
+
+   References:
+
+   "Software Trace Cache"
+   A. Ramirez, J. Larriba-Pey, C. Navarro, J. Torrellas and M. Valero; 1999
+   http://citeseer.nj.nec.com/15361.html
 
-   "Profile Guided Code Positioning"
-   Pettis and Hanson; PLDI '90.
-
-   TODO:
-
-   (1) Consider:
-
-		if (p) goto A;		// predict taken
-		foo ();
-	      A:
-		if (q) goto B;		// predict taken
-		bar ();
-	      B:
-		baz ();
-		return;
-
-       We'll currently reorder this as
-
-		if (!p) goto C;
-	      A:
-		if (!q) goto D;
-	      B:
-		baz ();
-		return;
-	      D:
-		bar ();
-		goto B;
-	      C:
-		foo ();
-		goto A;
-
-       A better ordering is
-
-		if (!p) goto C;
-		if (!q) goto D;
-	      B:
-		baz ();
-		return;
-	      C:
-		foo ();
-		if (q) goto B;
-	      D:
-		bar ();
-		goto B;
-
-       This requires that we be able to duplicate the jump at A, and
-       adjust the graph traversal such that greedy placement doesn't
-       fix D before C is considered.
-
-   (2) Coordinate with shorten_branches to minimize the number of
-       long branches.
-
-   (3) Invent a method by which sufficiently non-predicted code can
-       be moved to either the end of the section or another section
-       entirely.  Some sort of NOTE_INSN note would work fine.
-
-       This completely scroggs all debugging formats, so the user
-       would have to explicitly ask for it.
 */
 
 #include "config.h"
@@ -89,215 +74,999 @@
 #include "flags.h"
 #include "output.h"
 #include "cfglayout.h"
-#include "function.h"
+#include "fibheap.h"
 #include "target.h"
+#include "profile.h"
+
+/* The number of rounds.  */
+#define N_ROUNDS 4
+
+/* Branch thresholds in thousandths (per mille) of the REG_BR_PROB_BASE.  */
+static int branch_threshold[N_ROUNDS] = {400, 200, 100, 0};
+
+/* Exec thresholds in thousandths (per mille) of the frequency of bb 0.  */
+static int exec_threshold[N_ROUNDS] = {500, 200, 50, 0};
+
+/* If edge frequency is lower than DUPLICATION_THRESHOLD per mille of entry
+   block the edge destination is not duplicated while connecting traces.  */
+#define DUPLICATION_THRESHOLD 100
+
+/* Length of unconditional jump instruction.  */
+static int uncond_jump_length;
+
+/* The current size of the following dynamic arrays.  */
+static int array_size;
+
+/* To avoid frequent reallocation the size of arrays is greater than needed,
+   the number of elements is (not less than) 1.25 * size_wanted.  */
+#define GET_ARRAY_SIZE(X) ((((X) / 4) + 1) * 5)
+
+/* Which trace is the bb start of (-1 means it is not a start of a trace).  */
+static int *start_of_trace;
+static int *end_of_trace;
+
+/* Which heap and node is BB in?  */
+static fibheap_t *bb_heap;
+static fibnode_t *bb_node;
+
+/* Free the memory and set the pointer to NULL.  */
+#define FREE(P) \
+  do { if (P) { free (P); P = 0; } else { abort (); } } while (0)
+
+/* Structure for holding information about a trace.  */
+struct trace
+{
+  /* First and last basic block of the trace.  */
+  basic_block first, last;
+
+  /* The round of the STC creation which this trace was found in.  */
+  int round;
+
+  /* The length (i.e. the number of basic blocks) of the trace.  */
+  int length;
+};
+
+/* Maximum frequency and count of one of the entry blocks.  */
+int max_entry_frequency;
+gcov_type max_entry_count;
 
 /* Local function prototypes.  */
-static void make_reorder_chain		PARAMS ((void));
-static basic_block make_reorder_chain_1	PARAMS ((basic_block, basic_block));
-static basic_block maybe_duplicate_computed_goto_succ PARAMS ((basic_block));
+static void find_traces			PARAMS ((int *, struct trace *));
+static basic_block rotate_loop		PARAMS ((edge, struct trace *, int));
+static void mark_bb_visited		PARAMS ((basic_block, int));
+static void find_traces_1_round		PARAMS ((int, int, gcov_type,
+						 struct trace *, int *, int,
+						 fibheap_t *));
+static basic_block copy_bb		PARAMS ((basic_block, edge,
+						 basic_block, int));
+static fibheapkey_t bb_to_key		PARAMS ((basic_block));
+static bool better_edge_p		PARAMS ((basic_block, edge, int, int,
+						 int, int));
+static void connect_traces		PARAMS ((int, struct trace *));
+static bool copy_bb_p			PARAMS ((basic_block, bool));
+static int get_uncond_jump_length	PARAMS ((void));
 
-/* Compute an ordering for a subgraph beginning with block BB.  Record the
-   ordering in RBI()->index and chained through RBI()->next.  */
+/* Find the traces for Software Trace Cache.  Chain each trace through
+   RBI()->next.  Store the number of traces to N_TRACES and description of
+   traces to TRACES.  */
 
 static void
-make_reorder_chain ()
+find_traces (n_traces, traces)
+     int *n_traces;
+     struct trace *traces;
 {
-  basic_block prev = NULL;
-  basic_block next, bb;
+  int i;
+  edge e;
+  fibheap_t heap;
 
-  /* Loop until we've placed every block.  */
-  do
+  /* We need to know some information for each basic block.  */
+  array_size = GET_ARRAY_SIZE (last_basic_block);
+  start_of_trace = xmalloc (array_size * sizeof (int));
+  end_of_trace = xmalloc (array_size * sizeof (int));
+  bb_heap = xmalloc (array_size * sizeof (fibheap_t));
+  bb_node = xmalloc (array_size * sizeof (fibnode_t));
+  for (i = 0; i < array_size; i++)
     {
-      next = NULL;
+      start_of_trace[i] = -1;
+      end_of_trace[i] = -1;
+      bb_heap[i] = NULL;
+      bb_node[i] = NULL;
+    }
 
-      /* Find the next unplaced block.  */
-      /* ??? Get rid of this loop, and track which blocks are not yet
-	 placed more directly, so as to avoid the O(N^2) worst case.
-	 Perhaps keep a doubly-linked list of all to-be-placed blocks;
-	 remove from the list as we place.  The head of that list is
-	 what we're looking for here.  */
-
-      FOR_EACH_BB (bb)
-	if (! RBI (bb)->visited)
-	  {
-	    next = bb;
-	    break;
-	  }
+  /* Insert entry points of function into heap.  */
+  heap = fibheap_new ();
+  max_entry_frequency = 0;
+  max_entry_count = 0;
+  for (e = ENTRY_BLOCK_PTR->succ; e; e = e->succ_next)
+    {
+      int bb_index = e->dest->index;
+      bb_heap[bb_index] = heap;
+      bb_node[bb_index] = fibheap_insert (heap, bb_to_key (e->dest), e->dest);
+      if (e->dest->frequency > max_entry_frequency)
+	max_entry_frequency = e->dest->frequency;
+      if (e->dest->count > max_entry_count)
+	max_entry_count = e->dest->count;
+    }
+
+  /* Find the traces.  */
+  for (i = 0; i < N_ROUNDS; i++)
+    {
+      gcov_type count_threshold;
+
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, "STC - round %d\n", i + 1);
 
-      if (next)
-	prev = make_reorder_chain_1 (next, prev);
+      if (max_entry_count < INT_MAX / 1000)
+	count_threshold = max_entry_count * exec_threshold[i] / 1000;
+      else
+	count_threshold = max_entry_count / 1000 * exec_threshold[i];
+
+      find_traces_1_round (REG_BR_PROB_BASE * branch_threshold[i] / 1000,
+			   max_entry_frequency * exec_threshold[i] / 1000,
+			   count_threshold, traces, n_traces, i, &heap);
+    }
+  fibheap_delete (heap);
+  FREE (bb_node);
+  FREE (bb_heap);
+
+  if (rtl_dump_file)
+    {
+      for (i = 0; i < *n_traces; i++)
+	{
+	  basic_block bb;
+	  fprintf (rtl_dump_file, "Trace %d (round %d):  ", i + 1,
+		   traces[i].round + 1);
+	  for (bb = traces[i].first; bb != traces[i].last; bb = RBI (bb)->next)
+	    fprintf (rtl_dump_file, "%d [%d] ", bb->index, bb->frequency);
+	  fprintf (rtl_dump_file, "%d [%d]\n", bb->index, bb->frequency);
+	}
+      fflush (rtl_dump_file);
     }
-  while (next);
-  RBI (prev)->next = NULL;
 }
 
-/* If the successor is our artificial computed_jump block, duplicate it.  */
+/* Rotate loop whose back edge is BACK_EDGE in the tail of trace TRACE
+   (with sequential number TRACE_N).  */
+
+static basic_block
+rotate_loop (back_edge, trace, trace_n)
+     edge back_edge;
+     struct trace *trace;
+     int trace_n;
+{
+  basic_block bb;
 
-static inline basic_block
-maybe_duplicate_computed_goto_succ (bb)
+  /* Information about the best end (end after rotation) of the loop.  */
+  basic_block best_bb = NULL;
+  edge best_edge = NULL;
+  int best_freq = -1;
+  gcov_type best_count = -1;
+  /* The best edge is preferred when its destination is not visited yet
+     or is a start block of some trace.  */
+  bool is_preferred = false;
+
+  /* Find the most frequent edge that goes out from current trace.  */
+  bb = back_edge->dest;
+  do
+    {
+      edge e;
+      for (e = bb->succ; e; e = e->succ_next)
+	if (e->dest != EXIT_BLOCK_PTR
+	    && RBI (e->dest)->visited != trace_n
+	    && (e->flags & EDGE_CAN_FALLTHRU)
+	    && !(e->flags & EDGE_COMPLEX))
+	{
+	  if (is_preferred)
+	    {
+	      /* The best edge is preferred.  */
+	      if (!RBI (e->dest)->visited
+		  || start_of_trace[e->dest->index] >= 0)
+		{
+		  /* The current edge E is also preferred.  */
+		  int freq = EDGE_FREQUENCY (e);
+		  if (freq > best_freq || e->count > best_count)
+		    {
+		      best_freq = freq;
+		      best_count = e->count;
+		      best_edge = e;
+		      best_bb = bb;
+		    }
+		}
+	    }
+	  else
+	    {
+	      if (!RBI (e->dest)->visited
+		  || start_of_trace[e->dest->index] >= 0)
+		{
+		  /* The current edge E is preferred.  */
+		  is_preferred = true;
+		  best_freq = EDGE_FREQUENCY (e);
+		  best_count = e->count;
+		  best_edge = e;
+		  best_bb = bb;
+		}
+	      else
+		{
+		  int freq = EDGE_FREQUENCY (e);
+		  if (!best_edge || freq > best_freq || e->count > best_count)
+		    {
+		      best_freq = freq;
+		      best_count = e->count;
+		      best_edge = e;
+		      best_bb = bb;
+		    }
+		}
+	    }
+	}
+      bb = RBI (bb)->next;
+    }
+  while (bb != back_edge->dest);
+
+  if (best_bb)
+    {
+      /* Rotate the loop so that the BEST_EDGE goes out from the last block of
+	 the trace.  */
+      if (back_edge->dest == trace->first)
+	{
+	  trace->first = RBI (best_bb)->next;
+	}
+      else
+	{
+	  basic_block prev_bb;
+	  
+	  for (prev_bb = trace->first;
+	       RBI (prev_bb)->next != back_edge->dest;
+	       prev_bb = RBI (prev_bb)->next)
+	    ;
+	  RBI (prev_bb)->next = RBI (best_bb)->next;
+
+	  /* Try to get rid of uncond jump to cond jump.  */
+	  if (prev_bb->succ && !prev_bb->succ->succ_next)
+	    {
+	      basic_block header = prev_bb->succ->dest;
+
+	      /* Duplicate HEADER if it is a small block containing condjump
+		 in the end.  */
+	      if (any_condjump_p (header->end) && copy_bb_p (header, false))
+		{
+		  copy_bb (header, prev_bb->succ, prev_bb, trace_n);
+		}
+	    }
+	}
+    }
+  else
+    {
+      /* We have not found suitable loop tail so do no rotation.  */
+      best_bb = back_edge->src;
+    }
+  RBI (best_bb)->next = NULL;
+  return best_bb;
+}
+
+/* This function marks BB that it was visited in trace number TRACE.  */
+
+static void
+mark_bb_visited (bb, trace)
      basic_block bb;
+     int trace;
 {
-  edge e;
-  basic_block next;
+  RBI (bb)->visited = trace;
+  if (bb_heap[bb->index])
+    {
+      fibheap_delete_node (bb_heap[bb->index], bb_node[bb->index]);
+      bb_heap[bb->index] = NULL;
+      bb_node[bb->index] = NULL;
+    }
+}
+
+/* One round of finding traces. Find traces for BRANCH_TH and EXEC_TH i.e. do
+   not include basic blocks their probability is lower than BRANCH_TH or their
+   frequency is lower than EXEC_TH into traces (or count is lower than
+   COUNT_TH).  It stores the new traces into TRACES and modifies the number of
+   traces *N_TRACES. Sets the round (which the trace belongs to) to ROUND. It
+   expects that starting basic blocks are in *HEAP and at the end it deletes
+   *HEAP and stores starting points for the next round into new *HEAP.  */
+
+static void
+find_traces_1_round (branch_th, exec_th, count_th, traces, n_traces, round,
+		     heap)
+     int branch_th;
+     int exec_th;
+     gcov_type count_th;
+     struct trace *traces;
+     int *n_traces;
+     int round;
+     fibheap_t *heap;
+{
+  /* Heap for discarded basic blocks which are possible starting points for
+     the next round.  */
+  fibheap_t new_heap = fibheap_new ();
 
-  /* Note that we can't rely on computed_goto_common_label still being in
-     the instruction stream -- cfgloop.c likes to munge things about.  But
-     we can still use it's non-null-ness to avoid a fruitless search.  */
-  if (!cfun->computed_goto_common_label)
-    return NULL;
-
-  /* Only want to duplicate when coming from a simple branch.  */
-  e = bb->succ;
-  if (!e || e->succ_next)
-    return NULL;
-
-  /* Only duplicate if we've already layed out this block once.  */
-  next = e->dest;
-  if (!RBI (next)->visited)
-    return NULL;
-
-  /* See if the block contains only a computed branch.  */
-  if ((next->head == next->end
-       || next_active_insn (next->head) == next->end)
-      && computed_jump_p (next->end))
+  while (!fibheap_empty (*heap))
     {
+      basic_block bb;
+      struct trace *trace;
+      edge best_edge, e;
+      int bb_index;
+      fibheapkey_t key;
+
+      bb = fibheap_extract_min (*heap);
+      bb_heap[bb->index] = NULL;
+      bb_node[bb->index] = NULL;
+
       if (rtl_dump_file)
-	fprintf (rtl_dump_file, "Duplicating block %d after %d\n",
-		 next->index, bb->index);
-      return cfg_layout_duplicate_bb (next, e);
+	fprintf (rtl_dump_file, "Getting bb %d\n", bb->index);
+
+      /* If the BB's frequency is too low send BB to the next round.  */
+      if (bb->frequency < exec_th || bb->count < count_th
+	  || ((round < N_ROUNDS - 1) && probably_never_executed_bb_p (bb)))
+	{
+	  int key = bb_to_key (bb);
+	  bb_heap[bb->index] = new_heap;
+	  bb_node[bb->index] = fibheap_insert (new_heap, key, bb);
+
+	  if (rtl_dump_file)
+	    fprintf (rtl_dump_file,
+		     "  Possible start point of next round: %d (key: %d)\n",
+		     bb->index, key);
+	  continue;
+	}
+
+      trace = traces + *n_traces;
+      trace->first = bb;
+      trace->round = round;
+      trace->length = 0;
+      (*n_traces)++;
+
+      do
+	{
+	  int prob, freq;
+
+	  /* The probability and frequency of the best edge.  */
+	  int best_prob = INT_MIN / 2;
+	  int best_freq = INT_MIN / 2;
+
+	  best_edge = NULL;
+	  mark_bb_visited (bb, *n_traces);
+	  trace->length++;
+
+	  if (rtl_dump_file)
+	    fprintf (rtl_dump_file, "Basic block %d was visited in trace %d\n",
+		     bb->index, *n_traces - 1);
+
+	  /* Select the successor that will be placed after BB.  */
+	  for (e = bb->succ; e; e = e->succ_next)
+	    {
+	      if (e->flags & EDGE_FAKE)
+		abort ();
+
+	      if (e->dest == EXIT_BLOCK_PTR)
+		continue;
+
+	      if (RBI (e->dest)->visited
+		  && RBI (e->dest)->visited != *n_traces)
+		continue;
+
+	      prob = e->probability;
+	      freq = EDGE_FREQUENCY (e);
+
+	      /* Edge that cannot be fallthru or improbable or infrequent
+		 successor (ie. it is unsuitable successor).  */
+	      if (!(e->flags & EDGE_CAN_FALLTHRU) || (e->flags & EDGE_COMPLEX)
+		  || prob < branch_th || freq < exec_th || e->count < count_th)
+		continue;
+
+	      if (better_edge_p (bb, e, prob, freq, best_prob, best_freq))
+		{
+		  best_edge = e;
+		  best_prob = prob;
+		  best_freq = freq;
+		}
+	    }
+
+	  /* Add all non-selected successors to the heaps.  */
+	  for (e = bb->succ; e; e = e->succ_next)
+	    {
+	      if (e == best_edge
+		  || e->dest == EXIT_BLOCK_PTR
+		  || RBI (e->dest)->visited)
+		continue;
+
+	      key = bb_to_key (e->dest);
+	      bb_index = e->dest->index;
+
+	      if (bb_heap[bb_index])
+		{
+		  if (key != bb_node[bb_index]->key)
+		    {
+		      if (rtl_dump_file)
+			{
+			  fprintf (rtl_dump_file,
+				   "Changing key for bb %d from %ld to %ld.\n",
+				   bb_index, (long) bb_node[bb_index]->key,
+				   key);
+			}
+		      fibheap_replace_key (bb_heap[bb_index],
+					   bb_node[bb_index], key);
+		    }
+		}
+	      else
+		{
+		  fibheap_t which_heap = *heap;
+
+		  prob = e->probability;
+		  freq = EDGE_FREQUENCY (e);
+
+		  if (!(e->flags & EDGE_CAN_FALLTHRU)
+		      || (e->flags & EDGE_COMPLEX)
+		      || prob < branch_th || freq < exec_th
+		      || e->count < count_th)
+		    {
+		      if (round < N_ROUNDS - 1)
+			which_heap = new_heap;
+		    }
+
+		  bb_heap[bb_index] = which_heap;
+		  bb_node[bb_index] = fibheap_insert (which_heap, key, e->dest);
+
+		  if (rtl_dump_file)
+		    {
+		      fprintf (rtl_dump_file,
+			       "  Possible start of %s round: %d (key: %ld)\n",
+			       (which_heap == new_heap) ? "next" : "this",
+			       bb_index, (long) key);
+		    }
+
+		}
+	    }
+
+	  if (best_edge) /* Suitable successor was found.  */
+	    {
+	      if (RBI (best_edge->dest)->visited == *n_traces)
+		{
+		  /* We do nothing with one basic block loops.  */
+		  if (best_edge->dest != bb)
+		    {
+		      if (EDGE_FREQUENCY (best_edge)
+			  > 4 * best_edge->dest->frequency / 5)
+			{
+			  /* The loop has at least 4 iterations.  If the loop
+			     header is not the first block of the function
+			     we can rotate the loop.  */
+
+			  if (best_edge->dest != ENTRY_BLOCK_PTR->next_bb)
+			    {
+			      if (rtl_dump_file)
+				{
+				  fprintf (rtl_dump_file, 
+					   "Rotating loop %d - %d\n",
+					   best_edge->dest->index, bb->index);
+				}
+			      RBI (bb)->next = best_edge->dest;
+			      bb = rotate_loop (best_edge, trace, *n_traces);
+			    }
+			}
+		      else
+			{
+			  /* The loop has less than 4 iterations.  */
+
+			  /* Check whether there is another edge from BB.  */
+			  edge another_edge;
+			  for (another_edge = bb->succ;
+			       another_edge;
+			       another_edge = another_edge->succ_next)
+			    if (another_edge != best_edge)
+			      break;
+				
+			  if (!another_edge && copy_bb_p (best_edge->dest,
+							  !optimize_size))
+			    {
+			      bb = copy_bb (best_edge->dest, best_edge, bb,
+					    *n_traces);
+			    }
+			}
+		    }
+
+		  /* Terminate the trace.  */
+		  break;
+		}
+	      else
+		{
+		  /* Check for a situation
+
+		    A
+		   /|
+		  B |
+		   \|
+		    C
+
+		  where
+		  EDGE_FREQUENCY (AB) + EDGE_FREQUENCY (BC)
+		    >= EDGE_FREQUENCY (AC).
+		  (i.e. 2 * B->frequency >= EDGE_FREQUENCY (AC) )
+		  Best ordering is then A B C.
+
+		  This situation is created for example by:
+
+		  if (A) B;
+		  C;
+
+		  */
+
+		  for (e = bb->succ; e; e = e->succ_next)
+		    if (e != best_edge
+			&& (e->flags & EDGE_CAN_FALLTHRU)
+			&& !(e->flags & EDGE_COMPLEX)
+			&& !RBI (e->dest)->visited
+			&& !e->dest->pred->pred_next
+			&& e->dest->succ
+			&& (e->dest->succ->flags & EDGE_CAN_FALLTHRU)
+			&& !(e->dest->succ->flags & EDGE_COMPLEX)
+			&& !e->dest->succ->succ_next
+			&& e->dest->succ->dest == best_edge->dest
+			&& 2 * e->dest->frequency >= EDGE_FREQUENCY (best_edge))
+		      {
+			best_edge = e;
+			if (rtl_dump_file)
+			  fprintf (rtl_dump_file, "Selecting BB %d\n",
+				   best_edge->dest->index);
+			break;
+		      }
+
+		  RBI (bb)->next = best_edge->dest;
+		  bb = best_edge->dest;
+		}
+	    }
+	}
+      while (best_edge);
+      trace->last = bb;
+      start_of_trace[trace->first->index] = *n_traces - 1;
+      end_of_trace[trace->last->index] = *n_traces - 1;
+
+      /* The trace is terminated so we have to recount the keys in heap
+	 (some block can have a lower key because now one of its predecessors
+	 is an end of the trace).  */
+      for (e = bb->succ; e; e = e->succ_next)
+	{
+	  if (e->dest == EXIT_BLOCK_PTR
+	      || RBI (e->dest)->visited)
+	    continue;
+	  
+	  bb_index = e->dest->index;
+	  if (bb_heap[bb_index])
+	    {
+	      key = bb_to_key (e->dest);
+	      if (key != bb_node[bb_index]->key)
+		{
+		  if (rtl_dump_file)
+		    {
+		      fprintf (rtl_dump_file,
+			       "Changing key for bb %d from %ld to %ld.\n",
+			       bb_index, (long) bb_node[bb_index]->key, key);
+		    }
+		  fibheap_replace_key (bb_heap[bb_index], bb_node[bb_index],
+				       key);
+		}
+	    }
+	}
     }
 
-  return NULL;
+  fibheap_delete (*heap);
+
+  /* "Return" the new heap.  */
+  *heap = new_heap;
 }
 
-/* A helper function for make_reorder_chain.
+/* Create a duplicate of the basic block OLD_BB and redirect edge E to it, add
+   it to trace after BB, mark OLD_BB visited and update pass' data structures
+   (TRACE is a number of trace which OLD_BB is duplicated to).  */
 
-   We do not follow EH edges, or non-fallthru edges to noreturn blocks.
-   These are assumed to be the error condition and we wish to cluster
-   all of them at the very end of the function for the benefit of cache
-   locality for the rest of the function.
+static basic_block
+copy_bb (old_bb, e, bb, trace)
+     basic_block old_bb;
+     edge e;
+     basic_block bb;
+     int trace;
+{
+  basic_block new_bb;
 
-   ??? We could do slightly better by noticing earlier that some subgraph
-   has all paths leading to noreturn functions, but for there to be more
-   than one block in such a subgraph is rare.  */
+  new_bb = cfg_layout_duplicate_bb (old_bb, e);
+  if (e->dest != new_bb)
+    abort ();
+  if (RBI (e->dest)->visited)
+    abort ();
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file,
+	     "Duplicated bb %d (created bb %d)\n",
+	     old_bb->index, new_bb->index);
+  RBI (new_bb)->visited = trace;
+  RBI (new_bb)->next = RBI (bb)->next;
+  RBI (bb)->next = new_bb;
 
-static basic_block
-make_reorder_chain_1 (bb, prev)
+  if (new_bb->index >= array_size || last_basic_block > array_size)
+    {
+      int i;
+      int new_size;
+      
+      new_size = MAX (last_basic_block, new_bb->index + 1);
+      new_size = GET_ARRAY_SIZE (new_size);
+      
+      start_of_trace = xrealloc (start_of_trace, new_size * sizeof (int));
+      end_of_trace = xrealloc (end_of_trace, new_size * sizeof (int));
+      for (i = array_size; i < new_size; i++)
+	{
+	  start_of_trace[i] = -1;
+	  end_of_trace[i] = -1;
+	}
+      if (bb_heap)
+	{
+	  bb_heap = xrealloc (bb_heap, new_size * sizeof (fibheap_t));
+	  bb_node = xrealloc (bb_node, new_size * sizeof (fibnode_t));
+	  for (i = array_size; i < new_size; i++)
+	    {
+	      bb_heap[i] = NULL;
+	      bb_node[i] = NULL;
+	    }
+	}
+      array_size = new_size;
+      
+      if (rtl_dump_file)
+	{
+	  fprintf (rtl_dump_file,
+		   "Growing the dynamic arrays to %d elements.\n",
+		   array_size);
+	}
+    }
+
+  return new_bb;
+}
+
+/* Compute and return the key (for the heap) of the basic block BB.  */
+
+static fibheapkey_t
+bb_to_key (bb)
      basic_block bb;
-     basic_block prev;
 {
   edge e;
-  basic_block next;
-  rtx note;
 
-  /* Mark this block visited.  */
-  if (prev)
-    {
- restart:
-      RBI (prev)->next = bb;
+  int priority = 2;
 
-      if (rtl_dump_file && prev->next_bb != bb)
-	fprintf (rtl_dump_file, "Reordering block %d after %d\n",
-		 bb->index, prev->index);
-    }
-  else
+  /* Do not start in probably never executed blocks.  */
+  if (probably_never_executed_bb_p (bb))
+    return BB_FREQ_MAX;
+
+  /* Decrease the priority if there is an unvisited predecessor.  */
+  for (e = bb->pred; e; e = e->pred_next)
+    if (!(e->flags & EDGE_DFS_BACK) && !RBI (e->src)->visited)
+      {
+	priority = 0;
+	break;
+      }
+
+  /* Increase the priority if there is a predecessor which is an end of some
+     trace.  */
+  for (e = bb->pred; e; e = e->pred_next)
     {
-      if (bb->prev_bb != ENTRY_BLOCK_PTR)
-	abort ();
+      if (e->src != ENTRY_BLOCK_PTR && end_of_trace[e->src->index] >= 0)
+	{
+	  priority++;
+	  break;
+	}
     }
-  RBI (bb)->visited = 1;
-  prev = bb;
 
-  if (bb->succ == NULL)
-    return prev;
+  return -100 * BB_FREQ_MAX * priority - bb->frequency;
+}
 
-  /* Find the most probable block.  */
+/* Return true when the edge E from basic block BB is better than the temporary
+   best edge (details are in function).  The probability of edge E is PROB. The
+   frequency of the successor is FREQ.  The current best probability is
+   BEST_PROB, the best frequency is BEST_FREQ.
+   The edge is considered to be equivalent when PROB does not differ much from
+   BEST_PROB; similarly for frequency.  */
 
-  next = NULL;
-  if (any_condjump_p (bb->end)
-      && (note = find_reg_note (bb->end, REG_BR_PROB, 0)) != NULL)
+static bool
+better_edge_p (bb, e, prob, freq, best_prob, best_freq)
+     basic_block bb;
+     edge e;
+     int prob;
+     int freq;
+     int best_prob;
+     int best_freq;
+{
+  bool is_better_edge;
+
+  /* The BEST_* values do not have to be best, but can be a bit smaller than
+     maximum values.  */
+  int diff_prob = best_prob / 10;
+  int diff_freq = best_freq / 10;
+
+  if (prob > best_prob + diff_prob)
+    /* The edge has higher probability than the temporary best edge.  */
+    is_better_edge = true;
+  else if (prob < best_prob - diff_prob)
+    /* The edge has lower probability than the temporary best edge.  */
+    is_better_edge = false;
+  else if (freq < best_freq - diff_freq)
+    /* The edge and the temporary best edge  have almost equivalent
+       probabilities.  The higher frequency of a successor now means
+       that there is another edge going into that successor.
+       This successor has lower frequency so it is better.  */
+    is_better_edge = true;
+  else if (freq > best_freq + diff_freq)
+    /* This successor has higher frequency so it is worse.  */
+    is_better_edge = false;
+  else if (e->dest->prev_bb == bb)
+    /* The edges have equivalent probabilities and the successors
+       have equivalent frequencies.  Select the previous successor.  */
+    is_better_edge = true;
+  else
+    is_better_edge = false;
+
+  return is_better_edge;
+}
+
+/* Connect traces in array TRACES, N_TRACES is the count of traces.  */
+
+static void
+connect_traces (n_traces, traces)
+     int n_traces;
+     struct trace *traces;
+{
+  int i;
+  bool *connected;
+  int last_trace;
+  int freq_threshold;
+  gcov_type count_threshold;
+
+  freq_threshold = max_entry_frequency * DUPLICATION_THRESHOLD / 1000;
+  if (max_entry_count < INT_MAX / 1000)
+    count_threshold = max_entry_count * DUPLICATION_THRESHOLD / 1000;
+  else
+    count_threshold = max_entry_count / 1000 * DUPLICATION_THRESHOLD;
+
+  connected = xcalloc (n_traces, sizeof (bool));
+  last_trace = -1;
+  for (i = 0; i < n_traces; i++)
     {
-      int taken, probability;
-      edge e_taken, e_fall;
+      int t = i;
+      int t2;
+      edge e, best;
+      int best_len;
 
-      probability = INTVAL (XEXP (note, 0));
-      taken = probability > REG_BR_PROB_BASE / 2;
+      if (connected[t])
+	continue;
 
-      /* Find the normal taken edge and the normal fallthru edge.
+      connected[t] = true;
 
-	 Note, conditional jumps with other side effects may not
-	 be fully optimized.  In this case it is possible for
-	 the conditional jump to branch to the same location as
-	 the fallthru path.
+      /* Find the predecessor traces.  */
+      for (t2 = t; t2 > 0;)
+	{
+	  best = NULL;
+	  best_len = 0;
+	  for (e = traces[t2].first->pred; e; e = e->pred_next)
+	    {
+	      int si = e->src->index;
+
+	      if (e->src != ENTRY_BLOCK_PTR
+		  && (e->flags & EDGE_CAN_FALLTHRU)
+		  && !(e->flags & EDGE_COMPLEX)
+		  && end_of_trace[si] >= 0
+		  && !connected[end_of_trace[si]]
+		  && (!best 
+		      || e->probability > best->probability
+		      || (e->probability == best->probability
+			  && traces[end_of_trace[si]].length > best_len)))
+		{
+		  best = e;
+		  best_len = traces[end_of_trace[si]].length;
+		}
+	    }
+	  if (best)
+	    {
+	      RBI (best->src)->next = best->dest;
+	      t2 = end_of_trace[best->src->index];
+	      connected[t2] = true;
+	      if (rtl_dump_file)
+		{
+		  fprintf (rtl_dump_file, "Connection: %d %d\n",
+			   best->src->index, best->dest->index);
+		}
+	    }
+	  else
+	    break;
+	}
 
-	 We should probably work to improve optimization of that
-	 case; however, it seems silly not to also deal with such
-	 problems here if they happen to occur.  */
+      if (last_trace >= 0)
+	RBI (traces[last_trace].last)->next = traces[t2].first;
+      last_trace = t;
 
-      e_taken = e_fall = NULL;
-      for (e = bb->succ; e ; e = e->succ_next)
+      /* Find the successor traces.  */
+      while (1)
 	{
-	  if (e->flags & EDGE_FALLTHRU)
-	    e_fall = e;
-	  else if (! (e->flags & EDGE_EH))
-	    e_taken = e;
+	  /* Find the continuation of the chain.  */
+	  best = NULL;
+	  best_len = 0;
+	  for (e = traces[t].last->succ; e; e = e->succ_next)
+	    {
+	      int di = e->dest->index;
+
+	      if (e->dest != EXIT_BLOCK_PTR
+		  && (e->flags & EDGE_CAN_FALLTHRU)
+		  && !(e->flags & EDGE_COMPLEX)
+		  && start_of_trace[di] >= 0
+		  && !connected[start_of_trace[di]]
+		  && (!best
+		      || e->probability > best->probability
+		      || (e->probability == best->probability
+			  && traces[start_of_trace[di]].length > best_len)))
+		{
+		  best = e;
+		  best_len = traces[start_of_trace[di]].length;
+		}
+	    }
+
+	  if (best)
+	    {
+	      if (rtl_dump_file)
+		{
+		  fprintf (rtl_dump_file, "Connection: %d %d\n",
+			   best->src->index, best->dest->index);
+		}
+	      t = start_of_trace[best->dest->index];
+	      RBI (traces[last_trace].last)->next = traces[t].first;
+	      connected[t] = true;
+	      last_trace = t;
+	    }
+	  else
+	    {
+	      /* Try to connect the traces by duplication of 1 block.  */
+	      edge e2;
+	      basic_block next_bb = NULL;
+
+	      for (e = traces[t].last->succ; e; e = e->succ_next)
+		if (e->dest != EXIT_BLOCK_PTR
+		    && (e->flags & EDGE_CAN_FALLTHRU)
+		    && !(e->flags & EDGE_COMPLEX)
+		    && (EDGE_FREQUENCY (e) >= freq_threshold)
+		    && (e->count >= count_threshold)
+		    && (!best 
+			|| e->probability > best->probability))
+		  {
+		    edge best2 = NULL;
+		    int best2_len = 0;
+
+		    for (e2 = e->dest->succ; e2; e2 = e2->succ_next)
+		      {
+			int di = e2->dest->index;
+
+			if (e2->dest == EXIT_BLOCK_PTR
+			    || ((e2->flags & EDGE_CAN_FALLTHRU)
+				&& !(e2->flags & EDGE_COMPLEX)
+				&& start_of_trace[di] >= 0
+				&& !connected[start_of_trace[di]]
+				&& (EDGE_FREQUENCY (e2) >= freq_threshold)
+				&& (e2->count >= count_threshold)
+				&& (!best2
+				    || e2->probability > best2->probability
+				    || (e2->probability == best2->probability
+					&& traces[start_of_trace[di]].length
+					   > best2_len))))
+			  {
+			    best = e;
+			    best2 = e2;
+			    if (e2->dest != EXIT_BLOCK_PTR)
+			      best2_len = traces[start_of_trace[di]].length;
+			    else
+			      best2_len = INT_MAX;
+			    next_bb = e2->dest;
+			  }
+		      }
+		  }
+	      if (best && next_bb && copy_bb_p (best->dest, !optimize_size))
+		{
+		  basic_block new_bb;
+
+		  if (rtl_dump_file)
+		    {
+		      fprintf (rtl_dump_file, "Connection: %d %d ",
+			       traces[t].last->index, best->dest->index);
+		      if (next_bb == EXIT_BLOCK_PTR)
+			fprintf (rtl_dump_file, "exit\n");
+		      else
+			fprintf (rtl_dump_file, "%d\n", next_bb->index);
+		    }
+
+		  new_bb = copy_bb (best->dest, best, traces[t].last, t);
+		  traces[t].last = new_bb;
+		  if (next_bb != EXIT_BLOCK_PTR)
+		    {
+		      t = start_of_trace[next_bb->index];
+		      RBI (traces[last_trace].last)->next = traces[t].first;
+		      connected[t] = true;
+		      last_trace = t;
+		    }
+		  else
+		    break;	/* Stop finding the successor traces.  */
+		}
+	      else
+		break;	/* Stop finding the successor traces.  */
+	    }
 	}
+    }
 
-      next = ((taken && e_taken) ? e_taken : e_fall)->dest;
+  if (rtl_dump_file)
+    {
+      basic_block bb;
+
+      fprintf (rtl_dump_file, "Final order:\n");
+      for (bb = traces[0].first; bb; bb = RBI (bb)->next)
+	fprintf (rtl_dump_file, "%d ", bb->index);
+      fprintf (rtl_dump_file, "\n");
+      fflush (rtl_dump_file);
     }
 
-  /* If the successor is our artificial computed_jump block, duplicate it.  */
-  else
-    next = maybe_duplicate_computed_goto_succ (bb);
+  FREE (connected);
+  FREE (end_of_trace);
+  FREE (start_of_trace);
+}
 
-  /* In the absence of a prediction, disturb things as little as possible
-     by selecting the old "next" block from the list of successors.  If
-     there had been a fallthru edge, that will be the one.  */
-  /* Note that the fallthru block may not be next any time we eliminate
-     forwarder blocks.  */
-  if (! next)
-    {
-      for (e = bb->succ; e ; e = e->succ_next)
-	if (e->flags & EDGE_FALLTHRU)
-	  {
-	    next = e->dest;
-	    break;
-	  }
-	else if (e->dest == bb->next_bb)
-	  {
-	    if (! (e->flags & (EDGE_ABNORMAL_CALL | EDGE_EH)))
-	      next = e->dest;
-	  }
-    }
-
-  /* Make sure we didn't select a silly next block.  */
-  if (! next || next == EXIT_BLOCK_PTR || RBI (next)->visited)
-    next = NULL;
-
-  /* Recurse on the successors.  Unroll the last call, as the normal
-     case is exactly one or two edges, and we can tail recurse.  */
-  for (e = bb->succ; e; e = e->succ_next)
-    if (e->dest != EXIT_BLOCK_PTR
-	&& ! RBI (e->dest)->visited
-	&& e->dest->succ
-	&& ! (e->flags & (EDGE_ABNORMAL_CALL | EDGE_EH)))
-      {
-	if (next)
-	  {
-	    prev = make_reorder_chain_1 (next, prev);
-	    next = RBI (e->dest)->visited ? NULL : e->dest;
-	  }
-	else
-	  next = e->dest;
-      }
-  if (next)
+/* Return true when BB can and should be copied. CODE_MAY_GROW is true
+   when code size is allowed to grow by duplication.  */
+
+static bool
+copy_bb_p (bb, code_may_grow)
+     basic_block bb;
+     bool code_may_grow;
+{
+  int size = 0;
+  int max_size = uncond_jump_length;
+  rtx insn;
+
+  if (!bb->frequency)
+    return false;
+  if (!bb->pred || !bb->pred->pred_next)
+    return false;
+  if (!cfg_layout_can_duplicate_bb_p (bb))
+    return false;
+
+  if (code_may_grow && maybe_hot_bb_p (bb))
+    max_size *= 8;
+
+  for (insn = bb->head; insn != NEXT_INSN (bb->end);
+       insn = NEXT_INSN (insn))
+    {
+      if (INSN_P (insn))
+	size += get_attr_length (insn);
+    }
+
+  if (size <= max_size)
+    return true;
+
+  if (rtl_dump_file)
     {
-      bb = next;
-      goto restart;
+      fprintf (rtl_dump_file,
+	       "Block %d can't be copied because its size = %d.\n",
+	       bb->index, size);
     }
 
-  return prev;
+  return false;
+}
+
+/* Return the maximum length of unconditional jump instruction.  */
+
+static int
+get_uncond_jump_length ()
+{
+  rtx label, jump;
+  int length;
+
+  label = emit_label_before (gen_label_rtx (), get_insns ());
+  jump = emit_jump_insn (gen_jump (label));
+
+  length = get_attr_length (jump);
+
+  delete_insn (jump);
+  delete_insn (label);
+  return length;
 }
 
 /* Reorder basic blocks.  The main entry point to this file.  */
@@ -305,15 +1074,26 @@ make_reorder_chain_1 (bb, prev)
 void
 reorder_basic_blocks ()
 {
+  int n_traces;
+  struct trace *traces;
+
   if (n_basic_blocks <= 1)
     return;
 
   if ((* targetm.cannot_modify_jumps_p) ())
     return;
 
-  cfg_layout_initialize ();
+  cfg_layout_initialize (NULL);
 
-  make_reorder_chain ();
+  set_edge_can_fallthru_flag ();
+  mark_dfs_back_edges ();
+  uncond_jump_length = get_uncond_jump_length ();
+
+  traces = xmalloc (n_basic_blocks * sizeof (struct trace));
+  n_traces = 0;
+  find_traces (&n_traces, traces);
+  connect_traces (n_traces, traces);
+  FREE (traces);
 
   if (rtl_dump_file)
     dump_flow_info (rtl_dump_file);
--- gcc-3.3.1/gcc/builtins.c.hammer-branch	2003-06-26 00:48:25.000000000 +0200
+++ gcc-3.3.1/gcc/builtins.c	2003-08-05 18:22:46.000000000 +0200
@@ -62,7 +62,7 @@ Software Foundation, 59 Temple Place - S
 const char *const built_in_class_names[4]
   = {"NOT_BUILT_IN", "BUILT_IN_FRONTEND", "BUILT_IN_MD", "BUILT_IN_NORMAL"};
 
-#define DEF_BUILTIN(X, N, C, T, LT, B, F, NA, AT) STRINGX(X),
+#define DEF_BUILTIN(X, N, C, T, LT, B, F, NA, AT, IM) STRINGX(X),
 const char *const built_in_names[(int) END_BUILTINS] =
 {
 #include "builtins.def"
@@ -72,6 +72,10 @@ const char *const built_in_names[(int) E
 /* Setup an array of _DECL trees, make sure each element is
    initialized to NULL_TREE.  */
 tree built_in_decls[(int) END_BUILTINS];
+/* Declarations used when constructing the builtin implicitly in the compiler.
+   It may be NULL_TREE when this is invalid (for instance runtime is not
+   required to implement the function call in all cases.  */
+tree implicit_built_in_decls[(int) END_BUILTINS];
 
 static int get_pointer_alignment	PARAMS ((tree, unsigned int));
 static tree c_strlen			PARAMS ((tree));
@@ -150,8 +154,8 @@ static tree fold_builtin_constant_p	PARA
 static tree fold_builtin_classify_type	PARAMS ((tree));
 static tree fold_builtin_inf		PARAMS ((tree, int));
 static tree fold_builtin_nan		PARAMS ((tree, tree, int));
-static tree build_function_call_expr	PARAMS ((tree, tree));
 static int validate_arglist		PARAMS ((tree, ...));
+static tree fold_trunc_transparent_mathfn PARAMS ((tree));
 
 /* Return the alignment in bits of EXP, a pointer valued expression.
    But don't return more than MAX_ALIGN no matter what.
@@ -1468,6 +1472,170 @@ expand_builtin_constant_p (exp)
   return tmp;
 }
 
+/* Return mathematic function equivalent to FN but operating directly on TYPE,
+   if available.  */
+tree
+mathfn_built_in (type, fn)
+     tree type;
+     enum built_in_function fn;
+{
+  enum built_in_function fcode = NOT_BUILT_IN;
+  if (TYPE_MODE (type) == TYPE_MODE (double_type_node))
+    switch (fn)
+      {
+      case BUILT_IN_SQRT:
+      case BUILT_IN_SQRTF:
+      case BUILT_IN_SQRTL:
+	fcode = BUILT_IN_SQRT;
+	break;
+      case BUILT_IN_SIN:
+      case BUILT_IN_SINF:
+      case BUILT_IN_SINL:
+	fcode = BUILT_IN_SIN;
+	break;
+      case BUILT_IN_COS:
+      case BUILT_IN_COSF:
+      case BUILT_IN_COSL:
+	fcode = BUILT_IN_COS;
+	break;
+      case BUILT_IN_EXP:
+      case BUILT_IN_EXPF:
+      case BUILT_IN_EXPL:
+	fcode = BUILT_IN_EXP;
+	break;
+      case BUILT_IN_FLOOR:
+      case BUILT_IN_FLOORF:
+      case BUILT_IN_FLOORL:
+	fcode = BUILT_IN_FLOOR;
+	break;
+      case BUILT_IN_CEIL:
+      case BUILT_IN_CEILF:
+      case BUILT_IN_CEILL:
+	fcode = BUILT_IN_CEIL;
+	break;
+      case BUILT_IN_TRUNC:
+      case BUILT_IN_TRUNCF:
+      case BUILT_IN_TRUNCL:
+	fcode = BUILT_IN_TRUNC;
+	break;
+      case BUILT_IN_ROUND:
+      case BUILT_IN_ROUNDF:
+      case BUILT_IN_ROUNDL:
+	fcode = BUILT_IN_ROUND;
+	break;
+      case BUILT_IN_NEARBYINT:
+      case BUILT_IN_NEARBYINTF:
+      case BUILT_IN_NEARBYINTL:
+	fcode = BUILT_IN_NEARBYINT;
+	break;
+      default:
+	abort ();
+      }
+  else if (TYPE_MODE (type) == TYPE_MODE (float_type_node))
+    switch (fn)
+      {
+      case BUILT_IN_SQRT:
+      case BUILT_IN_SQRTF:
+      case BUILT_IN_SQRTL:
+	fcode = BUILT_IN_SQRTF;
+	break;
+      case BUILT_IN_SIN:
+      case BUILT_IN_SINF:
+      case BUILT_IN_SINL:
+	fcode = BUILT_IN_SINF;
+	break;
+      case BUILT_IN_COS:
+      case BUILT_IN_COSF:
+      case BUILT_IN_COSL:
+	fcode = BUILT_IN_COSF;
+	break;
+      case BUILT_IN_EXP:
+      case BUILT_IN_EXPF:
+      case BUILT_IN_EXPL:
+	fcode = BUILT_IN_EXPF;
+	break;
+      case BUILT_IN_FLOOR:
+      case BUILT_IN_FLOORF:
+      case BUILT_IN_FLOORL:
+	fcode = BUILT_IN_FLOORF;
+	break;
+      case BUILT_IN_CEIL:
+      case BUILT_IN_CEILF:
+      case BUILT_IN_CEILL:
+	fcode = BUILT_IN_CEILF;
+	break;
+      case BUILT_IN_TRUNC:
+      case BUILT_IN_TRUNCF:
+      case BUILT_IN_TRUNCL:
+	fcode = BUILT_IN_TRUNCF;
+	break;
+      case BUILT_IN_ROUND:
+      case BUILT_IN_ROUNDF:
+      case BUILT_IN_ROUNDL:
+	fcode = BUILT_IN_ROUNDF;
+	break;
+      case BUILT_IN_NEARBYINT:
+      case BUILT_IN_NEARBYINTF:
+      case BUILT_IN_NEARBYINTL:
+	fcode = BUILT_IN_NEARBYINTF;
+	break;
+      default:
+	abort ();
+      }
+  else if (TYPE_MODE (type) == TYPE_MODE (long_double_type_node))
+    switch (fn)
+      {
+      case BUILT_IN_SQRT:
+      case BUILT_IN_SQRTF:
+      case BUILT_IN_SQRTL:
+	fcode = BUILT_IN_SQRTL;
+	break;
+      case BUILT_IN_SIN:
+      case BUILT_IN_SINF:
+      case BUILT_IN_SINL:
+	fcode = BUILT_IN_SINL;
+	break;
+      case BUILT_IN_COS:
+      case BUILT_IN_COSF:
+      case BUILT_IN_COSL:
+	fcode = BUILT_IN_COSL;
+	break;
+      case BUILT_IN_EXP:
+      case BUILT_IN_EXPF:
+      case BUILT_IN_EXPL:
+	fcode = BUILT_IN_EXPL;
+	break;
+      case BUILT_IN_FLOOR:
+      case BUILT_IN_FLOORF:
+      case BUILT_IN_FLOORL:
+	fcode = BUILT_IN_FLOORL;
+	break;
+      case BUILT_IN_CEIL:
+      case BUILT_IN_CEILF:
+      case BUILT_IN_CEILL:
+	fcode = BUILT_IN_CEILL;
+	break;
+      case BUILT_IN_TRUNC:
+      case BUILT_IN_TRUNCF:
+      case BUILT_IN_TRUNCL:
+	fcode = BUILT_IN_TRUNCL;
+	break;
+      case BUILT_IN_ROUND:
+      case BUILT_IN_ROUNDF:
+      case BUILT_IN_ROUNDL:
+	fcode = BUILT_IN_ROUNDL;
+	break;
+      case BUILT_IN_NEARBYINT:
+      case BUILT_IN_NEARBYINTF:
+      case BUILT_IN_NEARBYINTL:
+	fcode = BUILT_IN_NEARBYINTL;
+	break;
+      default:
+	abort ();
+      }
+  return implicit_built_in_decls[fcode];
+}
+
 /* Expand a call to one of the builtin math functions (sin, cos, or sqrt).
    Return 0 if a normal call should be emitted rather than expanding the
    function in-line.  EXP is the expression that is a call to the builtin
@@ -1484,6 +1652,7 @@ expand_builtin_mathfn (exp, target, subt
   tree fndecl = TREE_OPERAND (TREE_OPERAND (exp, 0), 0);
   tree arglist = TREE_OPERAND (exp, 1);
   enum machine_mode argmode;
+  bool errno_set = true;
 
   if (!validate_arglist (arglist, REAL_TYPE, VOID_TYPE))
     return 0;
@@ -1534,6 +1703,26 @@ expand_builtin_mathfn (exp, target, subt
     case BUILT_IN_LOGF:
     case BUILT_IN_LOGL:
       builtin_optab = log_optab; break;
+    case BUILT_IN_FLOOR:
+    case BUILT_IN_FLOORF:
+    case BUILT_IN_FLOORL:
+      errno_set = false ; builtin_optab = floor_optab; break;
+    case BUILT_IN_CEIL:
+    case BUILT_IN_CEILF:
+    case BUILT_IN_CEILL:
+      errno_set = false ; builtin_optab = ceil_optab; break;
+    case BUILT_IN_TRUNC:
+    case BUILT_IN_TRUNCF:
+    case BUILT_IN_TRUNCL:
+      errno_set = false ; builtin_optab = trunc_optab; break;
+    case BUILT_IN_ROUND:
+    case BUILT_IN_ROUNDF:
+    case BUILT_IN_ROUNDL:
+      errno_set = false ; builtin_optab = round_optab; break;
+    case BUILT_IN_NEARBYINT:
+    case BUILT_IN_NEARBYINTF:
+    case BUILT_IN_NEARBYINTL:
+      errno_set = false ; builtin_optab = nearbyint_optab; break;
     default:
       abort ();
     }
@@ -1554,7 +1743,7 @@ expand_builtin_mathfn (exp, target, subt
 
   /* If errno must be maintained, we must set it to EDOM for NaN results.  */
 
-  if (flag_errno_math && HONOR_NANS (argmode))
+  if (flag_errno_math && errno_set && HONOR_NANS (argmode))
     {
       rtx lab1;
 
@@ -1734,7 +1923,7 @@ expand_builtin_strstr (arglist, target, 
       if (p2[1] != '\0')
 	return 0;
 
-      fn = built_in_decls[BUILT_IN_STRCHR];
+      fn = implicit_built_in_decls[BUILT_IN_STRCHR];
       if (!fn)
 	return 0;
 
@@ -1838,7 +2027,7 @@ expand_builtin_strrchr (arglist, target,
       if (! integer_zerop (s2))
 	return 0;
 
-      fn = built_in_decls[BUILT_IN_STRCHR];
+      fn = implicit_built_in_decls[BUILT_IN_STRCHR];
       if (!fn)
 	return 0;
 
@@ -1896,7 +2085,7 @@ expand_builtin_strpbrk (arglist, target,
       if (p2[1] != '\0')
 	return 0;  /* Really call strpbrk.  */
 
-      fn = built_in_decls[BUILT_IN_STRCHR];
+      fn = implicit_built_in_decls[BUILT_IN_STRCHR];
       if (!fn)
 	return 0;
 
@@ -2035,7 +2224,7 @@ expand_builtin_strcpy (exp, target, mode
   if (!validate_arglist (arglist, POINTER_TYPE, POINTER_TYPE, VOID_TYPE))
     return 0;
 
-  fn = built_in_decls[BUILT_IN_MEMCPY];
+  fn = implicit_built_in_decls[BUILT_IN_MEMCPY];
   if (!fn)
     return 0;
 
@@ -2141,7 +2330,7 @@ expand_builtin_strncpy (arglist, target,
 	}
 
       /* OK transform into builtin memcpy.  */
-      fn = built_in_decls[BUILT_IN_MEMCPY];
+      fn = implicit_built_in_decls[BUILT_IN_MEMCPY];
       if (!fn)
 	return 0;
       return expand_expr (build_function_call_expr (fn, arglist),
@@ -2557,7 +2746,7 @@ expand_builtin_strcmp (exp, target, mode
   if (TREE_SIDE_EFFECTS (len))
     return 0;
 
-  fn = built_in_decls[BUILT_IN_MEMCMP];
+  fn = implicit_built_in_decls[BUILT_IN_MEMCMP];
   if (!fn)
     return 0;
 
@@ -2653,7 +2842,7 @@ expand_builtin_strncmp (exp, target, mod
   if (!len)
     return 0;
 
-  fn = built_in_decls[BUILT_IN_MEMCMP];
+  fn = implicit_built_in_decls[BUILT_IN_MEMCMP];
   if (!fn)
     return 0;
 
@@ -2734,7 +2923,7 @@ expand_builtin_strncat (arglist, target,
 	{
 	  tree newarglist
 	    = tree_cons (NULL_TREE, dst, build_tree_list (NULL_TREE, src));
-	  tree fn = built_in_decls[BUILT_IN_STRCAT];
+	  tree fn = implicit_built_in_decls[BUILT_IN_STRCAT];
 
 	  /* If the replacement _DECL isn't initialized, don't do the
 	     transformation.  */
@@ -2822,7 +3011,7 @@ expand_builtin_strcspn (arglist, target,
       if (p2 && *p2 == '\0')
 	{
 	  tree newarglist = build_tree_list (NULL_TREE, s1),
-	    fn = built_in_decls[BUILT_IN_STRLEN];
+	    fn = implicit_built_in_decls[BUILT_IN_STRLEN];
 
 	  /* If the replacement _DECL isn't initialized, don't do the
 	     transformation.  */
@@ -3429,10 +3618,10 @@ expand_builtin_fputs (arglist, ignore, u
      int unlocked;
 {
   tree len, fn;
-  tree fn_fputc = unlocked ? built_in_decls[BUILT_IN_FPUTC_UNLOCKED]
-    : built_in_decls[BUILT_IN_FPUTC];
-  tree fn_fwrite = unlocked ? built_in_decls[BUILT_IN_FWRITE_UNLOCKED]
-    : built_in_decls[BUILT_IN_FWRITE];
+  tree fn_fputc = unlocked ? implicit_built_in_decls[BUILT_IN_FPUTC_UNLOCKED]
+    : implicit_built_in_decls[BUILT_IN_FPUTC];
+  tree fn_fwrite = unlocked ? implicit_built_in_decls[BUILT_IN_FWRITE_UNLOCKED]
+    : implicit_built_in_decls[BUILT_IN_FWRITE];
 
   /* If the return value is used, or the replacement _DECL isn't
      initialized, don't do the transformation.  */
@@ -3757,6 +3946,21 @@ expand_builtin (exp, target, subtarget, 
       case BUILT_IN_FPUTC_UNLOCKED:
       case BUILT_IN_FPUTS_UNLOCKED:
       case BUILT_IN_FWRITE_UNLOCKED:
+      case BUILT_IN_FLOOR:
+      case BUILT_IN_FLOORF:
+      case BUILT_IN_FLOORL:
+      case BUILT_IN_CEIL:
+      case BUILT_IN_CEILF:
+      case BUILT_IN_CEILL:
+      case BUILT_IN_TRUNC:
+      case BUILT_IN_TRUNCF:
+      case BUILT_IN_TRUNCL:
+      case BUILT_IN_ROUND:
+      case BUILT_IN_ROUNDF:
+      case BUILT_IN_ROUNDL:
+      case BUILT_IN_NEARBYINT:
+      case BUILT_IN_NEARBYINTF:
+      case BUILT_IN_NEARBYINTL:
 	return expand_call (exp, target, ignore);
 
       default:
@@ -3807,6 +4011,21 @@ expand_builtin (exp, target, subtarget, 
     case BUILT_IN_SQRT:
     case BUILT_IN_SQRTF:
     case BUILT_IN_SQRTL:
+    case BUILT_IN_FLOOR:
+    case BUILT_IN_FLOORF:
+    case BUILT_IN_FLOORL:
+    case BUILT_IN_CEIL:
+    case BUILT_IN_CEILF:
+    case BUILT_IN_CEILL:
+    case BUILT_IN_TRUNC:
+    case BUILT_IN_TRUNCF:
+    case BUILT_IN_TRUNCL:
+    case BUILT_IN_ROUND:
+    case BUILT_IN_ROUNDF:
+    case BUILT_IN_ROUNDL:
+    case BUILT_IN_NEARBYINT:
+    case BUILT_IN_NEARBYINTF:
+    case BUILT_IN_NEARBYINTL:
       target = expand_builtin_mathfn (exp, target, subtarget);
       if (target)
 	return target;
@@ -4093,6 +4312,37 @@ expand_builtin (exp, target, subtarget, 
   return expand_call (exp, target, ignore);
 }
 
+/* Determine whether a tree node represents a call to a built-in
+   math function.  If the tree T is a call to a built-in function
+   taking a single real argument, then the return value is the
+   DECL_FUNCTION_CODE of the call, e.g. BUILT_IN_SQRT.  Otherwise
+   the return value is END_BUILTINS.  */
+   
+enum built_in_function
+builtin_mathfn_code (t)
+     tree t;
+{
+  tree fndecl, arglist;
+
+  if (TREE_CODE (t) != CALL_EXPR
+      || TREE_CODE (TREE_OPERAND (t, 0)) != ADDR_EXPR)
+    return END_BUILTINS;
+
+  fndecl = TREE_OPERAND (TREE_OPERAND (t, 0), 0);
+  if (TREE_CODE (fndecl) != FUNCTION_DECL
+      || ! DECL_BUILT_IN (fndecl)
+      || DECL_BUILT_IN_CLASS (fndecl) == BUILT_IN_MD)
+    return END_BUILTINS;
+
+  arglist = TREE_OPERAND (t, 1);
+  if (! arglist
+      || TREE_CODE (TREE_TYPE (TREE_VALUE (arglist))) != REAL_TYPE
+      || TREE_CHAIN (arglist))
+    return END_BUILTINS;
+
+  return DECL_FUNCTION_CODE (fndecl);
+}
+
 /* Fold a call to __builtin_constant_p, if we know it will evaluate to a
    constant.  ARGLIST is the argument list of the call.  */
 
@@ -4184,6 +4434,32 @@ fold_builtin_nan (arglist, type, quiet)
 
   return build_real (type, real);
 }
+static tree
+fold_trunc_transparent_mathfn (exp)
+     tree exp;
+{
+  tree fndecl = TREE_OPERAND (TREE_OPERAND (exp, 0), 0);
+  tree arglist = TREE_OPERAND (exp, 1);
+  enum built_in_function fcode = DECL_FUNCTION_CODE (fndecl);
+
+  if (optimize && validate_arglist (arglist, REAL_TYPE, VOID_TYPE))
+    {
+      tree arg0 = strip_float_extensions (TREE_VALUE (arglist));
+      tree ftype = TREE_TYPE (exp);
+      tree newtype = TREE_TYPE (arg0);
+      tree decl;
+
+      if (TYPE_PRECISION (newtype) < TYPE_PRECISION (ftype)
+	  && (decl = mathfn_built_in (newtype, fcode)))
+	{
+	  arglist =
+	    build_tree_list (NULL_TREE, fold (convert (newtype, arg0)));
+	  return convert (ftype,
+			  build_function_call_expr (decl, arglist));
+	}
+    }
+  return 0;
+}
 
 /* Used by constant folding to eliminate some builtin calls early.  EXP is
    the CALL_EXPR of a call to a builtin function.  */
@@ -4221,6 +4497,105 @@ fold_builtin (exp)
 	}
       break;
 
+    case BUILT_IN_SQRT:
+    case BUILT_IN_SQRTF:
+    case BUILT_IN_SQRTL:
+      if (validate_arglist (arglist, REAL_TYPE, VOID_TYPE))
+	{
+	  enum built_in_function fcode;
+	  tree arg = TREE_VALUE (arglist);
+
+	  /* Optimize sqrt of constant value.  */
+#if 0
+	  if (TREE_CODE (arg) == REAL_CST
+	      && ! TREE_CONSTANT_OVERFLOW (arg))
+	    {
+	      enum machine_mode mode;
+	      REAL_VALUE_TYPE r, x;
+
+	      x = TREE_REAL_CST (arg);
+	      mode = TYPE_MODE (TREE_TYPE (arg));
+	      if (!HONOR_SNANS (mode) || !real_isnan (&x))
+	      {
+		real_sqrt (&r, mode, &x);
+		return build_real (TREE_TYPE (arg), r);
+	      }
+	    }
+#endif
+
+	  /* Optimize sqrt(exp(x)) = exp(x/2.0).  */
+	  fcode = builtin_mathfn_code (arg);
+	  if (flag_unsafe_math_optimizations
+	      && (fcode == BUILT_IN_EXP
+		  || fcode == BUILT_IN_EXPF
+		  || fcode == BUILT_IN_EXPL))
+	    {
+	      tree expfn = TREE_OPERAND (TREE_OPERAND (arg, 0), 0);
+	      arg = build (RDIV_EXPR, TREE_TYPE (arg),
+			   TREE_VALUE (TREE_OPERAND (arg, 1)),
+			   build_real (TREE_TYPE (arg), dconst2));
+	      arglist = build_tree_list (NULL_TREE, arg);
+	      return build_function_call_expr (expfn, arglist);
+	    }
+	}
+      break;
+
+    case BUILT_IN_EXP:
+    case BUILT_IN_EXPF:
+    case BUILT_IN_EXPL:
+      if (validate_arglist (arglist, REAL_TYPE, VOID_TYPE))
+	{
+	  enum built_in_function fcode;
+	  tree arg = TREE_VALUE (arglist);
+
+	  /* Optimize exp(0.0) = 1.0.  */
+	  if (real_zerop (arg))
+	    return build_real (TREE_TYPE (arg), dconst1);
+
+	  /* Optimize exp(log(x)) = x.  */
+	  fcode = builtin_mathfn_code (arg);
+	  if (flag_unsafe_math_optimizations
+	      && (fcode == BUILT_IN_LOG
+		  || fcode == BUILT_IN_LOGF
+		  || fcode == BUILT_IN_LOGL))
+	    return TREE_VALUE (TREE_OPERAND (arg, 1));
+	}
+      break;
+
+    case BUILT_IN_LOG:
+    case BUILT_IN_LOGF:
+    case BUILT_IN_LOGL:
+      if (validate_arglist (arglist, REAL_TYPE, VOID_TYPE))
+	{
+	  enum built_in_function fcode;
+	  tree arg = TREE_VALUE (arglist);
+
+	  /* Optimize log(1.0) = 0.0.  */
+	  if (real_onep (arg))
+	    return build_real (TREE_TYPE (arg), dconst0);
+
+	  /* Optimize log(exp(x)) = x.  */
+	  fcode = builtin_mathfn_code (arg);
+	  if (flag_unsafe_math_optimizations
+	      && (fcode == BUILT_IN_EXP
+		  || fcode == BUILT_IN_EXPF
+		  || fcode == BUILT_IN_EXPL))
+	    return TREE_VALUE (TREE_OPERAND (arg, 1));
+
+	  /* Optimize log(sqrt(x)) = log(x)/2.0.  */
+	  if (flag_unsafe_math_optimizations
+	      && (fcode == BUILT_IN_SQRT
+		  || fcode == BUILT_IN_SQRTF
+		  || fcode == BUILT_IN_SQRTL))
+	    {
+	      tree logfn = build_function_call_expr (fndecl,
+						     TREE_OPERAND (arg, 1));
+	      return fold (build (RDIV_EXPR, TREE_TYPE (arg), logfn,
+				  build_real (TREE_TYPE (arg), dconst2)));
+	    }
+	}
+      break;
+
     case BUILT_IN_INF:
     case BUILT_IN_INFF:
     case BUILT_IN_INFL:
@@ -4241,6 +4616,23 @@ fold_builtin (exp)
     case BUILT_IN_NANSL:
       return fold_builtin_nan (arglist, TREE_TYPE (TREE_TYPE (fndecl)), false);
 
+    case BUILT_IN_FLOOR:
+    case BUILT_IN_FLOORF:
+    case BUILT_IN_FLOORL:
+    case BUILT_IN_CEIL:
+    case BUILT_IN_CEILF:
+    case BUILT_IN_CEILL:
+    case BUILT_IN_TRUNC:
+    case BUILT_IN_TRUNCF:
+    case BUILT_IN_TRUNCL:
+    case BUILT_IN_ROUND:
+    case BUILT_IN_ROUNDF:
+    case BUILT_IN_ROUNDL:
+    case BUILT_IN_NEARBYINT:
+    case BUILT_IN_NEARBYINTF:
+    case BUILT_IN_NEARBYINTL:
+      return fold_trunc_transparent_mathfn (exp);
+
     default:
       break;
     }
@@ -4248,7 +4640,9 @@ fold_builtin (exp)
   return 0;
 }
 
-static tree
+/* Conveniently construct a function call expression.  */
+
+tree
 build_function_call_expr (fn, arglist)
      tree fn, arglist;
 {
--- gcc-3.3.1/gcc/builtins.def.hammer-branch	2003-05-05 18:59:19.000000000 +0200
+++ gcc-3.3.1/gcc/builtins.def	2003-08-05 18:22:46.000000000 +0200
@@ -22,7 +22,7 @@ Software Foundation, 59 Temple Place - S
 /* Before including this file, you should define a macro:
 
      DEF_BUILTIN (ENUM, NAME, CLASS, TYPE, LIBTYPE, BOTH_P,
-                  FALLBACK_P, NONANSI_P, ATTRS)
+                  FALLBACK_P, NONANSI_P, ATTRS, IMPLICIT)
 
    This macro will be called once for each builtin function.  The
    ENUM will be of type `enum built_in_function', and will indicate
@@ -53,7 +53,13 @@ Software Foundation, 59 Temple Place - S
    exist when compiling in ANSI conformant mode.
 
    ATTRs is an attribute list as defined in builtin-attrs.def that
-   describes the attributes of this builtin function.  */
+   describes the attributes of this builtin function.  
+
+   IMPLICIT specifies condition when the builtin can be produced by
+   compiler.  For instance C90 reserves floorf function, but does not
+   define it's meaning.  When user uses floorf we may assume that the
+   floorf has the meaning we expect, but we can't produce floorf by
+   simplifing floor((double)float) since runtime don't need to implement it.  */
    
 /* A GCC builtin (like __builtin_saveregs) is provided by the
    compiler, but does not correspond to a function in the standard
@@ -61,7 +67,7 @@ Software Foundation, 59 Temple Place - S
 #undef DEF_GCC_BUILTIN
 #define DEF_GCC_BUILTIN(ENUM, NAME, TYPE, ATTRS)		\
   DEF_BUILTIN (ENUM, NAME, BUILT_IN_NORMAL, TYPE, BT_LAST,	\
-               false, false, false, ATTRS)
+               false, false, false, ATTRS, true)
 
 
 /* A fallback builtin is a builtin (like __builtin_puts) that falls
@@ -71,7 +77,7 @@ Software Foundation, 59 Temple Place - S
 #undef DEF_FALLBACK_BUILTIN				
 #define DEF_FALLBACK_BUILTIN(ENUM, NAME, TYPE, ATTRS)	\
   DEF_BUILTIN (ENUM, NAME, BUILT_IN_NORMAL, TYPE, TYPE,	\
-	       false, true, false, ATTRS)
+	       false, true, false, ATTRS, true)
 
 /* Like DEF_FALLBACK_BUILTIN, except that the function is not one that
    is specified by ANSI/ISO C.  So, when we're being fully conformant
@@ -80,7 +86,7 @@ Software Foundation, 59 Temple Place - S
 #undef DEF_EXT_FALLBACK_BUILTIN
 #define DEF_EXT_FALLBACK_BUILTIN(ENUM, NAME, TYPE)	\
   DEF_BUILTIN (ENUM, NAME, BUILT_IN_NORMAL, TYPE, TYPE,	\
-	       false, true, true, ATTR_NOTHROW_LIST)
+	       false, true, true, ATTR_NOTHROW_LIST, true)
 
 /* A library builtin (like __builtin_strchr) is a builtin equivalent
    of an ANSI/ISO standard library function.  In addition to the
@@ -91,14 +97,14 @@ Software Foundation, 59 Temple Place - S
 #undef DEF_LIB_BUILTIN					
 #define DEF_LIB_BUILTIN(ENUM, NAME, TYPE, ATTRS)	\
   DEF_BUILTIN (ENUM, NAME, BUILT_IN_NORMAL, TYPE, TYPE,	\
-	       true, true, false, ATTRS)
+	       true, true, false, ATTRS, true)
 
 /* Like DEF_LIB_BUILTIN, except that a call to the builtin should
    never fall back to the library version.  */
 #undef DEF_LIB_ALWAYS_BUILTIN				
 #define DEF_LIB_ALWAYS_BUILTIN(ENUM, NAME, TYPE)	\
   DEF_BUILTIN (ENUM, NAME, BUILT_IN_NORMAL, TYPE, TYPE,	\
-    	       true, false, true, ATTR_CONST_NOTHROW_LIST)
+    	       true, false, true, ATTR_CONST_NOTHROW_LIST, true)
 
 /* Like DEF_LIB_BUILTIN, except that the function is not one that is
    specified by ANSI/ISO C.  So, when we're being fully conformant we
@@ -107,21 +113,29 @@ Software Foundation, 59 Temple Place - S
 #undef DEF_EXT_LIB_BUILTIN				
 #define DEF_EXT_LIB_BUILTIN(ENUM, NAME, TYPE, ATTRS)	\
   DEF_BUILTIN (ENUM, NAME, BUILT_IN_NORMAL, TYPE, TYPE,	\
-   	       true, true, true, ATTRS)
+   	       true, true, true, ATTRS, false)
 
 /* Like DEF_LIB_BUILTIN, except that the function is only a part of
    the standard in C99 or above.  */
 #undef DEF_C99_BUILTIN					
 #define DEF_C99_BUILTIN(ENUM, NAME, TYPE, ATTRS)	\
   DEF_BUILTIN (ENUM, NAME, BUILT_IN_NORMAL, TYPE, TYPE,	\
-   	       true, true, !flag_isoc99, ATTRS)
+   	       true, true, !flag_isoc99, ATTRS, TARGET_C99_FUNCTIONS)
+
+/* Builtin that is specified by C99 and C90 reserve the name for future use.
+   We can still recognize the builtin in C90 mode but we can't produce it
+   implicitly.  */
+#undef DEF_C99_C90RES_BUILTIN					
+#define DEF_C99_C90RES_BUILTIN(ENUM, NAME, TYPE, ATTRS)	\
+  DEF_BUILTIN (ENUM, NAME, BUILT_IN_NORMAL, TYPE, TYPE,	\
+   	       true, true, !flag_isoc99, ATTRS, TARGET_C99_FUNCTIONS)
 
 /* Like DEF_LIB_BUILTIN, except that the function is expanded in the
    front-end.  */
 #undef DEF_FRONT_END_LIB_BUILTIN			
 #define DEF_FRONT_END_LIB_BUILTIN(ENUM, NAME, TYPE, ATTRS)	\
   DEF_BUILTIN (ENUM, NAME, BUILT_IN_FRONTEND, TYPE, TYPE,	\
-	       true, true, false, ATTRS)
+	       true, true, false, ATTRS, true)
 
 /* Like DEF_FRONT_END_LIB_BUILTIN, except that the function is not one
    that is specified by ANSI/ISO C.  So, when we're being fully
@@ -130,13 +144,13 @@ Software Foundation, 59 Temple Place - S
 #undef DEF_EXT_FRONT_END_LIB_BUILTIN			
 #define DEF_EXT_FRONT_END_LIB_BUILTIN(ENUM, NAME, TYPE, ATTRS)	\
   DEF_BUILTIN (ENUM, NAME, BUILT_IN_FRONTEND, TYPE, TYPE,	\
-	       true, true, true, ATTRS)
+	       true, true, true, ATTRS, true)
 
 /* A built-in that is not currently used.  */
 #undef DEF_UNUSED_BUILTIN					
 #define DEF_UNUSED_BUILTIN(X)					\
   DEF_BUILTIN (X, (const char *) NULL, NOT_BUILT_IN, BT_LAST,	\
-	       BT_LAST, false, false, false, ATTR_NOTHROW_LIST)
+	       BT_LAST, false, false, false, ATTR_NOTHROW_LIST, false)
 
 /* If SMALL_STACK is defined, then `alloca' is only defined in its
    `__builtin' form.  */
@@ -169,50 +183,115 @@ DEF_LIB_ALWAYS_BUILTIN(BUILT_IN_FABSL,
 		       "__builtin_fabsl",
 		       BT_FN_LONG_DOUBLE_LONG_DOUBLE)
 
+DEF_LIB_BUILTIN(BUILT_IN_FLOOR,
+                "__builtin_floor",
+                BT_FN_DOUBLE_DOUBLE,
+		ATTR_CONST_NOTHROW_LIST)
+DEF_C99_C90RES_BUILTIN(BUILT_IN_FLOORF,
+		       "__builtin_floorf",
+		       BT_FN_FLOAT_FLOAT,
+		       ATTR_CONST_NOTHROW_LIST)
+DEF_C99_C90RES_BUILTIN(BUILT_IN_FLOORL,
+		       "__builtin_floorl",
+		       BT_FN_LONG_DOUBLE_LONG_DOUBLE,
+		       ATTR_CONST_NOTHROW_LIST)
+
+DEF_LIB_BUILTIN(BUILT_IN_CEIL,
+                "__builtin_ceil",
+                BT_FN_DOUBLE_DOUBLE,
+		ATTR_CONST_NOTHROW_LIST)
+DEF_C99_C90RES_BUILTIN(BUILT_IN_CEILF,
+		       "__builtin_ceilf",
+		       BT_FN_FLOAT_FLOAT,
+		       ATTR_CONST_NOTHROW_LIST)
+DEF_C99_C90RES_BUILTIN(BUILT_IN_CEILL,
+		       "__builtin_ceill",
+		       BT_FN_LONG_DOUBLE_LONG_DOUBLE,
+		       ATTR_CONST_NOTHROW_LIST)
+
+DEF_C99_BUILTIN(BUILT_IN_ROUND,
+		"__builtin_round",
+		BT_FN_DOUBLE_DOUBLE,
+		ATTR_CONST_NOTHROW_LIST)
+DEF_C99_BUILTIN(BUILT_IN_ROUNDF,
+		"__builtin_roundf",
+		BT_FN_FLOAT_FLOAT,
+		ATTR_CONST_NOTHROW_LIST)
+DEF_C99_BUILTIN(BUILT_IN_ROUNDL,
+		"__builtin_roundl",
+		BT_FN_LONG_DOUBLE_LONG_DOUBLE,
+		ATTR_CONST_NOTHROW_LIST)
+
+DEF_C99_BUILTIN(BUILT_IN_TRUNC,
+		"__builtin_trunc",
+		BT_FN_DOUBLE_DOUBLE,
+		ATTR_CONST_NOTHROW_LIST)
+DEF_C99_BUILTIN(BUILT_IN_TRUNCF,
+		"__builtin_truncf",
+		BT_FN_FLOAT_FLOAT,
+		ATTR_CONST_NOTHROW_LIST)
+DEF_C99_BUILTIN(BUILT_IN_TRUNCL,
+		"__builtin_truncl",
+		BT_FN_LONG_DOUBLE_LONG_DOUBLE,
+		ATTR_CONST_NOTHROW_LIST)
+
+DEF_C99_BUILTIN(BUILT_IN_NEARBYINT,
+		"__builtin_nearbyint",
+		BT_FN_DOUBLE_DOUBLE,
+		ATTR_CONST_NOTHROW_LIST)
+DEF_C99_BUILTIN(BUILT_IN_NEARBYINTF,
+		"__builtin_nearbyintf",
+		BT_FN_FLOAT_FLOAT,
+		ATTR_CONST_NOTHROW_LIST)
+DEF_C99_BUILTIN(BUILT_IN_NEARBYINTL,
+		"__builtin_nearbyintl",
+		BT_FN_LONG_DOUBLE_LONG_DOUBLE,
+		ATTR_CONST_NOTHROW_LIST)
+
 DEF_C99_BUILTIN(BUILT_IN_LLABS,
 		"__builtin_llabs",
 		BT_FN_LONGLONG_LONGLONG,
-		ATTR_NOTHROW_LIST)
+		ATTR_CONST_NOTHROW_LIST)
 DEF_C99_BUILTIN(BUILT_IN_IMAXABS,
 		"__builtin_imaxabs",
 		BT_FN_INTMAX_INTMAX,
-		ATTR_NOTHROW_LIST)
+		ATTR_CONST_NOTHROW_LIST)
 DEF_C99_BUILTIN(BUILT_IN_CONJ,
 		"__builtin_conj",
 		BT_FN_COMPLEX_DOUBLE_COMPLEX_DOUBLE,
-		ATTR_NOTHROW_LIST)
+		ATTR_CONST_NOTHROW_LIST)
 DEF_C99_BUILTIN(BUILT_IN_CONJF,
 		"__builtin_conjf",
 		BT_FN_COMPLEX_FLOAT_COMPLEX_FLOAT,
-		ATTR_NOTHROW_LIST)
+		ATTR_CONST_NOTHROW_LIST)
 DEF_C99_BUILTIN(BUILT_IN_CONJL,
 		"__builtin_conjl",
 		BT_FN_COMPLEX_LONG_DOUBLE_COMPLEX_LONG_DOUBLE,
-		ATTR_NOTHROW_LIST)
+		ATTR_CONST_NOTHROW_LIST)
 DEF_C99_BUILTIN(BUILT_IN_CREAL,
 		"__builtin_creal",
 		BT_FN_DOUBLE_COMPLEX_DOUBLE,
-		ATTR_NOTHROW_LIST)
+		ATTR_CONST_NOTHROW_LIST)
 DEF_C99_BUILTIN(BUILT_IN_CREALF,
 		"__builtin_crealf",
 		BT_FN_FLOAT_COMPLEX_FLOAT,
-		ATTR_NOTHROW_LIST)
+		ATTR_CONST_NOTHROW_LIST)
 DEF_C99_BUILTIN(BUILT_IN_CREALL,
 		"__builtin_creall",
 		BT_FN_LONG_DOUBLE_COMPLEX_LONG_DOUBLE,
-		ATTR_NOTHROW_LIST)
+		ATTR_CONST_NOTHROW_LIST)
 DEF_C99_BUILTIN(BUILT_IN_CIMAG,
 		"__builtin_cimag",
 		BT_FN_DOUBLE_COMPLEX_DOUBLE,
-		ATTR_NOTHROW_LIST)
+		ATTR_CONST_NOTHROW_LIST)
 DEF_C99_BUILTIN(BUILT_IN_CIMAGF,
 		"__builtin_cimagf",
 		BT_FN_FLOAT_COMPLEX_FLOAT,
-		ATTR_NOTHROW_LIST)
+		ATTR_CONST_NOTHROW_LIST)
 DEF_C99_BUILTIN(BUILT_IN_CIMAGL,
 		"__builtin_cimagl",
 		BT_FN_LONG_DOUBLE_COMPLEX_LONG_DOUBLE,
-		ATTR_NOTHROW_LIST)
+		ATTR_CONST_NOTHROW_LIST)
 
 DEF_UNUSED_BUILTIN(BUILT_IN_DIV)
 DEF_UNUSED_BUILTIN(BUILT_IN_LDIV)
@@ -230,14 +309,14 @@ DEF_BUILTIN (BUILT_IN_BZERO,
 	     BT_FN_VOID_PTR_SIZE, 
 	     BT_FN_VOID_VAR,
 	     true, true, true,
-	     ATTR_NOTHROW_LIST)
+	     ATTR_NOTHROW_LIST, false)
 DEF_BUILTIN (BUILT_IN_BCMP,
 	     "__builtin_bcmp",
 	     BUILT_IN_NORMAL,
 	     BT_FN_INT_CONST_PTR_CONST_PTR_SIZE,
 	     BT_FN_INT_VAR,
 	     true, true, true,
-	     ATTR_PURE_NOTHROW_LIST)
+	     ATTR_PURE_NOTHROW_LIST, false)
 
 DEF_EXT_LIB_BUILTIN(BUILT_IN_FFS,
 		    "__builtin_ffs",
@@ -349,68 +428,68 @@ DEF_LIB_BUILTIN(BUILT_IN_LOG,
 				: (flag_unsafe_math_optimizations
 				   ? ATTR_CONST_NOTHROW_LIST
 				   : ATTR_PURE_NOTHROW_LIST))
-DEF_LIB_BUILTIN(BUILT_IN_SQRTF,
-		"__builtin_sqrtf",
-		BT_FN_FLOAT_FLOAT,
-		flag_errno_math ? ATTR_NOTHROW_LIST
-				: (flag_unsafe_math_optimizations
-				   ? ATTR_CONST_NOTHROW_LIST
-				   : ATTR_PURE_NOTHROW_LIST))
-DEF_LIB_BUILTIN(BUILT_IN_SINF,
-		"__builtin_sinf",
-		BT_FN_FLOAT_FLOAT,
-		flag_unsafe_math_optimizations ? ATTR_CONST_NOTHROW_LIST
-					       : ATTR_PURE_NOTHROW_LIST)
-DEF_LIB_BUILTIN(BUILT_IN_COSF,
-		"__builtin_cosf",
-		BT_FN_FLOAT_FLOAT,
-		flag_unsafe_math_optimizations ? ATTR_CONST_NOTHROW_LIST
-					       : ATTR_PURE_NOTHROW_LIST)
-DEF_LIB_BUILTIN(BUILT_IN_EXPF,
-		"__builtin_expf",
-		BT_FN_FLOAT_FLOAT,
-		flag_errno_math ? ATTR_NOTHROW_LIST
-				: (flag_unsafe_math_optimizations
-				   ? ATTR_CONST_NOTHROW_LIST
-				   : ATTR_PURE_NOTHROW_LIST))
-DEF_LIB_BUILTIN(BUILT_IN_LOGF,
-		"__builtin_logf",
-		BT_FN_FLOAT_FLOAT,
-		flag_errno_math ? ATTR_NOTHROW_LIST
-				: (flag_unsafe_math_optimizations
-				   ? ATTR_CONST_NOTHROW_LIST
-				   : ATTR_PURE_NOTHROW_LIST))
-DEF_LIB_BUILTIN(BUILT_IN_SQRTL,
-		"__builtin_sqrtl",
-		BT_FN_LONG_DOUBLE_LONG_DOUBLE,
-		flag_errno_math ? ATTR_NOTHROW_LIST
-				: (flag_unsafe_math_optimizations
-				   ? ATTR_CONST_NOTHROW_LIST
-				   : ATTR_PURE_NOTHROW_LIST))
-DEF_LIB_BUILTIN(BUILT_IN_SINL,
-		"__builtin_sinl",
-		BT_FN_LONG_DOUBLE_LONG_DOUBLE,
-		flag_unsafe_math_optimizations ? ATTR_CONST_NOTHROW_LIST
-					       : ATTR_PURE_NOTHROW_LIST)
-DEF_LIB_BUILTIN(BUILT_IN_COSL,
-		"__builtin_cosl",
-		BT_FN_LONG_DOUBLE_LONG_DOUBLE,
-		flag_unsafe_math_optimizations ? ATTR_CONST_NOTHROW_LIST
-					       : ATTR_PURE_NOTHROW_LIST)
-DEF_LIB_BUILTIN(BUILT_IN_EXPL,
-		"__builtin_expl",
-		BT_FN_LONG_DOUBLE_LONG_DOUBLE,
-		flag_errno_math ? ATTR_NOTHROW_LIST
-				: (flag_unsafe_math_optimizations
-				   ? ATTR_CONST_NOTHROW_LIST
-				   : ATTR_PURE_NOTHROW_LIST))
-DEF_LIB_BUILTIN(BUILT_IN_LOGL,
-		"__builtin_logl",
-		BT_FN_LONG_DOUBLE_LONG_DOUBLE,
-		flag_errno_math ? ATTR_NOTHROW_LIST
-				: (flag_unsafe_math_optimizations
-				   ? ATTR_CONST_NOTHROW_LIST
-				   : ATTR_PURE_NOTHROW_LIST))
+DEF_C99_C90RES_BUILTIN(BUILT_IN_SQRTF,
+		       "__builtin_sqrtf",
+		       BT_FN_FLOAT_FLOAT,
+		       flag_errno_math ? ATTR_NOTHROW_LIST
+				       : (flag_unsafe_math_optimizations
+					  ? ATTR_CONST_NOTHROW_LIST
+					  : ATTR_PURE_NOTHROW_LIST))
+DEF_C99_C90RES_BUILTIN(BUILT_IN_SINF,
+		       "__builtin_sinf",
+		       BT_FN_FLOAT_FLOAT,
+		       flag_unsafe_math_optimizations ? ATTR_CONST_NOTHROW_LIST
+						      : ATTR_PURE_NOTHROW_LIST)
+DEF_C99_C90RES_BUILTIN(BUILT_IN_COSF,
+		       "__builtin_cosf",
+		       BT_FN_FLOAT_FLOAT,
+		       flag_unsafe_math_optimizations ? ATTR_CONST_NOTHROW_LIST
+						      : ATTR_PURE_NOTHROW_LIST)
+DEF_C99_C90RES_BUILTIN(BUILT_IN_EXPF,
+		       "__builtin_expf",
+		       BT_FN_FLOAT_FLOAT,
+		       flag_errno_math ? ATTR_NOTHROW_LIST
+				       : (flag_unsafe_math_optimizations
+					  ? ATTR_CONST_NOTHROW_LIST
+					  : ATTR_PURE_NOTHROW_LIST))
+DEF_C99_C90RES_BUILTIN(BUILT_IN_LOGF,
+		       "__builtin_logf",
+		       BT_FN_FLOAT_FLOAT,
+		       flag_errno_math ? ATTR_NOTHROW_LIST
+				       : (flag_unsafe_math_optimizations
+					  ? ATTR_CONST_NOTHROW_LIST
+					  : ATTR_PURE_NOTHROW_LIST))
+DEF_C99_C90RES_BUILTIN(BUILT_IN_SQRTL,
+		       "__builtin_sqrtl",
+		       BT_FN_LONG_DOUBLE_LONG_DOUBLE,
+		       flag_errno_math ? ATTR_NOTHROW_LIST
+				       : (flag_unsafe_math_optimizations
+					  ? ATTR_CONST_NOTHROW_LIST
+					  : ATTR_PURE_NOTHROW_LIST))
+DEF_C99_C90RES_BUILTIN(BUILT_IN_SINL,
+		       "__builtin_sinl",
+		       BT_FN_LONG_DOUBLE_LONG_DOUBLE,
+		       flag_unsafe_math_optimizations ? ATTR_CONST_NOTHROW_LIST
+						      : ATTR_PURE_NOTHROW_LIST)
+DEF_C99_C90RES_BUILTIN(BUILT_IN_COSL,
+		       "__builtin_cosl",
+		       BT_FN_LONG_DOUBLE_LONG_DOUBLE,
+		       flag_unsafe_math_optimizations ? ATTR_CONST_NOTHROW_LIST
+						      : ATTR_PURE_NOTHROW_LIST)
+DEF_C99_C90RES_BUILTIN(BUILT_IN_EXPL,
+		       "__builtin_expl",
+		       BT_FN_LONG_DOUBLE_LONG_DOUBLE,
+		       flag_errno_math ? ATTR_NOTHROW_LIST
+				       : (flag_unsafe_math_optimizations
+					  ? ATTR_CONST_NOTHROW_LIST
+					  : ATTR_PURE_NOTHROW_LIST))
+DEF_C99_C90RES_BUILTIN(BUILT_IN_LOGL,
+		       "__builtin_logl",
+		       BT_FN_LONG_DOUBLE_LONG_DOUBLE,
+		       flag_errno_math ? ATTR_NOTHROW_LIST
+				       : (flag_unsafe_math_optimizations
+					  ? ATTR_CONST_NOTHROW_LIST
+					  : ATTR_PURE_NOTHROW_LIST))
 
 DEF_GCC_BUILTIN(BUILT_IN_INF,
 		"__builtin_inf",
@@ -593,7 +672,7 @@ DEF_BUILTIN (BUILT_IN_FPUTS,
 	     BUILT_IN_NORMAL,
 	     BT_FN_INT_CONST_STRING_PTR,
 	     BT_FN_INT_VAR,
-	     true, true, false, ATTR_NOTHROW_LIST)
+	     true, true, false, ATTR_NOTHROW_LIST, true)
 DEF_FALLBACK_BUILTIN(BUILT_IN_FWRITE,
 		     "__builtin_fwrite",
 		     BT_FN_SIZE_CONST_PTR_SIZE_SIZE_PTR,
@@ -631,7 +710,7 @@ DEF_BUILTIN (BUILT_IN_FPUTS_UNLOCKED,
 	     BUILT_IN_NORMAL,
 	     BT_FN_INT_CONST_STRING_PTR,
 	     BT_FN_INT_VAR,
-	     true, true, true, ATTR_NOTHROW_LIST)
+	     true, true, true, ATTR_NOTHROW_LIST, true)
 DEF_EXT_FALLBACK_BUILTIN(BUILT_IN_FWRITE_UNLOCKED,
 			 "__builtin_fwrite_unlocked",
 			 BT_FN_SIZE_CONST_PTR_SIZE_SIZE_PTR)
@@ -736,7 +815,7 @@ DEF_BUILTIN (BUILT_IN_ABORT,
 	     BT_FN_VOID,
 	     BT_FN_VOID,
 	     1, 0, 0,
-	     ATTR_NORETURN_NOTHROW_LIST)
+	     ATTR_NORETURN_NOTHROW_LIST, true)
 
 DEF_BUILTIN (BUILT_IN_EXIT,
 	     "__builtin_exit",
@@ -744,7 +823,7 @@ DEF_BUILTIN (BUILT_IN_EXIT,
 	     BT_FN_VOID_INT,
 	     BT_FN_VOID_INT,
 	     1, 0, 0,
-	     ATTR_NORETURN_NOTHROW_LIST)
+	     ATTR_NORETURN_NOTHROW_LIST, true)
 
 DEF_BUILTIN (BUILT_IN__EXIT,
 	     "__builtin__exit",
@@ -752,7 +831,7 @@ DEF_BUILTIN (BUILT_IN__EXIT,
 	     BT_FN_VOID_INT,
 	     BT_FN_VOID_INT,
 	     1, 0, 1,
-	     ATTR_NORETURN_NOTHROW_LIST)
+	     ATTR_NORETURN_NOTHROW_LIST, false)
 
 DEF_BUILTIN (BUILT_IN__EXIT2,
 	     "__builtin__Exit",
@@ -760,5 +839,5 @@ DEF_BUILTIN (BUILT_IN__EXIT2,
 	     BT_FN_VOID_INT,
 	     BT_FN_VOID_INT,
 	     1, 0, !flag_isoc99,
-	     ATTR_NORETURN_NOTHROW_LIST)
+	     ATTR_NORETURN_NOTHROW_LIST, false)
 
--- gcc-3.3.1/gcc/c-common.c.hammer-branch	2003-06-08 00:13:24.000000000 +0200
+++ gcc-3.3.1/gcc/c-common.c	2003-08-05 18:22:46.000000000 +0200
@@ -3552,7 +3552,7 @@ c_common_nodes_and_builtins ()
     c_init_attributes ();
 
 #define DEF_BUILTIN(ENUM, NAME, CLASS, TYPE, LIBTYPE,			\
-		    BOTH_P, FALLBACK_P, NONANSI_P, ATTRS)		\
+		    BOTH_P, FALLBACK_P, NONANSI_P, ATTRS, IMPLICIT)	\
   if (NAME)								\
     {									\
       tree decl;							\
@@ -3579,6 +3579,8 @@ c_common_nodes_and_builtins ()
 				   built_in_attributes[(int) ATTRS]);	\
 									\
       built_in_decls[(int) ENUM] = decl;				\
+      if (IMPLICIT)							\
+        implicit_built_in_decls[(int) ENUM] = decl;			\
     }									
 #include "builtins.def"
 #undef DEF_BUILTIN
@@ -4512,9 +4514,9 @@ c_expand_builtin_printf (arglist, target
      int unlocked;
 {
   tree fn_putchar = unlocked ?
-    built_in_decls[BUILT_IN_PUTCHAR_UNLOCKED] : built_in_decls[BUILT_IN_PUTCHAR];
+    implicit_built_in_decls[BUILT_IN_PUTCHAR_UNLOCKED] : implicit_built_in_decls[BUILT_IN_PUTCHAR];
   tree fn_puts = unlocked ?
-    built_in_decls[BUILT_IN_PUTS_UNLOCKED] : built_in_decls[BUILT_IN_PUTS];
+    implicit_built_in_decls[BUILT_IN_PUTS_UNLOCKED] : implicit_built_in_decls[BUILT_IN_PUTS];
   tree fn, format_arg, stripped_string;
 
   /* If the return value is used, or the replacement _DECL isn't
@@ -4616,9 +4618,9 @@ c_expand_builtin_fprintf (arglist, targe
      int unlocked;
 {
   tree fn_fputc = unlocked ?
-    built_in_decls[BUILT_IN_FPUTC_UNLOCKED] : built_in_decls[BUILT_IN_FPUTC];
+    implicit_built_in_decls[BUILT_IN_FPUTC_UNLOCKED] : implicit_built_in_decls[BUILT_IN_FPUTC];
   tree fn_fputs = unlocked ?
-    built_in_decls[BUILT_IN_FPUTS_UNLOCKED] : built_in_decls[BUILT_IN_FPUTS];
+    implicit_built_in_decls[BUILT_IN_FPUTS_UNLOCKED] : implicit_built_in_decls[BUILT_IN_FPUTS];
   tree fn, format_arg, stripped_string;
 
   /* If the return value is used, or the replacement _DECL isn't
--- gcc-3.3.1/gcc/c-decl.c.hammer-branch	2003-07-16 14:37:53.000000000 +0200
+++ gcc-3.3.1/gcc/c-decl.c	2003-08-05 18:22:46.000000000 +0200
@@ -282,7 +282,7 @@ static tree grokdeclarator		PARAMS ((tre
 static tree grokparms			PARAMS ((tree, int));
 static void layout_array_type		PARAMS ((tree));
 static tree c_make_fname_decl           PARAMS ((tree, int));
-static void c_expand_body               PARAMS ((tree, int, int));
+static void c_expand_body_1             PARAMS ((tree, int));
 static void warn_if_shadowing		PARAMS ((tree, tree));
 static bool flexible_array_type_p	PARAMS ((tree));
 static tree set_save_expr_context	PARAMS ((tree *, int *, void *));
@@ -6447,10 +6447,62 @@ finish_function (nested, can_defer_p)
   free_after_compilation (cfun);
   cfun = NULL;
 
+  if (flag_unit_at_time)
+    {
+      cgraph_finalize_function (fndecl, DECL_SAVED_TREE (fndecl));
+      current_function_decl = NULL;
+      return;
+    }
+
   if (! nested)
     {
-      /* Generate RTL for the body of this function.  */
-      c_expand_body (fndecl, nested, can_defer_p);
+      /* Function is parsed.
+	 Generate RTL for the body of this function or deffer
+	 it for later expansion.  */
+      int uninlinable = 1;
+
+      /* There's no reason to do any of the work here if we're only doing
+	 semantic analysis; this code just generates RTL.  */
+      if (flag_syntax_only)
+	{
+	  current_function_decl = NULL;
+	  DECL_SAVED_TREE (fndecl) = NULL_TREE;
+	  return;
+	}
+
+      if (flag_inline_trees)
+	{
+	  /* First, cache whether the current function is inlinable.  Some
+	     predicates depend on cfun and current_function_decl to
+	     function completely.  */
+	  timevar_push (TV_INTEGRATION);
+	  uninlinable = ! tree_inlinable_function_p (fndecl, 0);
+	  
+	  if (! uninlinable && can_defer_p
+	      /* Save function tree for inlining.  Should return 0 if the
+		 language does not support function deferring or the
+		 function could not be deferred.  */
+	      && defer_fn (fndecl))
+	    {
+	      /* Let the back-end know that this function exists.  */
+	      (*debug_hooks->deferred_inline_function) (fndecl);
+	      timevar_pop (TV_INTEGRATION);
+	      current_function_decl = NULL;
+	      return;
+	    }
+	  
+	  /* Then, inline any functions called in it.  */
+	  optimize_inline_calls (fndecl);
+	  timevar_pop (TV_INTEGRATION);
+	}
+
+      c_expand_body (fndecl);
+
+      if (uninlinable && !dump_enabled_p (TDI_all))
+	{
+	  /* Allow the body of the function to be garbage collected.  */
+	  DECL_SAVED_TREE (fndecl) = NULL_TREE;
+	}
 
       /* Let the error reporting routines know that we're outside a
 	 function.  For a nested function, this value is used in
@@ -6469,7 +6521,13 @@ c_expand_deferred_function (fndecl)
      function was deferred, e.g. in duplicate_decls.  */
   if (DECL_INLINE (fndecl) && DECL_RESULT (fndecl))
     {
-      c_expand_body (fndecl, 0, 0);
+      if (flag_inline_trees)
+	{
+	  timevar_push (TV_INTEGRATION);
+	  optimize_inline_calls (fndecl);
+	  timevar_pop (TV_INTEGRATION);
+	}
+      c_expand_body (fndecl);
       current_function_decl = NULL;
     }
 }
@@ -6500,51 +6558,18 @@ set_save_expr_context (tp, walk_subtrees
    generation of RTL.  */
 
 static void
-c_expand_body (fndecl, nested_p, can_defer_p)
+c_expand_body_1 (fndecl, nested_p)
      tree fndecl;
-     int nested_p, can_defer_p;
+     int nested_p;
 {
-  int uninlinable = 1;
   int saved_lineno;
   const char *saved_input_filename;
 
-  /* There's no reason to do any of the work here if we're only doing
-     semantic analysis; this code just generates RTL.  */
-  if (flag_syntax_only)
-    return;
-
   saved_lineno = lineno;
   saved_input_filename = input_filename;
   lineno = DECL_SOURCE_LINE (fndecl);
   input_filename = DECL_SOURCE_FILE (fndecl);
 
-  if (flag_inline_trees)
-    {
-      /* First, cache whether the current function is inlinable.  Some
-         predicates depend on cfun and current_function_decl to
-         function completely.  */
-      timevar_push (TV_INTEGRATION);
-      uninlinable = ! tree_inlinable_function_p (fndecl);
-      
-      if (! uninlinable && can_defer_p
-	  /* Save function tree for inlining.  Should return 0 if the
-             language does not support function deferring or the
-             function could not be deferred.  */
-	  && defer_fn (fndecl))
-	{
-	  /* Let the back-end know that this function exists.  */
-	  (*debug_hooks->deferred_inline_function) (fndecl);
-          timevar_pop (TV_INTEGRATION);
-          lineno = saved_lineno;
-          input_filename = saved_input_filename;
-	  return;
-	}
-      
-      /* Then, inline any functions called in it.  */
-      optimize_inline_calls (fndecl);
-      timevar_pop (TV_INTEGRATION);
-    }
-
   timevar_push (TV_EXPAND);
 
   if (nested_p)
@@ -6592,13 +6617,6 @@ c_expand_body (fndecl, nested_p, can_def
   /* Generate the RTL for this function.  */
   expand_stmt (DECL_SAVED_TREE (fndecl));
 
-  /* Keep the function body if it's needed for inlining or dumping.  */
-  if (uninlinable && !dump_enabled_p (TDI_all))
-    {
-      /* Allow the body of the function to be garbage collected.  */
-      DECL_SAVED_TREE (fndecl) = NULL_TREE;
-    }
-
   /* We hard-wired immediate_size_expand to zero above.
      expand_function_end will decrement this variable.  So, we set the
      variable to one here, so that after the decrement it will remain
@@ -6698,6 +6716,15 @@ c_expand_body (fndecl, nested_p, can_def
   lineno = saved_lineno;
   input_filename = saved_input_filename;
 }
+
+/* Like c_expand_body_1 but only for unnested functions.  */
+
+void
+c_expand_body (fndecl)
+     tree fndecl;
+{
+  c_expand_body_1 (fndecl, 0);
+}
 
 /* Check the declarations given in a for-loop for satisfying the C99
    constraints.  */
@@ -6931,7 +6958,7 @@ c_expand_decl_stmt (t)
   if (TREE_CODE (decl) == FUNCTION_DECL
       && DECL_CONTEXT (decl) == current_function_decl
       && DECL_SAVED_TREE (decl))
-    c_expand_body (decl, /*nested_p=*/1, /*can_defer_p=*/0);
+    c_expand_body_1 (decl, 1);
 }
 
 /* Return the IDENTIFIER_GLOBAL_VALUE of T, for use in common code, since
--- gcc-3.3.1/gcc/c-lang.c.hammer-branch	2002-09-07 01:32:12.000000000 +0200
+++ gcc-3.3.1/gcc/c-lang.c	2003-08-05 18:22:46.000000000 +0200
@@ -97,6 +97,9 @@ static void c_init_options PARAMS ((void
 #undef LANG_HOOKS_TREE_DUMP_DUMP_TREE_FN
 #define LANG_HOOKS_TREE_DUMP_DUMP_TREE_FN c_dump_tree
 
+#undef LANG_HOOKS_CALLGRAPH_EXPAND_FUNCTION
+#define LANG_HOOKS_CALLGRAPH_EXPAND_FUNCTION c_expand_body
+
 #undef LANG_HOOKS_TYPE_FOR_MODE
 #define LANG_HOOKS_TYPE_FOR_MODE c_common_type_for_mode
 #undef LANG_HOOKS_TYPE_FOR_SIZE
--- gcc-3.3.1/gcc/c-objc-common.c.hammer-branch	2003-05-02 21:52:02.000000000 +0200
+++ gcc-3.3.1/gcc/c-objc-common.c	2003-08-05 18:22:46.000000000 +0200
@@ -357,7 +357,13 @@ finish_cdtor (body)
 void
 c_objc_common_finish_file ()
 {
-  expand_deferred_fns ();
+  if (flag_unit_at_time)
+    {
+      cgraph_finalize_compilation_unit ();
+      cgraph_optimize ();
+    }
+  else
+    expand_deferred_fns ();
 
   if (static_ctors)
     {
--- gcc-3.3.1/gcc/c-tree.h.hammer-branch	2002-09-16 20:33:18.000000000 +0200
+++ gcc-3.3.1/gcc/c-tree.h	2003-08-05 18:22:46.000000000 +0200
@@ -172,6 +172,7 @@ extern void finish_file				PARAMS ((void
 extern int objc_comptypes                 	PARAMS ((tree, tree, int));
 extern tree objc_message_selector		PARAMS ((void));
 extern tree lookup_objc_ivar			PARAMS ((tree));
+extern void c_expand_body			PARAMS ((tree));
 
 
 /* in c-parse.in */
--- gcc-3.3.1/gcc/c-typeck.c.hammer-branch	2003-07-25 13:00:11.000000000 +0200
+++ gcc-3.3.1/gcc/c-typeck.c	2003-08-05 18:22:46.000000000 +0200
@@ -3789,8 +3789,11 @@ build_c_cast (type, expr)
 		    get_alias_set (TREE_TYPE (type))))
 	    warning ("dereferencing type-punned pointer will break strict-aliasing rules");
 	}
-      
+
       ovalue = value;
+      /* Replace a nonvolatile const static variable with its value.  */
+      if (optimize && TREE_CODE (value) == VAR_DECL)
+	value = decl_constant_value (value);
       value = convert (type, value);
 
       /* Ignore any integer overflow caused by the cast.  */
--- gcc-3.3.1/gcc/callgraph.c.hammer-branch	2003-08-05 18:22:46.000000000 +0200
+++ gcc-3.3.1/gcc/callgraph.c	2003-08-05 18:22:46.000000000 +0200
@@ -0,0 +1,631 @@
+/* Callgraph handling code.
+   Copyright (C) 2003 Free Software Foundation, Inc.
+   Contributed by Jan Hubicka
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+#include "config.h"
+#include "system.h"
+#include "tree.h"
+#include "tree-inline.h"
+#include "langhooks.h"
+#include "hashtab.h"
+#include "toplev.h"
+#include "flags.h"
+#include "ggc.h"
+#include "debug.h"
+#include "target.h"
+#include "varray.h"
+
+/* The declarations we know about must not get garbage collected.
+   We do not want callgraph datastructure to be saved via PCH code
+   since it would make it dificult to extend it into untramodule
+   optimizer later, so we store the references into the array to avoid
+   garbage collector from doing it's job.  */
+extern varray_type known_fns;
+
+/* The cgraph data strutcture.
+   Each function decl has assigned cgraph_node listing calees and callers.  */
+
+struct cgraph_node
+{
+  tree decl;
+  struct cgraph_edge *callees;
+  struct cgraph_edge *callers;
+  struct cgraph_node *next;
+  /* For nested functions points to function the node is nested in.  */
+  struct cgraph_node *origin;
+  void *aux;
+
+  /* Set when function must be output - it is externally visible
+     or it's address is taken.  */
+  bool needed;
+  /* Set when function is reachable by call from other function
+     that is eighter reachable or needed.  */
+  bool reachable;
+  /* Set when the frontend has been asked to lower representation of this
+     function into trees.  Callees lists are not available when lowered
+     is not set.  */
+  bool lowered;
+  /* Set when function is scheduled to be assembled.  */
+  bool output;
+
+  /* Set when function is inlinable at all and when it is profitable to inline
+     it many times.  */
+  bool inline_single;
+  bool inline_many;
+};
+
+struct cgraph_edge
+{
+  struct cgraph_node *caller, *callee;
+  struct cgraph_edge *next_caller;
+  struct cgraph_edge *next_callee;
+};
+
+/* Hash table used to convert declarations into nodes.  */
+static htab_t cgraph_hash = 0;
+
+/* The linked list of cgraph nodes.  */
+static struct cgraph_node *cgraph_nodes;
+
+/* Number of nodes in existence.  */
+static int cgraph_n_nodes;
+
+static struct cgraph_node *cgraph_node PARAMS ((tree decl));
+static struct cgraph_edge *create_edge PARAMS ((struct cgraph_node *,
+						struct cgraph_node *));
+static void remove_edge PARAMS ((struct cgraph_node *, struct cgraph_node *));
+static struct cgraph_edge *record_call PARAMS ((tree, tree));
+static tree record_call_1 PARAMS ((tree *, int *, void *));
+static hashval_t hash_node PARAMS ((const PTR));
+static int eq_node PARAMS ((const PTR, const PTR));
+static struct cgraph_node *cgraph_node PARAMS ((tree));
+static tree fixup_calls PARAMS ((tree *, int *, void *));
+static void cgraph_expand_functions PARAMS ((void));
+
+/* Returns a hash code for P.  */
+
+static hashval_t
+hash_node (p)
+     const PTR p;
+{
+  return (hashval_t)
+    htab_hash_pointer (DECL_ASSEMBLER_NAME
+		       (((struct cgraph_node *) p)->decl));
+}
+
+/* Returns non-zero if P1 and P2 are equal.  */
+
+static int
+eq_node (p1, p2)
+     const PTR p1;
+     const PTR p2;
+{
+  return ((DECL_ASSEMBLER_NAME (((struct cgraph_node *) p1)->decl)) ==
+	  DECL_ASSEMBLER_NAME ((tree) p2));
+}
+
+/* Return cgraph node assigned to DECL.  Create new one when needed.  */
+static struct cgraph_node *
+cgraph_node (decl)
+     tree decl;
+{
+  struct cgraph_node *node;
+  struct cgraph_node **slot;
+
+  if (TREE_CODE (decl) != FUNCTION_DECL)
+    abort ();
+
+  if (!cgraph_hash)
+    {
+      cgraph_hash = htab_create (10, hash_node, eq_node, NULL);
+      VARRAY_TREE_INIT (known_fns, 32, "known_fns");
+    }
+
+  slot =
+    (struct cgraph_node **) htab_find_slot_with_hash (cgraph_hash, decl,
+						      htab_hash_pointer
+						      (DECL_ASSEMBLER_NAME
+						       (decl)), 1);
+  if (*slot)
+    return *slot;
+  node = xcalloc (sizeof (*node), 1);
+  node->decl = decl;
+  node->next = cgraph_nodes;
+  cgraph_nodes = node;
+  cgraph_n_nodes++;
+  *slot = node;
+  if (DECL_CONTEXT (decl))
+    node->origin = cgraph_node (DECL_CONTEXT (decl));
+  VARRAY_PUSH_TREE (known_fns, decl);
+  return node;
+}
+
+/* Create edge from CALLER to CALLEE in the cgraph.  */
+
+static struct cgraph_edge *
+create_edge (caller, callee)
+     struct cgraph_node *caller, *callee;
+{
+  struct cgraph_edge *edge = xmalloc (sizeof (struct cgraph_edge));
+
+  edge->caller = caller;
+  edge->callee = callee;
+  edge->next_caller = callee->callers;
+  edge->next_callee = caller->callees;
+  caller->callees = edge;
+  callee->callers = edge;
+  return edge;
+}
+
+/* Create edge from CALLER to CALLEE in the cgraph.  */
+
+static void
+remove_edge (caller, callee)
+     struct cgraph_node *caller, *callee;
+{
+  struct cgraph_edge **edge, **edge2;
+
+  for (edge = &callee->callers; *edge && (*edge)->caller != caller;
+       edge = &((*edge)->next_caller))
+    ;
+  if (!*edge)
+    abort ();
+  *edge = (*edge)->next_caller;
+  for (edge2 = &caller->callees; *edge2 && (*edge2)->callee != callee;
+       edge2 = &(*edge2)->next_callee)
+    ;
+  if (!*edge2)
+    abort ();
+  *edge2 = (*edge2)->next_callee;
+}
+
+/* Record call from CALLER to CALLEE  */
+
+static struct cgraph_edge *
+record_call (caller, callee)
+     tree caller, callee;
+{
+  return create_edge (cgraph_node (caller), cgraph_node (callee));
+}
+
+void
+cgraph_remove_call (caller, callee)
+     tree caller, callee;
+{
+  remove_edge (cgraph_node (caller), cgraph_node (callee));
+}
+
+/* Return true when CALLER_DECL calls CALLEE_DECL.  */
+
+bool
+cgraph_calls_p (caller_decl, callee_decl)
+     tree caller_decl, callee_decl;
+{
+  struct cgraph_node *caller = cgraph_node (caller_decl);
+  struct cgraph_node *callee = cgraph_node (callee_decl);
+  struct cgraph_edge *edge;
+
+  for (edge = callee->callers; edge && (edge)->caller != caller;
+       edge = (edge->next_caller))
+    ;
+  return edge != NULL;
+}
+
+/* Walk tree and record all calls.  Called via walk_tree.  */
+static tree
+record_call_1 (tp, walk_subtrees, data)
+     tree *tp;
+     int *walk_subtrees ATTRIBUTE_UNUSED;
+     void *data;
+{
+  /* Record dereferences to the functions.  This makes the functions
+     reachable unconditionally.  */
+  if (TREE_CODE (*tp) == ADDR_EXPR)
+    {
+      tree decl = TREE_OPERAND (*tp, 0);
+      if (TREE_CODE (decl) == FUNCTION_DECL)
+	{
+	  struct cgraph_node *n = cgraph_node (decl);
+	  n->needed = n->reachable = true;
+	}
+    }
+  else if (TREE_CODE (*tp) == CALL_EXPR)
+    {
+      tree decl = TREE_OPERAND (*tp, 0);
+      if (TREE_CODE (decl) == ADDR_EXPR)
+	decl = TREE_OPERAND (decl, 0);
+      if (TREE_CODE (decl) == FUNCTION_DECL)
+	{
+	  if (DECL_BUILT_IN (decl))
+	    return NULL;
+	  record_call (data, decl);
+	  walk_tree (&TREE_OPERAND (*tp, 1), record_call_1, data, NULL);
+	  *walk_subtrees = 0;
+	}
+    }
+  return NULL;
+}
+
+/* Create cgraph edges for function calles via BODY.  */
+
+void
+cgraph_create_edges (decl, body)
+     tree decl;
+     tree body;
+{
+  walk_tree (&body, record_call_1, decl, NULL);
+}
+
+/* Analyze function once it is parsed.  Set up the local information
+   available - create cgraph edges for function calles via BODY.  */
+
+void
+cgraph_finalize_function (decl, body)
+     tree decl;
+     tree body ATTRIBUTE_UNUSED;
+{
+  struct cgraph_node *node = cgraph_node (decl);
+
+  node->decl = decl;
+
+  node->inline_single = tree_inlinable_function_p (decl, 1);
+  node->inline_many = tree_inlinable_function_p (decl, 0);
+
+  (*debug_hooks->deferred_inline_function) (decl);
+}
+
+/* Dump the callgraph.  */
+
+void
+dump_cgraph (f)
+     FILE *f;
+{
+  struct cgraph_node *node;
+
+  fprintf (f, "\nCallgraph:\n\n");
+  for (node = cgraph_nodes; node; node = node->next)
+    {
+      struct cgraph_edge *edge;
+      fprintf (f, "%s", IDENTIFIER_POINTER (DECL_NAME (node->decl)));
+      if (node->origin)
+	fprintf (f, " nested in: %s",
+		 IDENTIFIER_POINTER (DECL_NAME (node->origin->decl)));
+      if (node->needed)
+	fprintf (f, " needed");
+      else if (node->reachable)
+	fprintf (f, " reachable");
+      if (node->inline_single)
+	fprintf (f, " inlinable");
+      if (node->inline_many)
+	fprintf (f, " small");
+      if (DECL_SAVED_TREE (node->decl))
+	fprintf (f, " tree");
+
+      fprintf (f, "\n  called by :");
+      for (edge = node->callers; edge; edge = edge->next_caller)
+	fprintf (f, "%s ",
+		 IDENTIFIER_POINTER (DECL_NAME (edge->caller->decl)));
+
+      fprintf (f, "\n  calls: ");
+      for (edge = node->callees; edge; edge = edge->next_callee)
+	fprintf (f, "%s ",
+		 IDENTIFIER_POINTER (DECL_NAME (edge->callee->decl)));
+      fprintf (f, "\n");
+    }
+}
+
+/* Replace forward declaration by real ones allowing reverse inlining.
+   Also replace types by new ones to allow optimizers to modify function
+   declaration (such as add attributes to it)  */
+
+static tree
+fixup_calls (tp, walk_subtrees, data)
+     tree *tp;
+     int *walk_subtrees ATTRIBUTE_UNUSED;
+     void *data ATTRIBUTE_UNUSED;
+{
+  if (TREE_CODE (*tp) == CALL_EXPR)
+    {
+      tree *addr = &TREE_OPERAND (*tp, 0);
+      tree *decl = &TREE_OPERAND (*tp, 0);
+      if (TREE_CODE (*decl) == ADDR_EXPR)
+	decl = &TREE_OPERAND (*decl, 0);
+      if (TREE_CODE (*decl) == FUNCTION_DECL)
+	{
+	  if (DECL_BUILT_IN (*decl))
+	    return NULL;
+	  *decl = cgraph_node (*decl)->decl;
+	  if (TREE_CODE (*addr) == ADDR_EXPR)
+	    {
+	      TREE_TYPE (*addr) = copy_node (TREE_TYPE (*addr));
+	      TREE_TYPE (TREE_TYPE (*addr)) = TREE_TYPE (*decl);
+	    }
+	}
+    }
+  return NULL;
+}
+
+/* Analyze whole compilation unit once it is parsed completely.  */
+
+void
+cgraph_finalize_compilation_unit ()
+{
+  struct cgraph_node *node;
+  struct cgraph_edge *edge;
+  bool changed = true;
+
+  if (!quiet_flag)
+    fprintf (stderr, "\n\nUnit entry points:");
+  /*  Lower representation of all reachable functions.  */
+  while (changed)
+    {
+      changed = false;
+      for (node = cgraph_nodes; node; node = node->next)
+	{
+	  tree decl = node->decl;
+
+	  if (node->lowered || !DECL_SAVED_TREE (decl))
+	    continue;
+	  if ((TREE_PUBLIC (decl) && !DECL_COMDAT (decl) && !DECL_EXTERNAL (decl))
+	      || (DECL_ASSEMBLER_NAME_SET_P (decl)
+		  && TREE_SYMBOL_REFERENCED (DECL_ASSEMBLER_NAME (decl))))
+	    {
+	      announce_function (decl);
+	      node->needed = node->reachable = true;
+	    }
+	  /* At the moment frontend automatically emits all nested functions.  */
+	  if (node->origin && node->origin->reachable)
+	    node->reachable = 1;
+	  if (!node->reachable)
+	    continue;
+
+	  if (lang_hooks.callgraph.lower_function)
+	    (*lang_hooks.callgraph.lower_function) (decl);
+	  /* First kill forward declaration so reverse inling works properly.  */
+	  walk_tree (&DECL_SAVED_TREE (decl), fixup_calls, NULL, NULL);
+	  cgraph_create_edges (decl, DECL_SAVED_TREE (decl));
+
+	  for (edge = node->callees; edge; edge = edge->next_callee)
+	    edge->callee->reachable = true;
+	  node->lowered = true;
+	  changed = true;
+	}
+    }
+  if (!quiet_flag)
+    {
+      /*dump_cgraph (stderr);*/
+      fprintf (stderr, "\n\nReclaiming functions:");
+    }
+
+  for (node = cgraph_nodes; node; node = node->next)
+    {
+      tree decl = node->decl;
+
+      if (!node->reachable && DECL_SAVED_TREE (decl))
+	{
+	  DECL_SAVED_TREE (decl) = NULL;
+	  announce_function (decl);
+	}
+    }
+  ggc_collect ();
+  /* Now inline functions marked for inlining.  */
+  for (node = cgraph_nodes; node; node = node->next)
+    {
+      tree decl = node->decl;
+      if (DECL_SAVED_TREE (decl)
+	  && node->needed
+	  && (TREE_PUBLIC (decl)
+	      || !node->inline_single) && !TREE_ASM_WRITTEN (decl))
+	optimize_inline_calls (decl);
+    }
+}
+
+/* Expand all functions that must be output.  */
+#if 0
+static void
+cgraph_expand_functions ()
+{
+  struct cgraph_node *node;
+  for (node = cgraph_nodes; node; node = node->next)
+    {
+      tree decl = node->decl;
+
+      if (DECL_SAVED_TREE (decl)
+	  && (node->needed
+	      || (DECL_UNINLINABLE (decl) && node->reachable)
+	      || TREE_SYMBOL_REFERENCED (DECL_ASSEMBLER_NAME (decl)))
+	  && !TREE_ASM_WRITTEN (decl) && !node->origin
+	  && !DECL_EXTERNAL (decl))
+	{
+	  announce_function (decl);
+	  optimize_inline_calls (decl);
+	  walk_tree (&DECL_SAVED_TREE (decl), fixup_calls, NULL, NULL);
+	  if (!node->reachable)
+	    abort ();
+	  (*lang_hooks.callgraph.expand_function) (decl);
+	  if (DECL_UNINLINABLE (decl))
+	    DECL_SAVED_TREE (decl) = NULL;
+	  current_function_decl = NULL;
+	}
+    }
+}
+#endif
+
+#define NPREDECESORS(node) (size_t)((node)->aux)
+#define SET_NPREDECESORS(node,n) (node)->aux = (void *) (n);
+
+/* Expand all functions that must be output.  */
+
+static void
+cgraph_expand_functions ()
+{
+  struct cgraph_node *node;
+  struct cgraph_node **stack =
+    xcalloc (sizeof (struct cgraph_node *), cgraph_n_nodes);
+  int stack_size = 0;
+  struct cgraph_edge *edge;
+
+  /* Figure out functions we want to assemble.  */
+  for (node = cgraph_nodes; node; node = node->next)
+    {
+      tree decl = node->decl;
+
+      if (DECL_SAVED_TREE (decl)
+	  && (node->needed
+	      || (DECL_UNINLINABLE (decl) && node->reachable)
+	      || TREE_SYMBOL_REFERENCED (DECL_ASSEMBLER_NAME (decl)))
+	  && !TREE_ASM_WRITTEN (decl) && !node->origin
+	  && !DECL_EXTERNAL (decl))
+	node->output = 1;
+    }
+
+  /* Attempt to topologically sort the nodes so function is output when
+     all called functions are already assembled to allow data to be propagated
+     accross the callgraph.  Use stack to get smaller distance between function
+     and it's callees (later we may use more sophisticated algorithm for
+     function reordering, we will likely want to use subsections to make output
+     functions to appear in top-down order, not bottom-up they are assembled).
+     */
+
+  for (node = cgraph_nodes; node; node = node->next)
+    if (node->output)
+      {
+	int n = 0;
+	for (edge = node->callees; edge; edge = edge->next_callee)
+	  if (edge->callee->output)
+	    n++;
+	SET_NPREDECESORS (node, n);
+	if (n == 0)
+	  stack[stack_size++] = node;
+      }
+  while (1)
+    {
+      struct cgraph_node *minnode;
+      while (stack_size)
+	{
+	  tree decl;
+
+	  node = stack[--stack_size];
+	  decl = node->decl;
+	  announce_function (decl);
+	  node->output = 0;
+
+	  for (edge = node->callers; edge; edge = edge->next_caller)
+	    if (edge->caller->output)
+	      {
+	        SET_NPREDECESORS (edge->caller,
+		    		  NPREDECESORS (edge->caller) - 1);
+		if (!NPREDECESORS (edge->caller))
+		  stack[stack_size++] = edge->caller;
+	      }
+	  optimize_inline_calls (decl);
+	  walk_tree (&DECL_SAVED_TREE (decl), fixup_calls, NULL, NULL);
+	  if (!node->reachable)
+	    abort ();
+	  (*lang_hooks.callgraph.expand_function) (decl);
+	  if (DECL_UNINLINABLE (decl))
+	    DECL_SAVED_TREE (decl) = NULL;
+	  current_function_decl = NULL;
+	}
+      minnode = NULL;
+      /* We found cycle.  Break it and try again.  */
+      for (node = cgraph_nodes; node; node = node->next)
+	if (node->output
+	    && (!minnode
+	        || NPREDECESORS (minnode) > NPREDECESORS (node)))
+	  minnode = node;
+      if (!minnode)
+	return;
+      stack[stack_size++] = minnode;
+    }
+}
+
+/* Perform simple optimizations based on callgraph.  */
+
+void
+cgraph_optimize ()
+{
+  struct cgraph_node *node;
+  bool changed = true;
+  struct cgraph_edge *edge;
+
+  if (!quiet_flag)
+    fprintf (stderr, "\n\nMarking as always_inline:");
+  /* Now look for function called only once and mark them to inline.  From this
+     point number of calls to given function won't grow.  */
+  for (node = cgraph_nodes; node; node = node->next)
+    {
+      tree decl = node->decl;
+
+      if (node->callers && !node->callers->next_caller && !TREE_PUBLIC (decl)
+	  && node->inline_single)
+	{
+	  DECL_ATTRIBUTES (decl) =
+	    chainon (DECL_ATTRIBUTES (decl),
+		     build_tree_list (get_identifier ("always_inline"),
+				      NULL_TREE));
+	  DECL_INLINE (decl) = true;
+	  DECL_UNINLINABLE (decl) = 0;
+	  announce_function (decl);
+	}
+    }
+
+  if (targetm.cgraph.optimize_local_function)
+    {
+      if (!quiet_flag)
+	fprintf (stderr, "\n\nMarking local functions:");
+      for (node = cgraph_nodes; node; node = node->next)
+	{
+	  tree decl = node->decl;
+
+	  if (!node->needed && !TREE_PUBLIC (decl)
+	      && DECL_SAVED_TREE (decl)
+	      && (!DECL_ASSEMBLER_NAME_SET_P (decl)
+		  || !TREE_SYMBOL_REFERENCED (DECL_ASSEMBLER_NAME (decl))))
+	    {
+	      (*targetm.cgraph.optimize_local_function) (decl);
+	      announce_function (decl);
+	    }
+	}
+    }
+  if (!quiet_flag)
+    fprintf (stderr, "\n\nAssembling functions:");
+
+  /* Output everything.  
+     ??? Our inline heuristic may decide to not inline functions previously
+     marked as inlinable thus adding new function bodies that must be output.
+     Later we should move all inlining decisions to callgraph code to make
+     this impossible.  */
+  cgraph_expand_functions ();
+  while (changed)
+    {
+      changed = false;
+      for (node = cgraph_nodes; node; node = node->next)
+	{
+	  if (!node->needed)
+	    continue;
+
+	  for (edge = node->callees; edge; edge = edge->next_callee)
+	    if (!edge->callee->needed)
+	      changed = edge->callee->needed = true;
+	}
+    }
+  cgraph_expand_functions ();
+}
--- gcc-3.3.1/gcc/calls.c.hammer-branch	2003-07-19 11:32:37.000000000 +0200
+++ gcc-3.3.1/gcc/calls.c	2003-08-05 18:22:46.000000000 +0200
@@ -2629,7 +2629,7 @@ expand_call (exp, target, ignore)
 	 is subject to race conditions, just as with multithreaded
 	 programs.  */
 
-      emit_library_call (gen_rtx_SYMBOL_REF (Pmode, "__bb_fork_func"),
+      emit_library_call (gen_rtx_SYMBOL_REF (Pmode, "__gcov_flush"),
 		      	 LCT_ALWAYS_RETURN,
 			 VOIDmode, 0);
     }
--- gcc-3.3.1/gcc/cfg.c.hammer-branch	2003-03-25 21:31:42.000000000 +0100
+++ gcc-3.3.1/gcc/cfg.c	2003-08-05 18:22:46.000000000 +0200
@@ -516,54 +516,7 @@ void
 dump_flow_info (file)
      FILE *file;
 {
-  int i;
-  int max_regno = max_reg_num ();
   basic_block bb;
-  static const char * const reg_class_names[] = REG_CLASS_NAMES;
-
-  fprintf (file, "%d registers.\n", max_regno);
-  for (i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)
-    if (REG_N_REFS (i))
-      {
-	enum reg_class class, altclass;
-
-	fprintf (file, "\nRegister %d used %d times across %d insns",
-		 i, REG_N_REFS (i), REG_LIVE_LENGTH (i));
-	if (REG_BASIC_BLOCK (i) >= 0)
-	  fprintf (file, " in block %d", REG_BASIC_BLOCK (i));
-	if (REG_N_SETS (i))
-	  fprintf (file, "; set %d time%s", REG_N_SETS (i),
-		   (REG_N_SETS (i) == 1) ? "" : "s");
-	if (regno_reg_rtx[i] != NULL && REG_USERVAR_P (regno_reg_rtx[i]))
-	  fprintf (file, "; user var");
-	if (REG_N_DEATHS (i) != 1)
-	  fprintf (file, "; dies in %d places", REG_N_DEATHS (i));
-	if (REG_N_CALLS_CROSSED (i) == 1)
-	  fprintf (file, "; crosses 1 call");
-	else if (REG_N_CALLS_CROSSED (i))
-	  fprintf (file, "; crosses %d calls", REG_N_CALLS_CROSSED (i));
-	if (regno_reg_rtx[i] != NULL
-	    && PSEUDO_REGNO_BYTES (i) != UNITS_PER_WORD)
-	  fprintf (file, "; %d bytes", PSEUDO_REGNO_BYTES (i));
-
-	class = reg_preferred_class (i);
-	altclass = reg_alternate_class (i);
-	if (class != GENERAL_REGS || altclass != ALL_REGS)
-	  {
-	    if (altclass == ALL_REGS || class == ALL_REGS)
-	      fprintf (file, "; pref %s", reg_class_names[(int) class]);
-	    else if (altclass == NO_REGS)
-	      fprintf (file, "; %s or none", reg_class_names[(int) class]);
-	    else
-	      fprintf (file, "; pref %s, else %s",
-		       reg_class_names[(int) class],
-		       reg_class_names[(int) altclass]);
-	  }
-
-	if (regno_reg_rtx[i] != NULL && REG_POINTER (regno_reg_rtx[i]))
-	  fprintf (file, "; pointer");
-	fprintf (file, ".\n");
-      }
 
   fprintf (file, "\n%d basic blocks, %d edges.\n", n_basic_blocks, n_edges);
   FOR_EACH_BB (bb)
--- gcc-3.3.1/gcc/cfglayout.c.hammer-branch	2003-04-07 18:31:42.000000000 +0200
+++ gcc-3.3.1/gcc/cfglayout.c	2003-08-05 18:22:46.000000000 +0200
@@ -29,6 +29,8 @@ Software Foundation, 59 Temple Place - S
 #include "function.h"
 #include "obstack.h"
 #include "cfglayout.h"
+#include "cfgloop.h"
+#include "target.h"
 
 /* The contents of the current function definition are allocated
    in this obstack, and all are freed at the end of the function.  */
@@ -46,10 +48,11 @@ static void set_block_levels		PARAMS ((t
 static void change_scope		PARAMS ((rtx, tree, tree));
 
 void verify_insn_chain			PARAMS ((void));
-static void cleanup_unconditional_jumps	PARAMS ((void));
+static void cleanup_unconditional_jumps	PARAMS ((struct loops *));
 static void fixup_fallthru_exit_predecessor PARAMS ((void));
 static rtx unlink_insn_chain PARAMS ((rtx, rtx));
 static rtx duplicate_insn_chain PARAMS ((rtx, rtx));
+static void break_superblocks PARAMS ((void));
 
 static rtx
 unlink_insn_chain (first, last)
@@ -643,10 +646,12 @@ verify_insn_chain ()
 /* Remove any unconditional jumps and forwarder block creating fallthru
    edges instead.  During BB reordering, fallthru edges are not required
    to target next basic block in the linear CFG layout, so the unconditional
-   jumps are not needed.  */
+   jumps are not needed.  If LOOPS is not null, also update loop structure &
+   dominators.  */
 
 static void
-cleanup_unconditional_jumps ()
+cleanup_unconditional_jumps (loops)
+     struct loops *loops;
 {
   basic_block bb;
 
@@ -668,6 +673,25 @@ cleanup_unconditional_jumps ()
 		fprintf (rtl_dump_file, "Removing forwarder BB %i\n",
 			 bb->index);
 
+	      if (loops)
+		{
+		  /* bb cannot be loop header, as it only has one entry
+		     edge.  It could be a loop latch.  */
+		  if (bb->loop_father->header == bb)
+		    abort ();
+
+		  if (bb->loop_father->latch == bb)
+		    bb->loop_father->latch = bb->pred->src;
+
+		  if (get_immediate_dominator (loops->cfg.dom,
+					       bb->succ->dest) == bb)
+		    set_immediate_dominator (loops->cfg.dom,
+					     bb->succ->dest, bb->pred->src);
+
+		  remove_bb_from_loops (bb);
+		  delete_from_dominance_info (loops->cfg.dom, bb);
+		}
+
 	      redirect_edge_succ_nodup (bb->pred, bb->succ->dest);
 	      flow_delete_block (bb);
 	      bb = prev;
@@ -756,6 +780,21 @@ cfg_layout_can_duplicate_bb_p (bb)
       && (GET_CODE (PATTERN (next)) == ADDR_VEC
 	  || GET_CODE (PATTERN (next)) == ADDR_DIFF_VEC))
     return false;
+
+  /* Do not duplicate blocks containing insns that can't be copied.  */
+  if (targetm.cannot_copy_insn_p)
+    {
+      rtx insn = bb->head;
+      while (1)
+	{
+	  if (INSN_P (insn) && (*targetm.cannot_copy_insn_p) (insn))
+	    return false;
+	  if (insn == bb->end)
+	    break;
+	  insn = NEXT_INSN (insn);
+	}
+    }
+
   return true;
 }
 
@@ -773,7 +812,6 @@ duplicate_insn_chain (from, to)
      be reordered later.  */
   for (insn = from; insn != NEXT_INSN (to); insn = NEXT_INSN (insn))
     {
-      rtx new;
       switch (GET_CODE (insn))
 	{
 	case INSN:
@@ -785,7 +823,7 @@ duplicate_insn_chain (from, to)
 	  if (GET_CODE (PATTERN (insn)) == ADDR_VEC
 	      || GET_CODE (PATTERN (insn)) == ADDR_DIFF_VEC)
 	    break;
-	  new = emit_copy_of_insn_after (insn, get_last_insn ());
+	  emit_copy_of_insn_after (insn, get_last_insn ());
 	  break;
 
 	case CODE_LABEL:
@@ -854,13 +892,14 @@ duplicate_insn_chain (from, to)
 }
 
 /* Redirect Edge to DEST.  */
-void
+bool
 cfg_layout_redirect_edge (e, dest)
      edge e;
      basic_block dest;
 {
   basic_block src = e->src;
   basic_block old_next_bb = src->next_bb;
+  bool ret;
 
   /* Redirect_edge_and_branch may decide to turn branch into fallthru edge
      in the case the basic block appears to be in sequence.  Avoid this
@@ -888,9 +927,11 @@ cfg_layout_redirect_edge (e, dest)
 	    delete_insn (src->end);
 	}
       redirect_edge_succ_nodup (e, dest);
+
+      ret = true;
     }
   else
-    redirect_edge_and_branch (e, dest);
+    ret = redirect_edge_and_branch (e, dest);
 
   /* We don't want simplejumps in the insn stream during cfglayout.  */
   if (simplejump_p (src->end))
@@ -900,6 +941,22 @@ cfg_layout_redirect_edge (e, dest)
       src->succ->flags |= EDGE_FALLTHRU;
     }
   src->next_bb = old_next_bb;
+
+  return ret;
+}
+
+/* Same as split_block but update cfg_layout structures.  */
+edge
+cfg_layout_split_block (bb, insn)
+     basic_block bb;
+     rtx insn;
+{
+  edge fallthru = split_block (bb, insn);
+
+  alloc_aux_for_block (fallthru->dest, sizeof (struct reorder_block_def));
+  RBI (fallthru->dest)->footer = RBI (fallthru->src)->footer;
+  RBI (fallthru->src)->footer = NULL;
+  return fallthru;
 }
 
 /* Create a duplicate of the basic block BB and redirect edge E into it.  */
@@ -988,6 +1045,7 @@ cfg_layout_duplicate_bb (bb, e)
     bb->frequency = 0;
 
   RBI (new_bb)->original = bb;
+  RBI (bb)->copy = new_bb;
   return new_bb;
 }
 
@@ -995,17 +1053,47 @@ cfg_layout_duplicate_bb (bb, e)
    CFG layout changes.  It keeps LOOPS up-to-date if not null.  */
 
 void
-cfg_layout_initialize ()
+cfg_layout_initialize (loops)
+     struct loops *loops;
 {
   /* Our algorithm depends on fact that there are now dead jumptables
      around the code.  */
   alloc_aux_for_blocks (sizeof (struct reorder_block_def));
 
-  cleanup_unconditional_jumps ();
+  cleanup_unconditional_jumps (loops);
 
   record_effective_endpoints ();
 }
 
+/* Splits superblocks.  */
+static void
+break_superblocks ()
+{
+  sbitmap superblocks;
+  int i, need;
+
+  superblocks = sbitmap_alloc (n_basic_blocks);
+  sbitmap_zero (superblocks);
+
+  need = 0;
+
+  for (i = 0; i < n_basic_blocks; i++)
+    if (BASIC_BLOCK(i)->flags & BB_SUPERBLOCK)
+      {
+	BASIC_BLOCK(i)->flags &= ~BB_SUPERBLOCK;
+	SET_BIT (superblocks, i);
+	need = 1;
+      }
+
+  if (need)
+    {
+      rebuild_jump_labels (get_insns ());
+      find_many_sub_basic_blocks (superblocks);
+    }
+
+  free (superblocks);
+}
+
 /* Finalize the changes: reorder insn list according to the sequence, enter
    compensation code, rebuild scope forest.  */
 
@@ -1021,6 +1109,8 @@ cfg_layout_finalize ()
 
   free_aux_for_blocks ();
 
+  break_superblocks ();
+
 #ifdef ENABLE_CHECKING
   verify_flow_info ();
 #endif
--- gcc-3.3.1/gcc/cfglayout.h.hammer-branch	2002-05-08 11:17:17.000000000 +0200
+++ gcc-3.3.1/gcc/cfglayout.h	2003-08-05 18:22:46.000000000 +0200
@@ -25,6 +25,9 @@ typedef struct reorder_block_def
   rtx footer;
   basic_block next;
   basic_block original;
+  /* Used by loop copying.  */
+  basic_block copy;
+  int duplicated;
 
   /* These fields are used by bb-reorder pass.  */
   int visited;
@@ -32,10 +35,11 @@ typedef struct reorder_block_def
 
 #define RBI(BB)	((reorder_block_def) (BB)->aux)
 
-extern void cfg_layout_initialize	PARAMS ((void));
+extern void cfg_layout_initialize	PARAMS ((struct loops *));
 extern void cfg_layout_finalize		PARAMS ((void));
 extern bool cfg_layout_can_duplicate_bb_p PARAMS ((basic_block));
 extern basic_block cfg_layout_duplicate_bb PARAMS ((basic_block, edge));
 extern void scope_to_insns_initialize	PARAMS ((void));
 extern void scope_to_insns_finalize	PARAMS ((void));
-extern void cfg_layout_redirect_edge	PARAMS ((edge, basic_block));
+extern bool cfg_layout_redirect_edge	PARAMS ((edge, basic_block));
+extern edge cfg_layout_split_block	PARAMS ((basic_block, rtx));
--- gcc-3.3.1/gcc/cfgloop.c.hammer-branch	2002-09-22 04:03:15.000000000 +0200
+++ gcc-3.3.1/gcc/cfgloop.c	2003-08-05 18:22:46.000000000 +0200
@@ -24,6 +24,8 @@ Software Foundation, 59 Temple Place - S
 #include "hard-reg-set.h"
 #include "basic-block.h"
 #include "toplev.h"
+#include "cfgloop.h"
+#include "flags.h"
 
 /* Ratio of frequencies of edges so that one of more latch edges is
    considered to belong to inner loop with same header.  */
@@ -112,7 +114,7 @@ flow_loop_dump (loop, file, loop_dump_au
      int verbose;
 {
   basic_block *bbs;
-  int i;
+  unsigned i;
 
   if (! loop || ! loop->header)
     return;
@@ -204,7 +206,7 @@ flow_loops_free (loops)
 {
   if (loops->parray)
     {
-      int i;
+      unsigned i;
 
       if (! loops->num)
 	abort ();
@@ -273,7 +275,7 @@ flow_loop_exit_edges_find (loop)
 {
   edge e;
   basic_block node, *bbs;
-  int num_exits, i;
+  unsigned num_exits, i;
 
   loop->exit_edges = NULL;
   loop->num_exits = 0;
@@ -331,11 +333,9 @@ flow_loop_nodes_find (header, loop)
   basic_block *stack;
   int sp;
   int num_nodes = 1;
-  int findex, lindex;
 
   header->loop_father = loop;
   header->loop_depth = loop->depth;
-  findex = lindex = header->index;
 
   if (loop->latch->loop_father != loop)
     {
@@ -609,6 +609,10 @@ make_forwarder_block (bb, redirect_latch
 
   insn = PREV_INSN (first_insn_after_basic_block_note (bb));
 
+  /* For empty block split_block will return NULL.  */
+  if (bb->end == insn)
+    emit_note_after (NOTE_INSN_DELETED, insn);
+
   fallthru = split_block (bb, insn);
   dummy = fallthru->src;
   bb = fallthru->dest;
@@ -804,19 +808,20 @@ flow_loops_find (loops, flags)
      
       header->loop_depth = 0;
 
+      /* If we have an abnormal predecessor, do not consider the
+	 loop (not worth the problems).  */
+      for (e = header->pred; e; e = e->pred_next)
+	if (e->flags & EDGE_ABNORMAL)
+	  break;
+      if (e)
+	continue;
+
       for (e = header->pred; e; e = e->pred_next)
 	{
 	  basic_block latch = e->src;
 
 	  if (e->flags & EDGE_ABNORMAL)
-	    {
-	      if (more_latches)
-		{
-		  RESET_BIT (headers, header->index);
-		  num_loops--;
-		}
-	      break;
-	    }
+	    abort ();
 
 	  /* Look for back edges where a predecessor is dominated
 	     by this block.  A natural loop has a single entry
@@ -925,9 +930,11 @@ flow_loops_find (loops, flags)
       loops->cfg.dom = NULL;
       free_dominance_info (dom);
     }
+
+  loops->state = 0;
 #ifdef ENABLE_CHECKING
   verify_flow_info ();
-  verify_loop_structure (loops, 0);
+  verify_loop_structure (loops);
 #endif
 
   return loops->num;
@@ -991,7 +998,7 @@ get_loop_body (loop)
      const struct loop *loop;
 {
   basic_block *tovisit, bb;
-  int tv = 0;
+  unsigned tv = 0;
 
   if (!loop->num_nodes)
     abort ();
@@ -1002,7 +1009,7 @@ get_loop_body (loop)
   if (loop->latch == EXIT_BLOCK_PTR)
     {
       /* There may be blocks unreachable from EXIT_BLOCK.  */
-      if (loop->num_nodes != n_basic_blocks + 2)
+      if (loop->num_nodes != (unsigned) n_basic_blocks + 2)
 	abort ();
       FOR_EACH_BB (bb)
 	tovisit[tv++] = bb;
@@ -1072,18 +1079,57 @@ find_common_loop (loop_s, loop_d)
   return loop_s;
 }
 
+/* Cancels the LOOP; it must be innermost one.  */
+void
+cancel_loop (loops, loop)
+     struct loops *loops;
+     struct loop *loop;
+{
+  basic_block *bbs;
+  unsigned i;
+
+  if (loop->inner)
+    abort ();
+
+  /* Move blocks up one level (they should be removed as soon as possible).  */
+  bbs = get_loop_body (loop);
+  for (i = 0; i < loop->num_nodes; i++)
+    bbs[i]->loop_father = loop->outer;
+
+  /* Remove the loop from structure.  */
+  flow_loop_tree_node_remove (loop);
+
+  /* Remove loop from loops array.  */
+  loops->parray[loop->num] = NULL;
+
+  /* Free loop data.  */
+  flow_loop_free (loop);
+}
+
+/* Cancels LOOP and all its subloops.  */
+void
+cancel_loop_tree (loops, loop)
+     struct loops *loops;
+     struct loop *loop;
+{
+  while (loop->inner)
+    cancel_loop_tree (loops, loop->inner);
+  cancel_loop (loops, loop);
+}
+
 /* Checks that LOOPS are allright:
      -- sizes of loops are allright
      -- results of get_loop_body really belong to the loop
      -- loop header have just single entry edge and single latch edge
      -- loop latches have only single successor that is header of their loop
+     -- irreducible loops are correctly marked
   */
 void
-verify_loop_structure (loops, flags)
+verify_loop_structure (loops)
      struct loops *loops;
-     int flags;
 {
-  int *sizes, i, j;
+  unsigned *sizes, i, j;
+  sbitmap irreds;
   basic_block *bbs, bb;
   struct loop *loop;
   int err = 0;
@@ -1136,14 +1182,14 @@ verify_loop_structure (loops, flags)
       if (!loop)
 	continue;
 
-      if ((flags & VLS_EXPECT_PREHEADERS)
+      if ((loops->state & LOOPS_HAVE_PREHEADERS)
 	  && (!loop->header->pred->pred_next
 	      || loop->header->pred->pred_next->pred_next))
 	{
 	  error ("Loop %d's header does not have exactly 2 entries.", i);
 	  err = 1;
 	}
-      if (flags & VLS_EXPECT_SIMPLE_LATCHES)
+      if (loops->state & LOOPS_HAVE_SIMPLE_LATCHES)
 	{
 	  if (!loop->latch->succ
 	      || loop->latch->succ->succ_next)
@@ -1169,6 +1215,39 @@ verify_loop_structure (loops, flags)
 	}
     }
 
+  /* Check irreducible loops.  */
+  if (loops->state & LOOPS_HAVE_MARKED_IRREDUCIBLE_REGIONS)
+    {
+      /* Record old info.  */
+      irreds = sbitmap_alloc (last_basic_block);
+      FOR_EACH_BB (bb)
+	if (bb->flags & BB_IRREDUCIBLE_LOOP)
+	  SET_BIT (irreds, bb->index);
+	else
+	  RESET_BIT (irreds, bb->index);
+
+      /* Recount it.  */
+      mark_irreducible_loops (loops);
+
+      /* Compare.  */
+      FOR_EACH_BB (bb)
+	{
+	  if ((bb->flags & BB_IRREDUCIBLE_LOOP)
+	      && !TEST_BIT (irreds, bb->index))
+	    {
+	      error ("Basic block %d should be marked irreducible.", bb->index);
+	      err = 1;
+	    }
+	  else if (!(bb->flags & BB_IRREDUCIBLE_LOOP)
+	      && TEST_BIT (irreds, bb->index))
+	    {
+	      error ("Basic block %d should not be marked irreducible.", bb->index);
+	      err = 1;
+	    }
+	}
+      free (irreds);
+    }
+
   if (err)
     abort ();
 }
@@ -1176,7 +1255,7 @@ verify_loop_structure (loops, flags)
 /* Returns latch edge of LOOP.  */
 edge
 loop_latch_edge (loop)
-     struct loop *loop;
+     const struct loop *loop;
 {
   edge e;
 
@@ -1189,7 +1268,7 @@ loop_latch_edge (loop)
 /* Returns preheader edge of LOOP.  */
 edge
 loop_preheader_edge (loop)
-     struct loop *loop;
+     const struct loop *loop;
 {
   edge e;
 
--- gcc-3.3.1/gcc/cfgloop.h.hammer-branch	2003-08-05 18:22:46.000000000 +0200
+++ gcc-3.3.1/gcc/cfgloop.h	2003-08-05 18:22:46.000000000 +0200
@@ -0,0 +1,352 @@
+/* Natural loop functions
+   Copyright (C) 1987, 1997, 1998, 1999, 2000, 2001, 2002
+   Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+/* Structure to hold decision about unrolling/peeling.  */
+enum lpt_dec
+{
+  LPT_NONE,
+  LPT_PEEL_COMPLETELY,
+  LPT_PEEL_SIMPLE,
+  LPT_UNROLL_CONSTANT,
+  LPT_UNROLL_RUNTIME,
+  LPT_UNROLL_STUPID
+};
+
+struct lpt_decision
+{
+  enum lpt_dec decision;
+  unsigned times;
+};
+
+/* Description of loop for simple loop unrolling.  */
+struct loop_desc
+{
+  int postincr;		/* 1 if increment/decrement is done after loop exit condition.  */
+  rtx stride;		/* Value added to VAR in each iteration.  */
+  rtx var;		/* Loop control variable.  */
+  rtx var_alts;		/* List of definitions of its initial value.  */
+  rtx lim;		/* Expression var is compared with.  */
+  rtx lim_alts;		/* List of definitions of its initial value.  */
+  bool const_iter;      /* True if it iterates constant number of times.  */
+  unsigned HOST_WIDE_INT niter;
+			/* Number of iterations if it is constant.  */
+  bool may_be_zero;     /* If we cannot determine that the first iteration will pass.  */
+  enum rtx_code cond;	/* Exit condition.  */
+  int neg;		/* Set to 1 if loop ends when condition is satisfied.  */
+  edge out_edge;	/* The exit edge.  */
+  edge in_edge;		/* And the other one.  */
+  int n_branches;	/* Number of branches inside the loop.  */
+};
+
+/* Structure to hold information for each natural loop.  */
+struct loop
+{
+  /* Index into loops array.  */
+  int num;
+
+  /* Basic block of loop header.  */
+  basic_block header;
+
+  /* Basic block of loop latch.  */
+  basic_block latch;
+
+  /* Basic block of loop pre-header or NULL if it does not exist.  */
+  basic_block pre_header;
+
+  /* For loop unrolling/peeling decision.  */
+  struct lpt_decision lpt_decision;
+
+  /* Simple loop description.  */
+  int simple;
+  struct loop_desc desc;
+  int has_desc;
+
+  /* Number of loop insns.  */
+  unsigned ninsns;
+
+  /* Array of edges along the pre-header extended basic block trace.
+     The source of the first edge is the root node of pre-header
+     extended basic block, if it exists.  */
+  edge *pre_header_edges;
+
+  /* Number of edges along the pre_header extended basic block trace.  */
+  int num_pre_header_edges;
+
+  /* The first block in the loop.  This is not necessarily the same as
+     the loop header.  */
+  basic_block first;
+
+  /* The last block in the loop.  This is not necessarily the same as
+     the loop latch.  */
+  basic_block last;
+
+  /* Bitmap of blocks contained within the loop.  */
+  sbitmap nodes;
+
+  /* Number of blocks contained within the loop.  */
+  unsigned num_nodes;
+
+  /* Array of edges that enter the loop.  */
+  edge *entry_edges;
+
+  /* Number of edges that enter the loop.  */
+  int num_entries;
+
+  /* Array of edges that exit the loop.  */
+  edge *exit_edges;
+
+  /* Number of edges that exit the loop.  */
+  int num_exits;
+
+  /* Bitmap of blocks that dominate all exits of the loop.  */
+  sbitmap exits_doms;
+
+  /* The loop nesting depth.  */
+  int depth;
+
+  /* Superloops of the loop.  */
+  struct loop **pred;
+
+  /* The height of the loop (enclosed loop levels) within the loop
+     hierarchy tree.  */
+  int level;
+
+  /* The outer (parent) loop or NULL if outermost loop.  */
+  struct loop *outer;
+
+  /* The first inner (child) loop or NULL if innermost loop.  */
+  struct loop *inner;
+
+  /* Link to the next (sibling) loop.  */
+  struct loop *next;
+
+  /* Loop that is copy of this loop.  */
+  struct loop *copy;
+
+  /* Non-zero if the loop is invalid (e.g., contains setjmp.).  */
+  int invalid;
+
+  /* Auxiliary info specific to a pass.  */
+  void *aux;
+
+  /* The following are currently used by loop.c but they are likely to
+     disappear as loop.c is converted to use the CFG.  */
+
+  /* Non-zero if the loop has a NOTE_INSN_LOOP_VTOP.  */
+  rtx vtop;
+
+  /* Non-zero if the loop has a NOTE_INSN_LOOP_CONT.
+     A continue statement will generate a branch to NEXT_INSN (cont).  */
+  rtx cont;
+
+  /* The dominator of cont.  */
+  rtx cont_dominator;
+
+  /* The NOTE_INSN_LOOP_BEG.  */
+  rtx start;
+
+  /* The NOTE_INSN_LOOP_END.  */
+  rtx end;
+
+  /* For a rotated loop that is entered near the bottom,
+     this is the label at the top.  Otherwise it is zero.  */
+  rtx top;
+
+  /* Place in the loop where control enters.  */
+  rtx scan_start;
+
+  /* The position where to sink insns out of the loop.  */
+  rtx sink;
+
+  /* List of all LABEL_REFs which refer to code labels outside the
+     loop.  Used by routines that need to know all loop exits, such as
+     final_biv_value and final_giv_value.
+
+     This does not include loop exits due to return instructions.
+     This is because all bivs and givs are pseudos, and hence must be
+     dead after a return, so the presense of a return does not affect
+     any of the optimizations that use this info.  It is simpler to
+     just not include return instructions on this list.  */
+  rtx exit_labels;
+
+  /* The number of LABEL_REFs on exit_labels for this loop and all
+     loops nested inside it.  */
+  int exit_count;
+};
+  
+/* Histogram of a loop.  */
+struct loop_histogram
+{
+  unsigned steps;
+  gcov_type *counts;
+  gcov_type more;
+};
+
+/* Flags for state of loop structure.  */
+enum
+{
+  LOOPS_HAVE_PREHEADERS = 1,
+  LOOPS_HAVE_SIMPLE_LATCHES = 2,
+  LOOPS_HAVE_MARKED_IRREDUCIBLE_REGIONS = 4
+};
+
+/* Structure to hold CFG information about natural loops within a function.  */
+struct loops
+{
+  /* Number of natural loops in the function.  */
+  unsigned num;
+
+  /* Maximum nested loop level in the function.  */
+  unsigned levels;
+
+  /* Array of natural loop descriptors (scanning this array in reverse order
+     will find the inner loops before their enclosing outer loops).  */
+  struct loop *array;
+
+  /* The above array is unused in new loop infrastructure and is kept only for
+     purposes of the old loop optimizer.  Instead we store just pointers to
+     loops here.  */
+  struct loop **parray;
+
+  /* Pointer to root of loop heirachy tree.  */
+  struct loop *tree_root;
+
+  /* Information derived from the CFG.  */
+  struct cfg
+  {
+    /* The bitmap vector of dominators or NULL if not computed.  */
+    dominance_info dom;
+
+    /* The ordering of the basic blocks in a depth first search.  */
+    int *dfs_order;
+
+    /* The reverse completion ordering of the basic blocks found in a
+       depth first search.  */
+    int *rc_order;
+  } cfg;
+
+  /* Headers shared by multiple loops that should be merged.  */
+  sbitmap shared_headers;
+
+  /* State of loops.  */
+  int state;
+};
+
+/* Flags for loop discovery.  */
+
+#define LOOP_TREE		1	/* Build loop hierarchy tree.  */
+#define LOOP_PRE_HEADER		2	/* Analyse loop pre-header.  */
+#define LOOP_ENTRY_EDGES	4	/* Find entry edges.  */
+#define LOOP_EXIT_EDGES		8	/* Find exit edges.  */
+#define LOOP_EDGES		(LOOP_ENTRY_EDGES | LOOP_EXIT_EDGES)
+#define LOOP_ALL	       15	/* All of the above  */
+
+/* Loop recognition.  */
+extern int flow_loops_find		PARAMS ((struct loops *, int flags));
+extern int flow_loops_update		PARAMS ((struct loops *, int flags));
+extern void flow_loops_free		PARAMS ((struct loops *));
+extern void flow_loops_dump		PARAMS ((const struct loops *, FILE *,
+						void (*)(const struct loop *,
+						FILE *, int), int));
+extern void flow_loop_dump		PARAMS ((const struct loop *, FILE *,
+						void (*)(const struct loop *,
+						FILE *, int), int));
+extern int flow_loop_scan		PARAMS ((struct loops *,
+						struct loop *, int));
+void mark_irreducible_loops		PARAMS ((struct loops *));
+
+/* Loop datastructure manipulation/querying.  */
+extern void flow_loop_tree_node_add	PARAMS ((struct loop *, struct loop *));
+extern void flow_loop_tree_node_remove	PARAMS ((struct loop *));
+extern bool flow_loop_outside_edge_p	PARAMS ((const struct loop *, edge));
+extern bool flow_loop_nested_p		PARAMS ((const struct loop *,
+						const struct loop *));
+extern bool flow_bb_inside_loop_p	PARAMS ((const struct loop *,
+						basic_block));
+extern struct loop * find_common_loop	PARAMS ((struct loop *, struct loop *));
+extern int num_loop_insns		PARAMS ((struct loop *));
+
+/* Loops & cfg manipulation.  */
+extern basic_block *get_loop_body	PARAMS ((const struct loop *));
+
+extern edge loop_preheader_edge		PARAMS ((const struct loop *));
+extern edge loop_latch_edge		PARAMS ((const struct loop *));
+
+extern void add_bb_to_loop		PARAMS ((basic_block, struct loop *));
+extern void remove_bb_from_loops	PARAMS ((basic_block));
+
+extern void cancel_loop			PARAMS ((struct loops *, struct loop *));
+extern void cancel_loop_tree		PARAMS ((struct loops *, struct loop *));
+
+extern basic_block loop_split_edge_with PARAMS ((edge, rtx, struct loops *));
+extern int fix_loop_placement		PARAMS ((struct loop *));
+
+enum
+{
+  CP_SIMPLE_PREHEADERS = 1,
+  CP_INSIDE_CFGLAYOUT = 2
+};
+
+extern void create_preheaders		PARAMS ((struct loops *, int));
+extern void force_single_succ_latches	PARAMS ((struct loops *));
+
+extern void verify_loop_structure	PARAMS ((struct loops *));
+
+/* Loop analysis.  */
+extern bool simple_loop_p		PARAMS ((struct loops *, struct loop *,
+						struct loop_desc *));
+extern rtx count_loop_iterations	PARAMS ((struct loop_desc *, rtx, rtx));
+extern bool just_once_each_iteration_p	PARAMS ((struct loops *,struct loop *,
+						 basic_block));
+extern unsigned expected_loop_iterations PARAMS ((const struct loop *));
+
+/* Loop manipulation.  */
+extern bool can_duplicate_loop_p	PARAMS ((struct loop *loop));
+
+#define DLTHE_FLAG_UPDATE_FREQ		1
+#define DLTHE_PROB_UPDATING(X)		(X & 6)
+#define DLTHE_USE_WONT_EXIT		2
+extern int duplicate_loop_to_header_edge PARAMS ((struct loop *, edge,
+						struct loops *, unsigned,
+						sbitmap, edge, edge *,
+						unsigned *, int));
+extern struct loop *loopify		PARAMS ((struct loops *, edge,
+						edge, basic_block));
+extern bool remove_path			PARAMS ((struct loops *, edge));
+extern edge split_loop_bb		PARAMS ((struct loops *, basic_block,
+						rtx));
+
+/* Loop optimizer initialization.  */
+extern struct loops *loop_optimizer_init PARAMS ((FILE *));
+extern void loop_optimizer_finalize	PARAMS ((struct loops *, FILE *));
+
+/* Optimization passes.  */
+extern void unswitch_loops		PARAMS ((struct loops *));
+
+enum
+{
+  UAP_PEEL = 1,
+  UAP_UNROLL = 2,
+  UAP_UNROLL_ALL = 4
+};
+
+extern void unroll_and_peel_loops	PARAMS ((struct loops *, int));
+
--- gcc-3.3.1/gcc/cfgloopanal.c.hammer-branch	2003-08-05 18:22:46.000000000 +0200
+++ gcc-3.3.1/gcc/cfgloopanal.c	2003-08-05 18:22:46.000000000 +0200
@@ -0,0 +1,1061 @@
+/* Natural loop analysis code for GNU compiler.
+   Copyright (C) 2002 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+#include "config.h"
+#include "system.h"
+#include "rtl.h"
+#include "hard-reg-set.h"
+#include "basic-block.h"
+#include "cfgloop.h"
+#include "expr.h"
+#include "output.h"
+
+struct unmark_altered_insn_data;
+static void unmark_altered	 PARAMS ((rtx, rtx, regset));
+static void blocks_invariant_registers PARAMS ((basic_block *, int, regset));
+static void unmark_altered_insn	 PARAMS ((rtx, rtx, struct unmark_altered_insn_data *));
+static void blocks_single_set_registers PARAMS ((basic_block *, int, rtx *));
+static int invariant_rtx_wrto_regs_p_helper PARAMS ((rtx *, regset));
+static bool invariant_rtx_wrto_regs_p PARAMS ((rtx, regset));
+static rtx test_for_iteration PARAMS ((struct loop_desc *desc,
+				       unsigned HOST_WIDE_INT));
+static bool constant_iterations PARAMS ((struct loop_desc *,
+					 unsigned HOST_WIDE_INT *,
+					 bool *));
+static bool simple_loop_exit_p PARAMS ((struct loops *, struct loop *,
+					edge, regset, rtx *,
+					struct loop_desc *));
+static rtx variable_initial_value PARAMS ((rtx, regset, rtx, rtx *));
+static rtx variable_initial_values PARAMS ((edge, rtx));
+static bool simple_condition_p PARAMS ((struct loop *, rtx,
+					regset, struct loop_desc *));
+static basic_block simple_increment PARAMS ((struct loops *, struct loop *,
+					     rtx *, struct loop_desc *));
+
+/* Checks whether BB is executed exactly once in each LOOP iteration.  */
+bool
+just_once_each_iteration_p (loops, loop, bb)
+     struct loops *loops;
+     struct loop *loop;
+     basic_block bb;
+{
+  /* It must be executed at least once each iteration.  */
+  if (!dominated_by_p (loops->cfg.dom, loop->latch, bb))
+    return false;
+
+  /* And just once.  */
+  if (bb->loop_father != loop)
+    return false;
+
+  /* But this was not enough.  We might have some irreducible loop here.  */
+  if (bb->flags & BB_IRREDUCIBLE_LOOP)
+    return false;
+
+  return true;
+}
+
+
+/* Unmarks modified registers; helper to blocks_invariant_registers.  */
+static void
+unmark_altered (what, by, regs)
+     rtx what;
+     rtx by ATTRIBUTE_UNUSED;
+     regset regs;
+{
+  if (GET_CODE (what) == SUBREG)
+    what = SUBREG_REG (what);
+  if (!REG_P (what))
+    return;
+  CLEAR_REGNO_REG_SET (regs, REGNO (what));
+}
+
+/* Marks registers that are invariant inside blocks BBS.  */
+static void
+blocks_invariant_registers (bbs, nbbs, regs)
+     basic_block *bbs;
+     int nbbs;
+     regset regs;
+{
+  rtx insn;
+  int i;
+
+  for (i = 0; i < max_reg_num (); i++)
+    SET_REGNO_REG_SET (regs, i);
+  for (i = 0; i < nbbs; i++)
+    for (insn = bbs[i]->head;
+	 insn != NEXT_INSN (bbs[i]->end);
+	 insn = NEXT_INSN (insn))
+      if (INSN_P (insn))
+	note_stores (PATTERN (insn),
+		     (void (*) PARAMS ((rtx, rtx, void *))) unmark_altered,
+		     regs);
+}
+
+/* Unmarks modified registers; helper to blocks_single_set_registers.  */
+struct unmark_altered_insn_data
+{
+  rtx *regs;
+  rtx insn;
+};
+
+static void
+unmark_altered_insn (what, by, data)
+     rtx what;
+     rtx by ATTRIBUTE_UNUSED;
+     struct unmark_altered_insn_data *data;
+{
+  int rn;
+
+  if (GET_CODE (what) == SUBREG)
+    what = SUBREG_REG (what);
+  if (!REG_P (what))
+    return;
+  rn = REGNO (what);
+  if (data->regs[rn] == data->insn)
+    return;
+  data->regs[rn] = NULL;
+}
+
+/* Marks registers that have just single simple set in BBS; the relevant
+   insn is returned in REGS.  */
+static void
+blocks_single_set_registers (bbs, nbbs, regs)
+     basic_block *bbs;
+     int nbbs;
+     rtx *regs;
+{
+  rtx insn;
+  int i;
+  struct unmark_altered_insn_data data;
+
+  for (i = 0; i < max_reg_num (); i++)
+    regs[i] = NULL;
+
+  for (i = 0; i < nbbs; i++)
+    for (insn = bbs[i]->head;
+	 insn != NEXT_INSN (bbs[i]->end);
+	 insn = NEXT_INSN (insn))
+      {
+	rtx set = single_set (insn);
+	if (!set)
+	  continue;
+	if (!REG_P (SET_DEST (set)))
+	  continue;
+	regs[REGNO (SET_DEST (set))] = insn;
+      }
+
+  data.regs = regs;
+  for (i = 0; i < nbbs; i++)
+    for (insn = bbs[i]->head;
+	 insn != NEXT_INSN (bbs[i]->end);
+	 insn = NEXT_INSN (insn))
+      {
+        if (!INSN_P (insn))
+	  continue;
+	data.insn = insn;
+	note_stores (PATTERN (insn),
+	    (void (*) PARAMS ((rtx, rtx, void *))) unmark_altered_insn,
+	    &data);
+      }
+}
+
+/* Helper for invariant_rtx_wrto_regs_p.  */
+static int
+invariant_rtx_wrto_regs_p_helper (expr, invariant_regs)
+     rtx *expr;
+     regset invariant_regs;
+{
+  switch (GET_CODE (*expr))
+    {
+    case CC0:
+    case PC:
+    case UNSPEC_VOLATILE:
+      return 1;
+
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case CONST:
+    case SYMBOL_REF:
+    case LABEL_REF:
+      return 0;
+
+    case ASM_OPERANDS:
+      return MEM_VOLATILE_P (*expr);
+
+    case MEM:
+      /* If the memory is not constant, assume it is modified.  If it is
+	 constant, we still have to check the address.  */
+      return !RTX_UNCHANGING_P (*expr);
+
+    case REG:
+      return !REGNO_REG_SET_P (invariant_regs, REGNO (*expr));
+
+    default:
+      return 0;
+    }
+}
+
+/* Checks that EXPR is invariant provided that INVARIANT_REGS are invariant. */
+static bool
+invariant_rtx_wrto_regs_p (expr, invariant_regs)
+     rtx expr;
+     regset invariant_regs;
+{
+  return !for_each_rtx (&expr, (rtx_function) invariant_rtx_wrto_regs_p_helper,
+			invariant_regs);
+}
+
+/* Checks whether CONDITION is a simple comparison in that one of operands
+   is register and the other one is invariant in the LOOP. Fills var, lim
+   and cond fields in DESC.  */
+static bool
+simple_condition_p (loop, condition, invariant_regs, desc)
+     struct loop *loop ATTRIBUTE_UNUSED;
+     rtx condition;
+     regset invariant_regs;
+     struct loop_desc *desc;
+{
+  rtx op0, op1;
+
+  /* Check condition.  */
+  switch (GET_CODE (condition))
+    {
+      case EQ:
+      case NE:
+      case LE:
+      case LT:
+      case GE:
+      case GT:
+      case GEU:
+      case GTU:
+      case LEU:
+      case LTU:
+	break;
+      default:
+	return false;
+    }
+
+  /* Of integers or pointers.  */
+  if (GET_MODE_CLASS (GET_MODE (XEXP (condition, 0))) != MODE_INT
+      && GET_MODE_CLASS (GET_MODE (XEXP (condition, 0))) != MODE_PARTIAL_INT)
+    return false;
+
+  /* One of operands must be a simple register.  */
+  op0 = XEXP (condition, 0);
+  op1 = XEXP (condition, 1);
+  
+  /* One of operands must be invariant.  */
+  if (invariant_rtx_wrto_regs_p (op0, invariant_regs))
+    {
+      /* And the other one must be a register.  */
+      if (!REG_P (op1))
+	return false;
+      desc->var = op1;
+      desc->lim = op0;
+
+      desc->cond = swap_condition (GET_CODE (condition));
+      if (desc->cond == UNKNOWN)
+	return false;
+      return true;
+    }
+
+  /* Check the other operand. */
+  if (!invariant_rtx_wrto_regs_p (op1, invariant_regs))
+    return false;
+  if (!REG_P (op0))
+    return false;
+
+  desc->var = op0;
+  desc->lim = op1;
+
+  desc->cond = GET_CODE (condition);
+
+  return true;
+}
+
+/* Checks whether DESC->var is incremented/decremented exactly once each
+   iteration.  Fills in DESC->stride and returns block in that DESC->var is
+   modified.  */
+static basic_block
+simple_increment (loops, loop, simple_increment_regs, desc)
+     struct loops *loops;
+     struct loop *loop;
+     rtx *simple_increment_regs;
+     struct loop_desc *desc;
+{
+  rtx mod_insn, set, set_src, set_add;
+  basic_block mod_bb;
+
+  /* Find insn that modifies var.  */
+  mod_insn = simple_increment_regs[REGNO (desc->var)];
+  if (!mod_insn)
+    return NULL;
+  mod_bb = BLOCK_FOR_INSN (mod_insn);
+
+  /* Check that it is executed exactly once each iteration.  */
+  if (!just_once_each_iteration_p (loops, loop, mod_bb))
+    return NULL;
+
+  /* mod_insn must be a simple increment/decrement.  */
+  set = single_set (mod_insn);
+  if (!set)
+    abort ();
+  if (!rtx_equal_p (SET_DEST (set), desc->var))
+    abort ();
+
+  set_src = find_reg_equal_equiv_note (mod_insn);
+  if (!set_src)
+    set_src = SET_SRC (set);
+  if (GET_CODE (set_src) != PLUS)
+    return NULL;
+  if (!rtx_equal_p (XEXP (set_src, 0), desc->var))
+    return NULL;
+
+  /* Set desc->stride.  */
+  set_add = XEXP (set_src, 1);
+  if (CONSTANT_P (set_add))
+    desc->stride = set_add;
+  else
+    return NULL;
+
+  return mod_bb;
+}
+
+/* Tries to find initial value of VAR in INSN.  This value must be invariant
+   wrto INVARIANT_REGS.  If SET_INSN is not NULL, insn in that var is set is
+   placed here.  */
+static rtx
+variable_initial_value (insn, invariant_regs, var, set_insn)
+     rtx insn;
+     regset invariant_regs;
+     rtx var;
+     rtx *set_insn;
+{
+  basic_block bb;
+  rtx set;
+
+  /* Go back through cfg.  */
+  bb = BLOCK_FOR_INSN (insn);
+  while (1)
+    {
+      for (; insn != bb->head; insn = PREV_INSN (insn))
+	{
+	  if (modified_between_p (var, PREV_INSN (insn), NEXT_INSN (insn)))
+	    break;
+	  if (INSN_P (insn))
+	    note_stores (PATTERN (insn),
+		(void (*) PARAMS ((rtx, rtx, void *))) unmark_altered,
+		invariant_regs);
+	}
+
+      if (insn != bb->head)
+	{
+	  /* We found place where var is set.  */
+	  rtx set_dest;
+	  rtx val;
+	  rtx note;
+          
+	  set = single_set (insn);
+	  if (!set)
+	    return NULL;
+	  set_dest = SET_DEST (set);
+	  if (!rtx_equal_p (set_dest, var))
+	    return NULL;
+
+	  note = find_reg_equal_equiv_note (insn);
+	  if (note && GET_CODE (XEXP (note, 0)) != EXPR_LIST)
+	    val = XEXP (note, 0);
+	  else
+	    val = SET_SRC (set);
+	  if (!invariant_rtx_wrto_regs_p (val, invariant_regs))
+	    return NULL;
+
+	  if (set_insn)
+	    *set_insn = insn;
+	  return val;
+	}
+
+
+      if (bb->pred->pred_next || bb->pred->src == ENTRY_BLOCK_PTR)
+	return NULL;
+
+      bb = bb->pred->src;
+      insn = bb->end;
+    }
+
+  return NULL;
+}
+
+/* Returns list of definitions of initial value of VAR at Edge.  */
+static rtx
+variable_initial_values (e, var)
+     edge e;
+     rtx var;
+{
+  rtx set_insn, list;
+  regset invariant_regs;
+  regset_head invariant_regs_head;
+  int i;
+
+  invariant_regs = INITIALIZE_REG_SET (invariant_regs_head);
+  for (i = 0; i < max_reg_num (); i++)
+    SET_REGNO_REG_SET (invariant_regs, i);
+
+  list = alloc_EXPR_LIST (0, copy_rtx (var), NULL);
+
+  if (e->src == ENTRY_BLOCK_PTR)
+    return list;
+
+  set_insn = e->src->end;
+  while (REG_P (var)
+	 && (var = variable_initial_value (set_insn, invariant_regs, var, &set_insn)))
+    list = alloc_EXPR_LIST (0, copy_rtx (var), list);
+
+  FREE_REG_SET (invariant_regs);
+  return list;
+}
+
+/* Counts constant number of iterations of the loop described by DESC;
+   returns false if impossible.  */
+static bool
+constant_iterations (desc, niter, may_be_zero)
+     struct loop_desc *desc;
+     unsigned HOST_WIDE_INT *niter;
+     bool *may_be_zero;
+{
+  rtx test, expr;
+  rtx ainit, alim;
+
+  test = test_for_iteration (desc, 0);
+  if (test == const0_rtx)
+    {
+      *niter = 0;
+      *may_be_zero = false;
+      return true;
+    }
+
+  *may_be_zero = (test != const_true_rtx);
+
+  /* It would make a little sense to check every with every when we
+     know that all but the first alternative are simply registers.  */
+  for (ainit = desc->var_alts; ainit; ainit = XEXP (ainit, 1))
+    {
+      alim = XEXP (desc->lim_alts, 0);
+      if (!(expr = count_loop_iterations (desc, XEXP (ainit, 0), alim)))
+	abort ();
+      if (GET_CODE (expr) == CONST_INT)
+	{
+	  *niter = INTVAL (expr);
+	  return true;
+	}
+    }
+  for (alim = XEXP (desc->lim_alts, 1); alim; alim = XEXP (alim, 1))
+    {
+      ainit = XEXP (desc->var_alts, 0);
+      if (!(expr = count_loop_iterations (desc, ainit, XEXP (alim, 0))))
+	abort ();
+      if (GET_CODE (expr) == CONST_INT)
+	{
+	  *niter = INTVAL (expr);
+	  return true;
+	}
+    }
+
+  return false;
+}
+
+/* Return RTX expression representing number of iterations of loop as bounded
+   by test described by DESC (in the case loop really has multiple exit
+   edges, fewer iterations may happen in the practice).  
+
+   Return NULL if it is unknown.  Additionally the value may be invalid for
+   paradoxical loop (lets define paradoxical loops as loops whose test is
+   failing at -1th iteration, for instance "for (i=5;i<1;i++);").
+   
+   These cases needs to be eighter cared by copying the loop test in the front
+   of loop or keeping the test in first iteration of loop.
+   
+   When INIT/LIM are set, they are used instead of var/lim of DESC. */
+rtx
+count_loop_iterations (desc, init, lim)
+     struct loop_desc *desc;
+     rtx init;
+     rtx lim;
+{
+  enum rtx_code cond = desc->cond;
+  rtx stride = desc->stride;
+  rtx mod, exp;
+
+  /* Give up on floating point modes and friends.  It can be possible to do
+     the job for constant loop bounds, but it is probably not worthwhile.  */
+  if (!INTEGRAL_MODE_P (GET_MODE (desc->var)))
+    return NULL;
+
+  init = copy_rtx (init ? init : desc->var);
+  lim = copy_rtx (lim ? lim : desc->lim);
+
+  /* Ensure that we always handle the condition to stay inside loop.  */
+  if (desc->neg)
+    cond = reverse_condition (cond);
+
+  /* Compute absolute value of the difference of initial and final value.  */
+  if (INTVAL (stride) > 0)
+    {
+      /* Bypass nonsential tests.  */
+      if (cond == EQ || cond == GE || cond == GT || cond == GEU
+	  || cond == GTU)
+	return NULL;
+      exp = simplify_gen_binary (MINUS, GET_MODE (desc->var),
+				 lim, init);
+    }
+  else
+    {
+      /* Bypass nonsential tests.  */
+      if (cond == EQ || cond == LE || cond == LT || cond == LEU
+	  || cond == LTU)
+	return NULL;
+      exp = simplify_gen_binary (MINUS, GET_MODE (desc->var),
+				 init, lim);
+      stride = simplify_gen_unary (NEG, GET_MODE (desc->var),
+				   stride, GET_MODE (desc->var));
+    }
+
+  /* Normalize difference so the value is always first examined
+     and later incremented.  */
+
+  if (!desc->postincr)
+    exp = simplify_gen_binary (MINUS, GET_MODE (desc->var),
+			       exp, stride);
+
+  /* Determine delta caused by exit condition.  */
+  switch (cond)
+    {
+    case NE:
+      /* For NE tests, make sure that the iteration variable won't miss
+	 the final value.  If EXP mod STRIDE is not zero, then the
+	 iteration variable will overflow before the loop exits, and we
+	 can not calculate the number of iterations easilly.  */
+      if (stride != const1_rtx
+	  && (simplify_gen_binary (UMOD, GET_MODE (desc->var), exp, stride)
+              != const0_rtx))
+	return NULL;
+      break;
+    case LT:
+    case GT:
+    case LTU:
+    case GTU:
+      break;
+    case LE:
+    case GE:
+    case LEU:
+    case GEU:
+      exp = simplify_gen_binary (PLUS, GET_MODE (desc->var),
+				 exp, const1_rtx);
+      break;
+    default:
+      abort ();
+    }
+
+  if (stride != const1_rtx)
+    {
+      /* Number of iterations is now (EXP + STRIDE - 1 / STRIDE),
+	 but we need to take care for overflows.   */
+
+      mod = simplify_gen_binary (UMOD, GET_MODE (desc->var), exp, stride);
+
+      /* This is dirty trick.  When we can't compute number of iterations
+	 to be constant, we simply ignore the possible overflow, as
+	 runtime unroller always use power of 2 amounts and does not
+	 care about possible lost bits.  */
+
+      if (GET_CODE (mod) != CONST_INT)
+	{
+	  rtx stridem1 = simplify_gen_binary (PLUS, GET_MODE (desc->var),
+					      stride, constm1_rtx);
+	  exp = simplify_gen_binary (PLUS, GET_MODE (desc->var),
+				     exp, stridem1);
+	  exp = simplify_gen_binary (UDIV, GET_MODE (desc->var), exp, stride);
+	}
+      else
+	{
+	  exp = simplify_gen_binary (UDIV, GET_MODE (desc->var), exp, stride);
+	  if (mod != const0_rtx)
+	    exp = simplify_gen_binary (PLUS, GET_MODE (desc->var),
+				       exp, const1_rtx);
+	}
+    }
+
+  if (rtl_dump_file)
+    {
+      fprintf (rtl_dump_file, ";  Number of iterations: ");
+      print_simple_rtl (rtl_dump_file, exp);
+      fprintf (rtl_dump_file, "\n");
+    }
+
+  return exp;
+}
+
+/* Return simplified RTX expression representing the value of test
+   described of DESC at given iteration of loop.  */
+
+static rtx
+test_for_iteration (desc, iter)
+     struct loop_desc *desc;
+     unsigned HOST_WIDE_INT iter;
+{
+  enum rtx_code cond = desc->cond;
+  rtx exp = XEXP (desc->var_alts, 0);
+  rtx addval;
+
+  /* Give up on floating point modes and friends.  It can be possible to do
+     the job for constant loop bounds, but it is probably not worthwhile.  */
+  if (!INTEGRAL_MODE_P (GET_MODE (desc->var)))
+    return NULL;
+
+  /* Ensure that we always handle the condition to stay inside loop.  */
+  if (desc->neg)
+    cond = reverse_condition (cond);
+
+  /* Compute the value of induction variable.  */
+  addval = simplify_gen_binary (MULT, GET_MODE (desc->var),
+				desc->stride,
+				gen_int_mode (desc->postincr
+					      ? iter : iter + 1,
+					      GET_MODE (desc->var)));
+  exp = simplify_gen_binary (PLUS, GET_MODE (desc->var), exp, addval);
+  /* Test at given condtion.  */
+  exp = simplify_gen_relational (cond, SImode,
+				 GET_MODE (desc->var), exp, desc->lim);
+
+  if (rtl_dump_file)
+    {
+      fprintf (rtl_dump_file,
+	       ";  Conditional to continue loop at %i th iteration: ", iter);
+      print_simple_rtl (rtl_dump_file, exp);
+      fprintf (rtl_dump_file, "\n");
+    }
+  return exp;
+}
+
+
+/* Tests whether exit at EXIT_EDGE from LOOP is simple.  Returns simple loop
+   description joined to it in in DESC.  INVARIANT_REGS and SINGLE_SET_REGS
+   are results of blocks_{invariant,single_set}_regs over BODY.  */
+static bool
+simple_loop_exit_p (loops, loop, exit_edge, invariant_regs, single_set_regs, desc)
+     struct loops *loops;
+     struct loop *loop;
+     edge exit_edge;
+     struct loop_desc *desc;
+     regset invariant_regs;
+     rtx *single_set_regs;
+{
+  basic_block mod_bb, exit_bb;
+  int fallthru_out;
+  rtx condition;
+  edge ei, e;
+
+  exit_bb = exit_edge->src;
+
+  fallthru_out = (exit_edge->flags & EDGE_FALLTHRU);
+
+  if (!exit_bb)
+    return false;
+
+  /* It must be tested (at least) once during any iteration.  */
+  if (!dominated_by_p (loops->cfg.dom, loop->latch, exit_bb))
+    return false;
+
+  /* It must end in a simple conditional jump.  */
+  if (!any_condjump_p (exit_bb->end))
+    return false;
+
+  ei = exit_bb->succ;
+  if (ei == exit_edge)
+    ei = ei->succ_next;
+
+  desc->out_edge = exit_edge;
+  desc->in_edge = ei;
+
+  /* Condition must be a simple comparison in that one of operands
+     is register and the other one is invariant.  */
+  if (!(condition = get_condition (exit_bb->end, NULL)))
+    return false;
+
+  if (!simple_condition_p (loop, condition, invariant_regs, desc))
+    return false;
+
+  /*  Var must be simply incremented or decremented in exactly one insn that
+     is executed just once every iteration.  */
+  if (!(mod_bb = simple_increment (loops, loop, single_set_regs, desc)))
+    return false;
+
+  /* OK, it is simple loop.  Now just fill in remaining info.  */
+  desc->postincr = !dominated_by_p (loops->cfg.dom, exit_bb, mod_bb);
+  desc->neg = !fallthru_out;
+
+  /* Find initial value of var and alternative values for lim.  */
+  e = loop_preheader_edge (loop);
+  desc->var_alts = variable_initial_values (e, desc->var);
+  desc->lim_alts = variable_initial_values (e, desc->lim);
+
+  /* Number of iterations. */
+  if (!count_loop_iterations (desc, NULL, NULL))
+    return false;
+  desc->const_iter =
+    constant_iterations (desc, &desc->niter, &desc->may_be_zero);
+  return true;
+}
+
+/* Tests whether LOOP is simple for loop.  Returns simple loop description
+   in DESC.  */
+bool
+simple_loop_p (loops, loop, desc)
+     struct loops *loops;
+     struct loop *loop;
+     struct loop_desc *desc;
+{
+  unsigned i;
+  basic_block *body;
+  edge e;
+  struct loop_desc act;
+  bool any = false;
+  regset invariant_regs;
+  regset_head invariant_regs_head;
+  rtx *single_set_regs;
+  int n_branches;
+  
+  body = get_loop_body (loop);
+
+  invariant_regs = INITIALIZE_REG_SET (invariant_regs_head);
+  single_set_regs = xmalloc (max_reg_num () * sizeof (rtx));
+
+  blocks_invariant_registers (body, loop->num_nodes, invariant_regs);
+  blocks_single_set_registers (body, loop->num_nodes, single_set_regs);
+
+  n_branches = 0;
+  for (i = 0; i < loop->num_nodes; i++)
+    {
+      for (e = body[i]->succ; e; e = e->succ_next)
+	if (!flow_bb_inside_loop_p (loop, e->dest)
+	    && simple_loop_exit_p (loops, loop, e,
+		   invariant_regs, single_set_regs, &act))
+	  {
+	    /* Prefer constant iterations; the less the better.  */
+	    if (!any)
+	      any = true;
+	    else if (!act.const_iter
+		     || (desc->const_iter && act.niter >= desc->niter))
+	      continue;
+	    *desc = act;
+	  }
+
+      if (body[i]->succ && body[i]->succ->succ_next)
+	n_branches++;
+    }
+  desc->n_branches = n_branches;
+
+  if (rtl_dump_file && any)
+    {
+      fprintf (rtl_dump_file, "; Simple loop %i\n", loop->num);
+      if (desc->postincr)
+	fprintf (rtl_dump_file,
+		 ";  does postincrement after loop exit condition\n");
+
+      fprintf (rtl_dump_file, ";  Induction variable:");
+      print_simple_rtl (rtl_dump_file, desc->var);
+      fputc ('\n', rtl_dump_file);
+
+      fprintf (rtl_dump_file, ";  Initial values:");
+      print_simple_rtl (rtl_dump_file, desc->var_alts);
+      fputc ('\n', rtl_dump_file);
+
+      fprintf (rtl_dump_file, ";  Stride:");
+      print_simple_rtl (rtl_dump_file, desc->stride);
+      fputc ('\n', rtl_dump_file);
+
+      fprintf (rtl_dump_file, ";  Compared with:");
+      print_simple_rtl (rtl_dump_file, desc->lim);
+      fputc ('\n', rtl_dump_file);
+
+      fprintf (rtl_dump_file, ";  Alternative values:");
+      print_simple_rtl (rtl_dump_file, desc->lim_alts);
+      fputc ('\n', rtl_dump_file);
+
+      fprintf (rtl_dump_file, ";  Exit condition:");
+      if (desc->neg)
+	fprintf (rtl_dump_file, "(negated)");
+      fprintf (rtl_dump_file, "%s\n", GET_RTX_NAME (desc->cond));
+
+      fprintf (rtl_dump_file, ";  Number of branches:");
+      fprintf (rtl_dump_file, "%d\n", desc->n_branches);
+
+      fputc ('\n', rtl_dump_file);
+    }
+
+  free (body);
+  FREE_REG_SET (invariant_regs);
+  free (single_set_regs);
+  return any;
+}
+
+/* Marks blocks that are part of non-reckognized loops; i.e. we throw away
+   all latch edges and mark blocks inside any remaining cycle.  Everything
+   is a bit complicated due to fact we do not want to do this for parts of
+   cycles that only "pass" through some loop -- i.e. for each cycle, we want
+   to mark blocks that belong directly to innermost loop containing the whole
+   cycle.  */
+void
+mark_irreducible_loops (loops)
+     struct loops *loops;
+{
+  int *dfs_in, *closed, *mr, *mri, *n_edges, *stack;
+  unsigned i;
+  edge **edges, e;
+  basic_block act;
+  int stack_top, tick, depth;
+  struct loop *cloop;
+
+  /* The first last_basic_block + 1 entries are for real blocks (including
+     entry); then we have loops->num - 1 fake blocks for loops to that we
+     assign edges leading from loops (fake loop 0 is not interesting).  */
+  dfs_in = xmalloc ((last_basic_block + loops->num) * sizeof (int));
+  closed = xmalloc ((last_basic_block + loops->num) * sizeof (int));
+  mr = xmalloc ((last_basic_block + loops->num) * sizeof (int));
+  mri = xmalloc ((last_basic_block + loops->num) * sizeof (int));
+  n_edges = xmalloc ((last_basic_block + loops->num) * sizeof (int));
+  edges = xmalloc ((last_basic_block + loops->num) * sizeof (edge *));
+  stack = xmalloc ((n_basic_blocks + loops->num) * sizeof (int));
+
+  /* Create the edge lists.  */
+  for (i = 0; i < last_basic_block + loops->num; i++)
+    n_edges[i] = 0;
+  FOR_BB_BETWEEN (act, ENTRY_BLOCK_PTR, EXIT_BLOCK_PTR, next_bb)
+    for (e = act->succ; e; e = e->succ_next)
+      {
+        /* Ignore edges to exit.  */
+        if (e->dest == EXIT_BLOCK_PTR)
+	  continue;
+	/* And latch edges.  */
+	if (e->dest->loop_father->header == e->dest
+	    && e->dest->loop_father->latch == act)
+	  continue;
+	/* Edges inside a single loop should be left where they are.  Edges
+	   to subloop headers should lead to representative of the subloop,
+	   but from the same place.  */
+	if (act->loop_father == e->dest->loop_father
+	    || act->loop_father == e->dest->loop_father->outer)
+	  {
+	    n_edges[act->index + 1]++;
+	    continue;
+	  }
+	/* Edges exiting loops remain.  They should lead from representative
+	   of the son of nearest common ancestor of the loops in that
+	   act lays.  */
+	depth = find_common_loop (act->loop_father, e->dest->loop_father)->depth + 1;
+	if (depth == act->loop_father->depth)
+	  cloop = act->loop_father;
+	else
+	  cloop = act->loop_father->pred[depth];
+	n_edges[cloop->num + last_basic_block]++;
+      }
+
+  for (i = 0; i < last_basic_block + loops->num; i++)
+    {
+      edges[i] = xmalloc (n_edges[i] * sizeof (edge));
+      n_edges[i] = 0;
+    }
+
+  FOR_BB_BETWEEN (act, ENTRY_BLOCK_PTR, EXIT_BLOCK_PTR, next_bb)
+    for (e = act->succ; e; e = e->succ_next)
+      {
+        if (e->dest == EXIT_BLOCK_PTR)
+	  continue;
+	if (e->dest->loop_father->header == e->dest
+	    && e->dest->loop_father->latch == act)
+	  continue;
+	if (act->loop_father == e->dest->loop_father
+	    || act->loop_father == e->dest->loop_father->outer)
+	  {
+	    edges[act->index + 1][n_edges[act->index + 1]++] = e;
+	    continue;
+	  }
+	depth = find_common_loop (act->loop_father, e->dest->loop_father)->depth + 1;
+	if (depth == act->loop_father->depth)
+	  cloop = act->loop_father;
+	else
+	  cloop = act->loop_father->pred[depth];
+	i = cloop->num + last_basic_block;
+	edges[i][n_edges[i]++] = e;
+      }
+
+  /* Compute dfs numbering, starting from loop headers, and mark found
+     loops.*/
+  tick = 0;
+  for (i = 0; i < last_basic_block + loops->num; i++)
+    {
+      dfs_in[i] = -1;
+      closed[i] = 0;
+      mr[i] = last_basic_block + loops->num;
+      mri[i] = -1;
+    }
+
+  stack_top = 0;
+  for (i = 0; i < loops->num; i++)
+    if (loops->parray[i])
+      stack[stack_top++] = loops->parray[i]->header->index + 1;
+
+  while (stack_top)
+    {
+      int idx, sidx;
+
+      idx = stack[stack_top - 1];
+      if (dfs_in[idx] < 0)
+	dfs_in[idx] = tick++;
+
+      while (n_edges[idx])
+	{
+	  e = edges[idx][--n_edges[idx]];
+	  sidx = e->dest->loop_father->header == e->dest
+	           ? e->dest->loop_father->num + last_basic_block
+	           : e->dest->index + 1;
+          if (closed[sidx])
+	    {
+	      if (mr[sidx] < mr[idx] && !closed[mri[sidx]])
+		{
+		  mr[idx] = mr[sidx];
+		  mri[idx] = mri[sidx];
+		}
+	      continue;
+	    }
+	  if (dfs_in[sidx] < 0)
+	    {
+	      stack[stack_top++] = sidx;
+	      goto next;
+	    }
+	  if (dfs_in[sidx] < mr[idx])
+	    {
+	      mr[idx] = dfs_in[sidx];
+	      mri[idx] = sidx;
+	    }
+	}
+
+      /* Return back.  */
+      closed[idx] = 1;
+      stack_top--;
+      if (stack_top && dfs_in[stack[stack_top - 1]] >= 0)
+        {
+	  /* Propagate information back.  */
+	  sidx = stack[stack_top - 1];
+	  if (mr[sidx] > mr[idx])
+	    {
+	      mr[sidx] = mr[idx];
+	      mri[sidx] = mri[idx];
+	    }
+	}
+      /* Mark the block if relevant.  */
+      if (idx && idx <= last_basic_block && mr[idx] <= dfs_in[idx])
+        BASIC_BLOCK (idx - 1)->flags |= BB_IRREDUCIBLE_LOOP;
+next:;
+    }
+
+  free (stack);
+  free (dfs_in);
+  free (closed);
+  free (mr);
+  free (mri);
+  for (i = 0; i < last_basic_block + loops->num; i++)
+    free (edges[i]);
+  free (edges);
+  free (n_edges);
+  loops->state |= LOOPS_HAVE_MARKED_IRREDUCIBLE_REGIONS;
+}
+
+/* Counts number of insns inside LOOP.  */
+int
+num_loop_insns (loop)
+     struct loop *loop;
+{
+  basic_block *bbs, bb;
+  unsigned i, ninsns = 0;
+  rtx insn;
+
+  bbs = get_loop_body (loop);
+  for (i = 0; i < loop->num_nodes; i++)
+    {
+      bb = bbs[i];
+      ninsns++;
+      for (insn = bb->head; insn != bb->end; insn = NEXT_INSN (insn))
+	ninsns++;
+    }
+  free(bbs);
+  
+  return ninsns;
+}
+
+/* Returns expected number of LOOP iterations.
+   Comput upper bound on number of iterations in case they do not fit integer
+   to help loop peeling heuristics.  Use exact counts if at all possible.  */
+unsigned
+expected_loop_iterations (loop)
+     const struct loop *loop;
+{
+  edge e;
+
+  if (loop->header->count)
+    {
+      gcov_type count_in, count_latch, expected;
+
+      count_in = 0;
+      count_latch = 0;
+
+      for (e = loop->header->pred; e; e = e->pred_next)
+	if (e->src == loop->latch)
+	  count_latch = e->count;
+	else
+	  count_in += e->count;
+
+      if (count_in == 0)
+	return 0;
+
+      expected = (count_latch + count_in - 1) / count_in;
+
+      /* Avoid overflows.  */
+      return (expected > REG_BR_PROB_BASE ? REG_BR_PROB_BASE : expected);
+    }
+  else
+    {
+      int freq_in, freq_latch;
+
+      freq_in = 0;
+      freq_latch = 0;
+
+      for (e = loop->header->pred; e; e = e->pred_next)
+	if (e->src == loop->latch)
+	  freq_latch = EDGE_FREQUENCY (e);
+	else
+	  freq_in += EDGE_FREQUENCY (e);
+
+      if (freq_in == 0)
+	return 0;
+
+      return (freq_latch + freq_in - 1) / freq_in;
+    }
+}
--- gcc-3.3.1/gcc/cfgloopmanip.c.hammer-branch	2003-08-05 18:22:46.000000000 +0200
+++ gcc-3.3.1/gcc/cfgloopmanip.c	2003-08-05 18:22:46.000000000 +0200
@@ -0,0 +1,1342 @@
+/* Loop manipulation code for GNU compiler.
+   Copyright (C) 2002 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+#include "config.h"
+#include "system.h"
+#include "rtl.h"
+#include "hard-reg-set.h"
+#include "basic-block.h"
+#include "cfgloop.h"
+#include "cfglayout.h"
+#include "output.h"
+
+static struct loop * duplicate_loop	PARAMS ((struct loops *,
+						struct loop *, struct loop *, int));
+static void duplicate_subloops		PARAMS ((struct loops *, struct loop *,
+						struct loop *, int));
+static void copy_loops_to		PARAMS ((struct loops *, struct loop **,
+						int, struct loop *, int));
+static void loop_redirect_edge		PARAMS ((edge, basic_block));
+static bool loop_delete_branch_edge	PARAMS ((edge));
+static void copy_bbs			PARAMS ((basic_block *, int, edge,
+						edge, basic_block **,
+						struct loops *, edge *,
+						edge *, int));
+static void remove_bbs			PARAMS ((dominance_info, basic_block *,
+						int));
+static bool rpe_enum_p			PARAMS ((basic_block, void *));
+static int find_branch			PARAMS ((edge, dominance_info,
+						basic_block **));
+static bool alp_enum_p			PARAMS ((basic_block, void *));
+static void add_loop			PARAMS ((struct loops *, struct loop *));
+static void fix_loop_placements		PARAMS ((struct loop *));
+static int fix_bb_placement		PARAMS ((struct loops *, basic_block));
+static void fix_bb_placements		PARAMS ((struct loops *, basic_block));
+static void place_new_loop		PARAMS ((struct loops *, struct loop *));
+static void scale_loop_frequencies	PARAMS ((struct loop *, int, int));
+static void scale_bbs_frequencies	PARAMS ((basic_block *, int, int, int));
+static void record_exit_edges		PARAMS ((edge, basic_block *, int,
+						edge *, unsigned *, int));
+static basic_block create_preheader	PARAMS ((struct loop *, dominance_info,
+						int));
+
+/* Splits basic block BB after INSN, returns created edge.  Updates loops
+   and dominators.  */
+edge
+split_loop_bb (loops, bb, insn)
+     struct loops *loops;
+     basic_block bb;
+     rtx insn;
+{
+  edge e;
+  basic_block *dom_bbs;
+  int n_dom_bbs, i;
+
+  /* Split the block.  */
+  e = split_block (bb, insn);
+
+  /* Add dest to loop.  */
+  add_bb_to_loop (e->dest, e->src->loop_father);
+
+  /* Fix dominators.  */
+  add_to_dominance_info (loops->cfg.dom, e->dest);
+  n_dom_bbs = get_dominated_by (loops->cfg.dom, e->src, &dom_bbs);
+  for (i = 0; i < n_dom_bbs; i++)
+    set_immediate_dominator (loops->cfg.dom, dom_bbs[i], e->dest);
+  free (dom_bbs);
+  set_immediate_dominator (loops->cfg.dom, e->dest, e->src);
+
+  /* Take care of RBI.  */
+  alloc_aux_for_block (e->dest, sizeof (struct reorder_block_def));
+
+  return e;
+}
+
+/* Checks whether BB is inside RPE_LOOP and is dominated by RPE_DOM.  */
+struct rpe_data
+ {
+   basic_block dom;
+   dominance_info doms;
+ };
+
+static bool
+rpe_enum_p (bb, data)
+     basic_block bb;
+     void *data;
+{
+  struct rpe_data *rpe = data;
+  return dominated_by_p (rpe->doms, bb, rpe->dom);
+}
+
+/* Remove BBS.  */
+static void
+remove_bbs (dom, bbs, nbbs)
+     dominance_info dom;
+     basic_block *bbs;
+     int nbbs;
+{
+  int i;
+  edge ae;
+
+  for (i = 0; i < nbbs; i++)
+    {
+      edge next_ae;
+      for (ae = bbs[i]->succ; ae; ae = next_ae)
+	{
+	  next_ae = ae->succ_next;
+	  remove_edge (ae);
+	}
+      remove_bb_from_loops (bbs[i]);
+      delete_from_dominance_info (dom, bbs[i]);
+      flow_delete_block (bbs[i]);
+    }
+}
+
+/* Find branch beginning at Edge and put it into BBS.  */
+static int
+find_branch (e, doms, bbs)
+     edge e;
+     dominance_info doms;
+     basic_block **bbs;
+{
+  edge ae = NULL;
+  struct rpe_data rpe;
+
+  if (e->dest->pred->pred_next)
+    {
+      for (ae = e->dest->pred; ae; ae = ae->pred_next)
+	if (ae != e && !dominated_by_p (doms, ae->src, e->dest))
+	  break;
+    }
+  if (ae)
+    {
+      /* Just the edge.  */
+      *bbs = NULL;
+      return 0;
+    }
+
+  /* Find bbs we are interested in.  */
+  rpe.dom = e->dest;
+  rpe.doms = doms;
+  *bbs = xcalloc (n_basic_blocks, sizeof (basic_block));
+  return dfs_enumerate_from (e->dest, 0, rpe_enum_p, *bbs,
+			     n_basic_blocks, &rpe);
+}
+
+/* Fix BB placement inside loop hierarchy.  */
+static int
+fix_bb_placement (loops, bb)
+     struct loops *loops;
+     basic_block bb;
+{
+  edge e;
+  struct loop *loop = loops->tree_root, *act;
+
+  for (e = bb->succ; e; e = e->succ_next)
+    {
+      if (e->dest == EXIT_BLOCK_PTR)
+	continue;
+
+      act = e->dest->loop_father;
+      if (act->header == e->dest)
+	act = act->outer;
+
+      if (flow_loop_nested_p (loop, act))
+	loop = act;
+    }
+
+  if (loop == bb->loop_father)
+    return 0;
+
+  remove_bb_from_loops (bb);
+  add_bb_to_loop (bb, loop);
+
+  return 1;
+}
+
+/* Fix bb placements, starting from FROM.  Also fix placement of subloops
+   of FROM->loop_father.  */
+static void
+fix_bb_placements (loops, from)
+     struct loops *loops;
+     basic_block from;
+{
+  sbitmap in_queue;
+  basic_block *queue, *qtop, *qbeg, *qend;
+  struct loop *base_loop;
+  edge e;
+
+  /* We pass through blocks backreachable from FROM, testing whether some
+     of their successors moved to outer loop.  It may be neccessary to
+     iterate several times, but it is finite, as we stop unless we move
+     the basic block up the loop structure.  The whole story is a bit
+     more complicated due to presence of subloops, those are moved using
+     fix_loop_placement.  */
+
+  base_loop = from->loop_father;
+  if (base_loop == loops->tree_root)
+    return;
+
+  in_queue = sbitmap_alloc (last_basic_block);
+  sbitmap_zero (in_queue);
+  SET_BIT (in_queue, from->index);
+  /* Prevent us from going out of the base_loop.  */
+  SET_BIT (in_queue, base_loop->header->index);
+
+  queue = xcalloc (base_loop->num_nodes + 1, sizeof (basic_block));
+  qtop = queue + base_loop->num_nodes + 1;
+  qbeg = queue;
+  qend = queue + 1;
+  *qbeg = from;
+
+  while (qbeg != qend)
+    {
+      /* Get element from queue.  */
+      from = *qbeg;
+      qbeg++;
+      if (qbeg == qtop)
+	qbeg = queue;
+      RESET_BIT (in_queue, from->index);
+
+      if (from->loop_father->header == from)
+	{
+	  /* Subloop header, maybe move the loop upwards.  */
+	  if (!fix_loop_placement (from->loop_father))
+	    continue;
+	}
+      else
+	{
+	  if (!fix_bb_placement (loops, from))
+	    continue;
+	}
+
+      /* Something has changed, insert predecessors into queue.  */
+      for (e = from->pred; e; e = e->pred_next)
+	{
+	  basic_block pred = e->src;
+	  struct loop *nca;
+
+	  if (TEST_BIT (in_queue, pred->index))
+	    continue;
+
+	  /* If it is subloop, then it either was not moved, or 
+	     the path up the loop tree from base_loop do not contain
+	     it.  */
+	  nca = find_common_loop (pred->loop_father, base_loop);
+	  if (pred->loop_father != base_loop
+	      && (nca == base_loop
+		  || nca != pred->loop_father))
+	    pred = pred->loop_father->header;
+	  else if (!flow_loop_nested_p (from->loop_father, pred->loop_father))
+	    {
+	      /* No point in processing it.  */
+	      continue;
+	    }
+
+	  if (TEST_BIT (in_queue, pred->index))
+	    continue;
+
+	  /* Schedule the basic block.  */
+	  *qend = pred;
+	  qend++;
+	  if (qend == qtop)
+	    qend = queue;
+	  SET_BIT (in_queue, pred->index);
+	}
+    }
+}
+
+/* Removes path beginning at E.  */
+bool
+remove_path (loops, e)
+     struct loops *loops;
+     edge e;
+{
+  edge ae;
+  basic_block *rem_bbs, *bord_bbs, *dom_bbs, from, bb;
+  int i, nrem, n_bord_bbs, n_dom_bbs;
+  sbitmap seen;
+
+  /* First identify the branch.  */
+  nrem = find_branch (e, loops->cfg.dom, &rem_bbs);
+
+  /* Find blocks whose immediate dominators may be affected.  */
+  n_dom_bbs = 0;
+  n_bord_bbs = 0;
+  bord_bbs = xcalloc (n_basic_blocks, sizeof (basic_block));
+  seen = sbitmap_alloc (last_basic_block);
+  sbitmap_zero (seen);
+
+  /* Find border hexes.  */
+  for (i = 0; i < nrem; i++)
+    SET_BIT (seen, rem_bbs[i]->index);
+  if (nrem)
+    {
+      for (i = 0; i < nrem; i++)
+	{
+	  bb = rem_bbs[i];
+	  for (ae = rem_bbs[i]->succ; ae; ae = ae->succ_next)
+	    if (ae->dest != EXIT_BLOCK_PTR && !TEST_BIT (seen, ae->dest->index))
+	      {
+		SET_BIT (seen, ae->dest->index);
+		bord_bbs[n_bord_bbs++] = ae->dest;
+	      }
+	}
+    }
+  else if (e->dest != EXIT_BLOCK_PTR)
+    bord_bbs[n_bord_bbs++] = e->dest;
+
+  /* OK. Remove the path.  */
+  from = e->src;
+  if (!loop_delete_branch_edge (e))
+    {
+      free (rem_bbs);
+      free (bord_bbs);
+      free (seen);
+      return false;
+    }
+  dom_bbs = xcalloc (n_basic_blocks, sizeof (basic_block));
+
+  /* Now cancel contained loops.  */
+  for (i = 0; i < nrem; i++)
+    if (rem_bbs[i]->loop_father->header == rem_bbs[i])
+      cancel_loop_tree (loops, rem_bbs[i]->loop_father);
+
+  remove_bbs (loops->cfg.dom, rem_bbs, nrem);
+  free (rem_bbs);
+
+  /* Find blocks with affected dominators.  */
+  sbitmap_zero (seen);
+  for (i = 0; i < n_bord_bbs; i++)
+    {
+      int j, nldom;
+      basic_block *ldom;
+
+      bb = get_immediate_dominator (loops->cfg.dom, bord_bbs[i]);
+      if (TEST_BIT (seen, bb->index))
+	continue;
+      SET_BIT (seen, bb->index);
+
+      nldom = get_dominated_by (loops->cfg.dom, bb, &ldom);
+      for (j = 0; j < nldom; j++)
+	if (!dominated_by_p (loops->cfg.dom, from, ldom[j]))
+	  dom_bbs[n_dom_bbs++] = ldom[j];
+      free(ldom);
+    }
+
+  free (bord_bbs);
+  free (seen);
+
+  /* Recount dominators.  */
+  iterate_fix_dominators (loops->cfg.dom, dom_bbs, n_dom_bbs);
+  free (dom_bbs);
+
+  /* Fix placements of basic blocks inside loops.  */
+  fix_bb_placements (loops, from);
+
+  /* Fix loop placements.  */
+  fix_loop_placements (from->loop_father);
+
+  return true;
+}
+
+/* Predicate for enumeration in add_loop.  */
+static bool
+alp_enum_p (bb, alp_header)
+     basic_block bb;
+     void *alp_header;
+{
+  return bb != (basic_block) alp_header;
+}
+
+/* Compute loop from header and latch info filled in LOOP and add it to
+   LOOPS.  */
+static void
+add_loop (loops, loop)
+     struct loops *loops;
+     struct loop *loop;
+{
+  basic_block *bbs;
+  int i, n;
+  
+  /* Add it to loop structure.  */
+  place_new_loop (loops, loop);
+  loop->level = 1;
+
+  /* Find its nodes.  */
+  bbs = xcalloc (n_basic_blocks, sizeof (basic_block));
+  n = dfs_enumerate_from (loop->latch, 1, alp_enum_p, bbs, n_basic_blocks, loop->header);
+
+  /* Add those nodes.  */
+  for (i = 0; i < n; i++)
+    add_bb_to_loop (bbs[i], loop);
+  add_bb_to_loop (loop->header, loop);
+
+  free (bbs);
+}
+
+/* Multiplies all frequencies of BBS by NUM/DEN.  */
+static void
+scale_bbs_frequencies (bbs, nbbs, num, den)
+     basic_block *bbs;
+     int nbbs;
+     int num;
+     int den;
+{
+  int i;
+  edge e;
+
+  for (i = 0; i < nbbs; i++)
+    {
+      bbs[i]->frequency = (bbs[i]->frequency * num) / den;
+      bbs[i]->count = (bbs[i]->count * num) / den;
+      for (e = bbs[i]->succ; e; e = e->succ_next)
+	e->count = (e->count * num) /den;
+    }
+}
+
+/* Multiplies all frequencies in LOOP by NUM/DEN.  */
+static void
+scale_loop_frequencies (loop, num, den)
+     struct loop *loop;
+     int num;
+     int den;
+{
+  basic_block *bbs;
+
+  bbs = get_loop_body (loop);
+  scale_bbs_frequencies (bbs, loop->num_nodes, num, den);
+  free (bbs);
+}
+
+/* Make area between HEADER_EDGE and LATCH_EDGE a loop by connecting
+   latch to header.  Everything between them plus LATCH_EDGE destination
+   must be dominated by HEADER_EDGE destination, and backreachable from
+   LATCH_EDGE source.  Add SWITCH_BB to original entry edge.  Returns newly
+   created loop.  */
+struct loop *
+loopify (loops, latch_edge, header_edge, switch_bb)
+     struct loops *loops;
+     edge latch_edge;
+     edge header_edge;
+     basic_block switch_bb;
+{
+  basic_block succ_bb = latch_edge->dest;
+  basic_block pred_bb = header_edge->src;
+  basic_block *dom_bbs, *body;
+  unsigned n_dom_bbs, i, j;
+  sbitmap seen;
+  struct loop *loop = xcalloc (1, sizeof (struct loop));
+  struct loop *outer = succ_bb->loop_father->outer;
+  int freq, prob, tot_prob;
+  gcov_type cnt;
+
+  loop->header = header_edge->dest;
+  loop->latch = latch_edge->src;
+
+  freq = EDGE_FREQUENCY (header_edge);
+  cnt = header_edge->count;
+  prob = switch_bb->succ->probability;
+  tot_prob = prob + switch_bb->succ->succ_next->probability;
+  if (tot_prob == 0)
+    tot_prob = 1;
+
+  /* Redirect edges.  */
+  loop_redirect_edge (latch_edge, loop->header);
+  loop_redirect_edge (header_edge, switch_bb);
+  loop_redirect_edge (switch_bb->succ->succ_next, loop->header);
+  loop_redirect_edge (switch_bb->succ, succ_bb);
+
+  /* And update dominators.  */
+  set_immediate_dominator (loops->cfg.dom, switch_bb, pred_bb);
+  set_immediate_dominator (loops->cfg.dom, loop->header, switch_bb);
+  set_immediate_dominator (loops->cfg.dom, succ_bb, switch_bb);
+
+  /* Compute new loop.  */
+  add_loop (loops, loop);
+  flow_loop_tree_node_add (outer, loop);
+
+  /* And add switch_bb to appropriate loop.  */
+  add_bb_to_loop (switch_bb, outer);
+
+  /* Now fix frequencies.  */
+  switch_bb->frequency = freq;
+  switch_bb->count = cnt;
+  switch_bb->succ->count =
+   (switch_bb->count * switch_bb->succ->probability) / REG_BR_PROB_BASE;
+  switch_bb->succ->succ_next->count =
+   (switch_bb->count * switch_bb->succ->succ_next->probability) / REG_BR_PROB_BASE;
+  scale_loop_frequencies (loop, prob, tot_prob);
+  scale_loop_frequencies (succ_bb->loop_father, tot_prob - prob, tot_prob);
+
+  /* Update dominators of outer blocks.  */
+  dom_bbs = xcalloc (n_basic_blocks, sizeof (basic_block));
+  n_dom_bbs = 0;
+  seen = sbitmap_alloc (last_basic_block);
+  sbitmap_zero (seen);
+  body = get_loop_body (loop);
+
+  for (i = 0; i < loop->num_nodes; i++)
+    SET_BIT (seen, body[i]->index);
+
+  for (i = 0; i < loop->num_nodes; i++)
+    {
+      unsigned nldom;
+      basic_block *ldom;
+
+      nldom = get_dominated_by (loops->cfg.dom, body[i], &ldom);
+      for (j = 0; j < nldom; j++)
+	if (!TEST_BIT (seen, ldom[j]->index))
+	  {
+	    SET_BIT (seen, ldom[j]->index);
+	    dom_bbs[n_dom_bbs++] = ldom[j];
+	  }
+      free (ldom);
+    }
+
+  iterate_fix_dominators (loops->cfg.dom, dom_bbs, n_dom_bbs);
+
+  free (body);
+  free (seen);
+  free (dom_bbs);
+
+  return loop;
+}
+
+/* Move LOOP up the hierarchy while it is not backward reachable from the
+   latch of the outer loop.  */
+int
+fix_loop_placement (loop)
+     struct loop *loop;
+{
+  basic_block *body;
+  unsigned i;
+  edge e;
+  struct loop *father = loop->pred[0], *act;
+
+  body = get_loop_body (loop);
+  for (i = 0; i < loop->num_nodes; i++)
+    for (e = body[i]->succ; e; e = e->succ_next)
+      if (!flow_bb_inside_loop_p (loop, e->dest))
+	{
+	  act = find_common_loop (loop, e->dest->loop_father);
+	  if (flow_loop_nested_p (father, act))
+	    father = act;
+	}
+  free (body);
+
+  if (father != loop->outer)
+    {
+      for (act = loop->outer; act != father; act = act->outer)
+	act->num_nodes -= loop->num_nodes;
+      flow_loop_tree_node_remove (loop);
+      flow_loop_tree_node_add (father, loop);
+      return 1;
+    }
+  return 0;
+}
+
+/* Fix placement of superloops of LOOP.  */
+static void
+fix_loop_placements (loop)
+     struct loop *loop;
+{
+  struct loop *outer;
+
+  while (loop->outer)
+    {
+      outer = loop->outer;
+      if (!fix_loop_placement (loop))
+        break;
+      loop = outer;
+    }
+}
+
+/* Creates place for a new LOOP.  */
+static void
+place_new_loop (loops, loop)
+     struct loops *loops;
+     struct loop *loop;
+{
+  loops->parray =
+    xrealloc (loops->parray, (loops->num + 1) * sizeof (struct loop *));
+  loops->parray[loops->num] = loop;
+
+  loop->num = loops->num++;
+}
+
+/* Copies structure of LOOP into TARGET, scaling histogram by PROB.  */
+static struct loop *
+duplicate_loop (loops, loop, target, prob)
+     struct loops *loops;
+     struct loop *loop;
+     struct loop *target;
+     int prob;
+{
+  struct loop *cloop;
+  cloop = xcalloc (1, sizeof (struct loop));
+  place_new_loop (loops, cloop);
+
+  /* Initialize copied loop.  */
+  cloop->level = loop->level;
+
+  /* Set it as copy of loop.  */
+  loop->copy = cloop;
+
+  /* Add it to target.  */
+  flow_loop_tree_node_add (target, cloop);
+
+  return cloop;
+}
+
+/* Copies structure of subloops of LOOP into TARGET; histograms are scaled by PROB.  */
+static void 
+duplicate_subloops (loops, loop, target, prob)
+     struct loops *loops;
+     struct loop *loop;
+     struct loop *target;
+     int prob;
+{
+  struct loop *aloop, *cloop;
+
+  for (aloop = loop->inner; aloop; aloop = aloop->next)
+    {
+      cloop = duplicate_loop (loops, aloop, target, prob);
+      duplicate_subloops (loops, aloop, cloop, prob);
+    }
+}
+
+/*  Copies structure of COPIED_LOOPS into TARGET.  */
+static void 
+copy_loops_to (loops, copied_loops, n, target, prob)
+     struct loops *loops;
+     struct loop **copied_loops;
+     int n;
+     struct loop *target;
+     int prob;
+{
+  struct loop *aloop;
+  int i;
+
+  for (i = 0; i < n; i++)
+    {
+      aloop = duplicate_loop (loops, copied_loops[i], target, prob);
+      duplicate_subloops (loops, copied_loops[i], aloop, prob);
+    }
+}
+
+/* Redirects edge E to DEST.  */
+static void
+loop_redirect_edge (e, dest)
+     edge e;
+     basic_block dest;
+{
+  if (e->dest == dest)
+    return;
+
+  cfg_layout_redirect_edge (e, dest);
+}
+
+/* Deletes edge if possible.  */
+static bool
+loop_delete_branch_edge (e)
+     edge e;
+{
+  basic_block src = e->src;
+
+  if (src->succ->succ_next)
+    {
+      basic_block newdest;
+      /* Cannot handle more than two exit edges.  */
+      if (src->succ->succ_next->succ_next)
+	return false;
+      /* Neither this.  */
+      if (!any_condjump_p (src->end))
+	return false;
+
+      newdest = (e == src->succ
+		 ? src->succ->succ_next->dest : src->succ->dest);
+      if (newdest == EXIT_BLOCK_PTR)
+	return false;
+
+      return cfg_layout_redirect_edge (e, newdest);
+    }
+  else
+    {
+      /* Cannot happen -- we are duplicating loop! */
+      abort ();
+    }
+
+  return false;  /* To avoid warning, cannot get here.  */
+}
+
+/* Duplicates BBS. Newly created bbs are placed into NEW_BBS, edges to
+   header (target of ENTRY) and copy of header are returned, edge ENTRY
+   is redirected to header copy.  Assigns bbs into loops, updates
+   dominators.  If ADD_IRREDUCIBLE_FLAG, basic blocks that are not
+   member of any inner loop are marked irreducible.  */
+static void
+copy_bbs (bbs, n, entry, latch_edge, new_bbs, loops, header_edge, copy_header_edge, add_irreducible_flag)
+     basic_block *bbs;
+     int n;
+     edge entry;
+     edge latch_edge;
+     basic_block **new_bbs;
+     struct loops *loops;
+     edge *header_edge;
+     edge *copy_header_edge;
+     int add_irreducible_flag;
+{
+  int i;
+  basic_block bb, new_bb, header = entry->dest, dom_bb;
+  edge e;
+
+  /* Duplicate bbs, update dominators, assign bbs to loops.  */
+  (*new_bbs) = xcalloc (n, sizeof (basic_block));
+  for (i = 0; i < n; i++)
+    {
+      /* Duplicate.  */
+      bb = bbs[i];
+      new_bb = (*new_bbs)[i] = cfg_layout_duplicate_bb (bb, NULL);
+      RBI (new_bb)->duplicated = 1;
+      /* Add to loop.  */
+      add_bb_to_loop (new_bb, bb->loop_father->copy);
+      add_to_dominance_info (loops->cfg.dom, new_bb);
+      /* Possibly set header.  */
+      if (bb->loop_father->header == bb && bb != header)
+	new_bb->loop_father->header = new_bb;
+      /* Or latch.  */
+      if (bb->loop_father->latch == bb &&
+	  bb->loop_father != header->loop_father)
+	new_bb->loop_father->latch = new_bb;
+      /* Take care of irreducible loops.  */
+      if (add_irreducible_flag
+	  && bb->loop_father == header->loop_father)
+	new_bb->flags |= BB_IRREDUCIBLE_LOOP;
+    }
+
+  /* Set dominators.  */
+  for (i = 0; i < n; i++)
+    {
+      bb = bbs[i];
+      new_bb = (*new_bbs)[i];
+      if (bb != header)
+	{
+	  /* For anything else than loop header, just copy it.  */
+	  dom_bb = get_immediate_dominator (loops->cfg.dom, bb);
+	  dom_bb = RBI (dom_bb)->copy;
+	}
+      else
+	{
+	  /* Copy of header is dominated by entry source.  */
+	  dom_bb = entry->src;
+	}
+      if (!dom_bb)
+	abort ();
+      set_immediate_dominator (loops->cfg.dom, new_bb, dom_bb);
+    }
+
+  /* Redirect edges.  */
+  for (i = 0; i < n; i++)
+    {
+      edge e_pred;
+      new_bb = (*new_bbs)[i];
+      bb = bbs[i];
+      for (e = bb->pred; e; e = e_pred)
+	{
+	  basic_block src = e->src;
+
+	  e_pred = e->pred_next;
+	  
+	  /* Does this edge interest us? */
+	  if (!RBI (src)->duplicated)
+	    continue;
+
+	  /* So it interests us; redirect it.  */
+	  if (bb != header)
+	    loop_redirect_edge (e, new_bb);
+	}
+    }
+
+  /* Redirect header edge.  */
+  bb = RBI (latch_edge->src)->copy;
+  for (e = bb->succ; e->dest != latch_edge->dest; e = e->succ_next);
+  *header_edge = e;
+  loop_redirect_edge (*header_edge, header);
+
+  /* Redirect entry to copy of header.  */
+  loop_redirect_edge (entry, RBI (header)->copy);
+  *copy_header_edge = entry;
+
+  /* Cancel duplicated flags.  */
+  for (i = 0; i < n; i++)
+   RBI ((*new_bbs)[i])->duplicated = 0;
+}
+
+/* Check whether LOOP's body can be duplicated.  */
+bool
+can_duplicate_loop_p (loop)
+     struct loop *loop;
+{
+  basic_block *bbs;
+  unsigned i;
+
+  bbs = get_loop_body (loop);
+
+  for (i = 0; i < loop->num_nodes; i++)
+    {
+      edge e;
+
+      /* In case loop contains abnormal edge we can not redirect,
+         we can't perform duplication.  */
+
+      for (e = bbs[i]->succ; e; e = e->succ_next)
+	if ((e->flags & EDGE_ABNORMAL)
+	    && flow_bb_inside_loop_p (loop, e->dest))
+	  {
+	    free (bbs);
+	    return false;
+	  }
+
+      if (!cfg_layout_can_duplicate_bb_p (bbs[i]))
+	{
+	  free (bbs);
+	  return false;
+	}
+    }
+  free (bbs);
+
+  return true;
+}
+
+/* Store edges created by copying ORIG edge (simply this ORIG edge if IS_ORIG)
+   (if ORIG is NULL, then all exit edges) into TO_REMOVE array.  */
+static void
+record_exit_edges (orig, bbs, nbbs, to_remove, n_to_remove, is_orig)
+     edge orig;
+     basic_block *bbs;
+     int nbbs;
+     edge *to_remove;
+     unsigned *n_to_remove;
+     int is_orig;
+{
+  sbitmap my_blocks;
+  int i;
+  edge e;
+
+  if (orig)
+    {
+      if (is_orig)
+	{
+	  to_remove[(*n_to_remove)++] = orig;
+	  return;
+	}
+
+      for (e = RBI (orig->src)->copy->succ; e; e = e->succ_next)
+	if (e->dest == orig->dest)
+	  break;
+      if (!e)
+	abort ();
+
+      to_remove[(*n_to_remove)++] = e;
+    }
+  else
+    {
+      my_blocks = sbitmap_alloc (last_basic_block);
+      sbitmap_zero (my_blocks);
+      for (i = 0; i < nbbs; i++)
+        SET_BIT (my_blocks, bbs[i]->index);
+
+      for (i = 0; i < nbbs; i++)
+	for (e = bbs[i]->succ; e; e = e->succ_next)
+	  if (e->dest == EXIT_BLOCK_PTR ||
+	      !TEST_BIT (my_blocks, e->dest->index))
+	    to_remove[(*n_to_remove)++] = e;
+
+      free (my_blocks);
+    }
+}
+
+
+#define RDIV(X,Y) (((X) + (Y) / 2) / (Y))
+
+/* Duplicates body of LOOP to given edge E NDUPL times.  Takes care of
+   updating LOOPS structure.  E's destination must be LOOP header for this to
+   work.  Store edges created by copying ORIG edge (if NULL, then all exit
+   edges) from copies corresponding to set bits in WONT_EXIT (bit 0 corresponds
+   to original LOOP body) into TO_REMOVE array.  Returns false if duplication
+   is impossible.  */
+int
+duplicate_loop_to_header_edge (loop, e, loops, ndupl, wont_exit, orig,
+			       to_remove, n_to_remove, flags)
+     struct loop *loop;
+     edge e;
+     struct loops *loops;
+     unsigned ndupl;
+     sbitmap wont_exit;
+     edge orig;
+     edge *to_remove;
+     unsigned *n_to_remove;
+     int flags;
+{
+  struct loop *target, *aloop;
+  struct loop **orig_loops;
+  unsigned n_orig_loops;
+  basic_block header = loop->header, latch = loop->latch;
+  basic_block *new_bbs, *bbs, *first_active;
+  basic_block new_bb, bb, first_active_latch = NULL;
+  edge ae, latch_edge, he;
+  unsigned i, j, n;
+  int is_latch = (latch == e->src);
+  int scale_act, *scale_step, scale_main, p, freq_in, freq_le, freq_out_orig;
+  int prob_pass_thru, prob_pass_wont_exit, prob_pass_main;
+  int add_irreducible_flag;
+
+  if (e->dest != loop->header)
+    abort ();
+  if (ndupl <= 0)
+    abort ();
+
+  if (orig)
+    {
+      /* Orig must be edge out of the loop.  */
+      if (!flow_bb_inside_loop_p (loop, orig->src))
+	abort ();
+      if (flow_bb_inside_loop_p (loop, orig->dest))
+	abort ();
+    }
+
+  bbs = get_loop_body (loop);
+
+  /* Check whether duplication is possible.  */
+
+  for (i = 0; i < loop->num_nodes; i++)
+    {
+      if (!cfg_layout_can_duplicate_bb_p (bbs[i]))
+	{
+	  free (bbs);
+	  return false;
+	}
+    }
+
+  add_irreducible_flag = !is_latch && (e->src->flags & BB_IRREDUCIBLE_LOOP);
+
+  /* Find edge from latch.  */
+  latch_edge = loop_latch_edge (loop);
+
+  if (flags & DLTHE_FLAG_UPDATE_FREQ)
+    {
+      /* For updating frequencies.  */
+      freq_in = header->frequency;
+      freq_le = EDGE_FREQUENCY (latch_edge);
+      if (freq_in == 0)
+	freq_in = 1;
+      if (freq_in < freq_le)
+	freq_in = freq_le;
+      freq_out_orig = orig ? EDGE_FREQUENCY (orig) : freq_in - freq_le;
+      if (freq_out_orig > freq_in - freq_le)
+	freq_out_orig = freq_in - freq_le;
+      prob_pass_thru = RDIV (REG_BR_PROB_BASE * freq_le, freq_in);
+      prob_pass_wont_exit = RDIV (REG_BR_PROB_BASE * (freq_le + freq_out_orig), freq_in);
+
+      scale_step = xmalloc (ndupl * sizeof (int));
+
+      switch (DLTHE_PROB_UPDATING (flags))
+	{
+	  case DLTHE_USE_WONT_EXIT:
+	    for (i = 1; i <= ndupl; i++)
+	      scale_step[i - 1] = TEST_BIT (wont_exit, i) ? prob_pass_wont_exit : prob_pass_thru;
+	    break;
+
+	  default:
+	    abort ();
+	}
+
+      if (is_latch)
+	{
+	  prob_pass_main = TEST_BIT (wont_exit, 0) ? prob_pass_wont_exit : prob_pass_thru;
+	  p = prob_pass_main;
+	  scale_main = REG_BR_PROB_BASE;
+	  for (i = 0; i < ndupl; i++)
+	    {
+	      scale_main += p;
+	      p = RDIV (p * scale_step[i], REG_BR_PROB_BASE);
+	    }
+	  scale_main = RDIV (REG_BR_PROB_BASE * REG_BR_PROB_BASE, scale_main);
+	  scale_act = RDIV (scale_main * prob_pass_main, REG_BR_PROB_BASE);
+	}
+      else
+	{
+	  scale_main = REG_BR_PROB_BASE;
+	  for (i = 0; i < ndupl; i++)
+	    scale_main = RDIV (scale_main * scale_step[i], REG_BR_PROB_BASE);
+	  scale_act = REG_BR_PROB_BASE - prob_pass_thru;
+	}
+      for (i = 0; i < ndupl; i++)
+	if (scale_step[i] < 0 || scale_step[i] > REG_BR_PROB_BASE)
+	  abort ();
+      if (scale_main < 0 || scale_main > REG_BR_PROB_BASE
+	  || scale_act < 0  || scale_act > REG_BR_PROB_BASE)
+	abort ();
+    }
+
+  /* Loop the new bbs will belong to.  */
+  target = find_common_loop (e->src->loop_father, e->dest->loop_father);
+
+  /* Original loops.  */
+  n_orig_loops = 0;
+  for (aloop = loop->inner; aloop; aloop = aloop->next)
+    n_orig_loops++;
+  orig_loops = xcalloc (n_orig_loops, sizeof (struct loop *));
+  for (aloop = loop->inner, i = 0; aloop; aloop = aloop->next, i++)
+    orig_loops[i] = aloop;
+
+  loop->copy = target;
+  
+  /* Original basic blocks.  */
+  n = loop->num_nodes;
+
+  first_active = xcalloc(n, sizeof (basic_block));
+  if (is_latch)
+    {
+      memcpy (first_active, bbs, n * sizeof (basic_block));
+      first_active_latch = latch;
+    }
+
+  /* Record exit edges in original loop body.  */
+  if (TEST_BIT (wont_exit, 0))
+    record_exit_edges (orig, bbs, n, to_remove, n_to_remove, true);
+  
+  for (j = 0; j < ndupl; j++)
+    {
+      /* Copy loops.  */
+      copy_loops_to (loops, orig_loops, n_orig_loops, target, scale_act);
+
+      /* Copy bbs.  */
+      copy_bbs (bbs, n, e, latch_edge, &new_bbs, loops, &e, &he, add_irreducible_flag);
+      if (is_latch)
+	loop->latch = RBI (latch)->copy;
+
+      /* Record exit edges in this copy.  */
+      if (TEST_BIT (wont_exit, j + 1))
+	record_exit_edges (orig, new_bbs, n, to_remove, n_to_remove, false);
+  
+      /* Set counts and frequencies.  */
+      for (i = 0; i < n; i++)
+	{
+	  new_bb = new_bbs[i];
+	  bb = bbs[i];
+
+	  if (flags & DLTHE_FLAG_UPDATE_FREQ)
+	    {
+	      new_bb->count = RDIV (scale_act * bb->count, REG_BR_PROB_BASE);
+	      new_bb->frequency = RDIV (scale_act * bb->frequency,
+     					REG_BR_PROB_BASE);
+	    }
+	  else
+	    {
+	      new_bb->count = bb->count;
+	      new_bb->frequency = bb->frequency;
+	    }
+
+	  for (ae = new_bb->succ; ae; ae = ae->succ_next)
+    	    ae->count = RDIV (new_bb->count * ae->probability,
+			      REG_BR_PROB_BASE);
+	}
+      if (flags & DLTHE_FLAG_UPDATE_FREQ)
+	scale_act = RDIV (scale_act * scale_step[j], REG_BR_PROB_BASE);
+
+      if (!first_active_latch)
+	{
+	  memcpy (first_active, new_bbs, n * sizeof (basic_block));
+	  first_active_latch = RBI (latch)->copy;
+	}
+      
+      free (new_bbs);
+
+      /* Original loop header is dominated by latch copy
+	 if we duplicated on its only entry edge.  */
+      if (!is_latch && !header->pred->pred_next->pred_next)
+	set_immediate_dominator (loops->cfg.dom, header, RBI (latch)->copy);
+      if (is_latch && j == 0)
+	{
+	  /* Update edge from latch.  */
+	  for (latch_edge = RBI (header)->copy->pred;
+	       latch_edge->src != latch;
+	       latch_edge = latch_edge->pred_next);
+	}
+    }
+  /* Now handle original loop.  */
+  
+  /* Update edge counts.  */
+  if (flags & DLTHE_FLAG_UPDATE_FREQ)
+    {
+      for (i = 0; i < n; i++)
+	{
+	  bb = bbs[i];
+	  bb->count = RDIV (scale_main * bb->count, REG_BR_PROB_BASE);
+	  bb->frequency = RDIV (scale_main * bb->frequency, REG_BR_PROB_BASE);
+	  for (ae = bb->succ; ae; ae = ae->succ_next)
+	    ae->count = RDIV (bb->count * ae->probability, REG_BR_PROB_BASE);
+	}
+      free (scale_step);
+    }
+  free (orig_loops);
+
+  /* Update dominators of other blocks if affected.  */
+  for (i = 0; i < n; i++)
+    {
+      basic_block dominated, dom_bb, *dom_bbs;
+      int n_dom_bbs,j;
+
+      bb = bbs[i];
+      n_dom_bbs = get_dominated_by (loops->cfg.dom, bb, &dom_bbs);
+      for (j = 0; j < n_dom_bbs; j++)
+	{
+	  dominated = dom_bbs[j];
+	  if (flow_bb_inside_loop_p (loop, dominated))
+	    continue;
+	  dom_bb = nearest_common_dominator (
+			loops->cfg.dom, first_active[i], first_active_latch);
+          set_immediate_dominator (loops->cfg.dom, dominated, dom_bb);
+	}
+      free (dom_bbs);
+    }
+  free (first_active);
+
+  free (bbs);
+
+  return true;
+}
+
+/* Creates a pre-header for a LOOP.  Returns newly created block.  Unless
+   CP_SIMPLE_PREHEADERS is set in FLAGS, we only force LOOP to have single
+   entry; otherwise we also force preheader block to have only one successor.
+   */
+static basic_block
+create_preheader (loop, dom, flags)
+     struct loop *loop;
+     dominance_info dom;
+     int flags;
+{
+  edge e, fallthru;
+  basic_block dummy;
+  basic_block jump, src;
+  struct loop *cloop, *ploop;
+  int nentry = 0;
+  rtx insn;
+
+  cloop = loop->outer;
+
+  for (e = loop->header->pred; e; e = e->pred_next)
+    {
+      if (e->src == loop->latch)
+	continue;
+      nentry++;
+    }
+  if (!nentry)
+    abort ();
+  if (nentry == 1)
+    {
+      for (e = loop->header->pred; e->src == loop->latch; e = e->pred_next);
+      if (!(flags & CP_SIMPLE_PREHEADERS)
+	  || !e->src->succ->succ_next)
+	return NULL;
+    }
+
+  insn = first_insn_after_basic_block_note (loop->header);
+  if (insn)
+    insn = PREV_INSN (insn);
+  else
+    insn = get_last_insn ();
+  if (insn == loop->header->end)
+    {
+      /* Split_block would not split block after its end.  */
+      emit_note_after (NOTE_INSN_DELETED, insn);
+    }
+  if (flags & CP_INSIDE_CFGLAYOUT)
+    fallthru = cfg_layout_split_block (loop->header, insn);
+  else
+    fallthru = split_block (loop->header, insn);
+  dummy = fallthru->src;
+  loop->header = fallthru->dest;
+
+  /* The header could be a latch of some superloop(s); due to design of
+     split_block, it would now move to fallthru->dest.  */
+  for (ploop = loop; ploop; ploop = ploop->outer)
+    if (ploop->latch == dummy)
+      ploop->latch = fallthru->dest;
+
+  add_to_dominance_info (dom, fallthru->dest);
+  
+  /* Redirect edges. */
+  for (e = dummy->pred; e; e = e->pred_next)
+    {
+      src = e->src;
+      if (src == loop->latch)
+	break;
+    }
+  if (!e)
+    abort ();
+
+  dummy->frequency -= EDGE_FREQUENCY (e);
+  dummy->count -= e->count;
+  fallthru->count -= e->count;
+  if (flags & CP_INSIDE_CFGLAYOUT)
+    cfg_layout_redirect_edge (e, loop->header);
+  else
+    {
+      jump = redirect_edge_and_branch_force (e, loop->header);
+      if (jump)
+	{
+	  add_to_dominance_info (dom, jump);
+	  set_immediate_dominator (dom, jump, src);
+	  add_bb_to_loop (jump, loop);
+	  loop->latch = jump;
+	}
+    }
+
+  /* Update structures.  */
+  redirect_immediate_dominators (dom, dummy, loop->header);
+  set_immediate_dominator (dom, loop->header, dummy);
+  loop->header->loop_father = loop;
+  add_bb_to_loop (dummy, cloop);
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, "Created preheader block for loop %i\n",
+	     loop->num);
+
+  return dummy;
+}
+
+/* Create preheaders for each loop; for meaning of flags see
+   create_preheader.  */
+void
+create_preheaders (loops, flags)
+     struct loops *loops;
+     int flags;
+{
+  unsigned i;
+  for (i = 1; i < loops->num; i++)
+    create_preheader (loops->parray[i], loops->cfg.dom, flags);
+  loops->state |= LOOPS_HAVE_PREHEADERS;
+}
+
+/* Forces all loop latches to have only single successor.  */
+void
+force_single_succ_latches (loops)
+     struct loops *loops;
+{
+  unsigned i;
+  struct loop *loop;
+  edge e;
+
+  for (i = 1; i < loops->num; i++)
+    {
+      loop = loops->parray[i];
+      if (!loop->latch->succ->succ_next)
+	continue;
+ 
+      for (e = loop->header->pred; e->src != loop->latch; e = e->pred_next);
+	loop_split_edge_with (e, NULL_RTX, loops);
+    }
+  loops->state |= LOOPS_HAVE_SIMPLE_LATCHES;
+}
+
+/* A quite stupid function to put INSNS on E. They are supposed to form
+   just one basic block. Jumps out are not handled, so cfg do not have to
+   be ok after this function.  */
+basic_block
+loop_split_edge_with (e, insns, loops)
+     edge e;
+     rtx insns;
+     struct loops *loops;
+{
+  basic_block src, dest, new_bb;
+  struct loop *loop_c;
+  edge new_e;
+  
+  src = e->src;
+  dest = e->dest;
+
+  loop_c = find_common_loop (src->loop_father, dest->loop_father);
+
+  /* Create basic block for it.  */
+
+  new_bb = create_basic_block (NULL_RTX, NULL_RTX, EXIT_BLOCK_PTR->prev_bb);
+  add_to_dominance_info (loops->cfg.dom, new_bb);
+  add_bb_to_loop (new_bb, loop_c);
+  new_bb->flags = insns ? BB_SUPERBLOCK : 0;
+  if (src->flags & BB_IRREDUCIBLE_LOOP)
+    {
+      /* We expect simple preheaders here.  */
+      if ((dest->flags & BB_IRREDUCIBLE_LOOP)
+          || dest->loop_father->header == dest)
+        new_bb->flags |= BB_IRREDUCIBLE_LOOP;
+    }
+
+  new_e = make_edge (new_bb, dest, EDGE_FALLTHRU);
+  new_e->probability = REG_BR_PROB_BASE;
+  new_e->count = e->count;
+
+  new_bb->count = e->count;
+
+  new_bb->frequency = EDGE_FREQUENCY (e);
+  cfg_layout_redirect_edge (e, new_bb);
+
+  alloc_aux_for_block (new_bb, sizeof (struct reorder_block_def));
+  if (insns)
+    {
+      start_sequence ();
+      emit_insn (insns);
+      insns = get_insns ();
+      end_sequence ();
+      emit_insn_after (insns, new_bb->end);
+    }
+
+  set_immediate_dominator (loops->cfg.dom, new_bb, src);
+  set_immediate_dominator (loops->cfg.dom, dest,
+    recount_dominator (loops->cfg.dom, dest));
+
+  if (dest->loop_father->latch == src)
+    dest->loop_father->latch = new_bb;
+  
+  return new_bb;
+}
+
--- gcc-3.3.1/gcc/cfgrtl.c.hammer-branch	2003-04-23 01:08:15.000000000 +0200
+++ gcc-3.3.1/gcc/cfgrtl.c	2003-08-05 18:22:46.000000000 +0200
@@ -465,7 +465,8 @@ update_bb_for_insn (bb)
 
   for (insn = bb->head; ; insn = NEXT_INSN (insn))
     {
-      set_block_for_insn (insn, bb);
+      if (GET_CODE (insn) != BARRIER)
+        set_block_for_insn (insn, bb);
       if (insn == bb->end)
 	break;
     }
@@ -1888,7 +1889,7 @@ verify_flow_info ()
 	  if (e->flags & EDGE_FALLTHRU)
 	    n_fallthru++;
 
-	  if ((e->flags & ~EDGE_DFS_BACK) == 0)
+	  if ((e->flags & ~(EDGE_DFS_BACK | EDGE_CAN_FALLTHRU)) == 0)
 	    n_branch++;
 
 	  if (e->flags & EDGE_ABNORMAL_CALL)
--- gcc-3.3.1/gcc/combine.c.hammer-branch	2003-03-24 12:37:32.000000000 +0100
+++ gcc-3.3.1/gcc/combine.c	2003-08-05 18:22:46.000000000 +0200
@@ -3744,6 +3744,8 @@ combine_simplify_rtx (x, op0_mode, last,
 	  if (general_operand (true_rtx, VOIDmode)
 	      && general_operand (false_rtx, VOIDmode))
 	    {
+	      enum rtx_code reversed;
+
 	      /* Restarting if we generate a store-flag expression will cause
 		 us to loop.  Just drop through in this case.  */
 
@@ -3752,9 +3754,10 @@ combine_simplify_rtx (x, op0_mode, last,
 	      if (true_rtx == const_true_rtx && false_rtx == const0_rtx)
 		x = gen_binary (cond_code, mode, cond, cop1);
 	      else if (true_rtx == const0_rtx && false_rtx == const_true_rtx
-		       && reverse_condition (cond_code) != UNKNOWN)
-		x = gen_binary (reverse_condition (cond_code),
-				mode, cond, cop1);
+		       && ((reversed = reversed_comparison_code_parts
+			   		(cond_code, cond, cop1, NULL))
+		           != UNKNOWN))
+		x = gen_binary (reversed, mode, cond, cop1);
 
 	      /* Likewise, we can make the negate of a comparison operation
 		 if the result values are - STORE_FLAG_VALUE and zero.  */
@@ -3767,11 +3770,13 @@ combine_simplify_rtx (x, op0_mode, last,
 					mode);
 	      else if (GET_CODE (false_rtx) == CONST_INT
 		       && INTVAL (false_rtx) == - STORE_FLAG_VALUE
-		       && true_rtx == const0_rtx)
+		       && true_rtx == const0_rtx
+		       && ((reversed = reversed_comparison_code_parts
+			   		(cond_code, cond, cop1, NULL))
+		           != UNKNOWN))
 		x = simplify_gen_unary (NEG, mode,
-					gen_binary (reverse_condition
-						    (cond_code),
-						    mode, cond, cop1),
+					gen_binary (reversed, mode,
+					  	    cond, cop1),
 					mode);
 	      else
 		return gen_rtx_IF_THEN_ELSE (mode,
@@ -4198,6 +4203,36 @@ combine_simplify_rtx (x, op0_mode, last,
 	  && GET_MODE (XEXP (XEXP (x, 0), 0)) == mode)
 	return XEXP (XEXP (x, 0), 0);
 
+      /* (float_truncate:SF (float_truncate:DF foo:XF)) 
+         = (float_truncate:SF foo:XF). 
+	 This may elliminate double rounding, so it is unsafe.
+
+         (float_truncate:SF (float_extend:XF foo:DF)) 
+         = (float_truncate:SF foo:DF). 
+
+         (float_truncate:DF (float_extend:XF foo:SF)) 
+         = (float_extend:SF foo:DF). */
+      if ((GET_CODE (XEXP (x, 0)) == FLOAT_TRUNCATE
+	   && flag_unsafe_math_optimizations)
+	  || GET_CODE (XEXP (x, 0)) == FLOAT_EXTEND)
+	return simplify_gen_unary (GET_MODE_SIZE (GET_MODE (XEXP (XEXP (x, 0),
+		  					    0)))
+	    			   > GET_MODE_SIZE (mode)
+				   ? FLOAT_TRUNCATE : FLOAT_EXTEND,
+	    			   mode,
+				   XEXP (XEXP (x, 0), 0), mode);
+
+      /*  (float_truncate (float x)) is (float x)  */
+      if (GET_CODE (XEXP (x, 0)) == FLOAT
+	  && (flag_unsafe_math_optimizations
+	      || ((unsigned)significand_size (GET_MODE (XEXP (x, 0)))
+		  >= (GET_MODE_BITSIZE (GET_MODE (XEXP (XEXP (x, 0), 0)))
+		      - num_sign_bit_copies (XEXP (XEXP (x, 0), 0),
+					     GET_MODE (XEXP (XEXP (x, 0), 0)))))))
+	return simplify_gen_unary (FLOAT, mode,
+				   XEXP (XEXP (x, 0), 0),
+				   GET_MODE (XEXP (XEXP (x, 0), 0)));
+
       /* (float_truncate:SF (OP:DF (float_extend:DF foo:sf))) is
 	 (OP:SF foo:SF) if OP is NEG or ABS.  */
       if ((GET_CODE (XEXP (x, 0)) == ABS
@@ -4214,7 +4249,23 @@ combine_simplify_rtx (x, op0_mode, last,
 	  && GET_CODE (SUBREG_REG (XEXP (x, 0))) == FLOAT_TRUNCATE)
 	return SUBREG_REG (XEXP (x, 0));
       break;
+    case FLOAT_EXTEND:
+      /*  (float_extend (float_extend x)) is (float_extend x)
+        
+	  (float_extend (float x)) is (float x) assuming that double
+	  rounding can't happen. 
+          */
+      if (GET_CODE (XEXP (x, 0)) == FLOAT_EXTEND
+	  || (GET_CODE (XEXP (x, 0)) == FLOAT
+	      && ((unsigned)significand_size (GET_MODE (XEXP (x, 0)))
+		  >= (GET_MODE_BITSIZE (GET_MODE (XEXP (XEXP (x, 0), 0)))
+		      - num_sign_bit_copies (XEXP (XEXP (x, 0), 0),
+					     GET_MODE (XEXP (XEXP (x, 0), 0)))))))
+	return simplify_gen_unary (GET_CODE (XEXP (x, 0)), mode,
+				   XEXP (XEXP (x, 0), 0),
+				   GET_MODE (XEXP (XEXP (x, 0), 0)));
 
+      break;
 #ifdef HAVE_cc0
     case COMPARE:
       /* Convert (compare FOO (const_int 0)) to FOO unless we aren't
@@ -9369,6 +9420,16 @@ simplify_shift_const (x, code, result_mo
 	      == 0))
 	code = LSHIFTRT;
 
+      if (code == LSHIFTRT
+	  && GET_MODE_BITSIZE (shift_mode) <= HOST_BITS_PER_WIDE_INT
+	  && !(nonzero_bits (varop, shift_mode) >> count))
+	varop = const0_rtx;
+      if (code == ASHIFT
+	  && GET_MODE_BITSIZE (shift_mode) <= HOST_BITS_PER_WIDE_INT
+	  && !((nonzero_bits (varop, shift_mode) << count)
+	       & GET_MODE_MASK (shift_mode)))
+	varop = const0_rtx;
+
       switch (GET_CODE (varop))
 	{
 	case SIGN_EXTEND:
@@ -12534,6 +12595,11 @@ distribute_notes (notes, from_insn, i3, 
 	  place = i3;
 	  break;
 
+	case REG_VALUE_HISTOGRAM:
+	  /* Given that I don't understand what's going on here at all,
+	     just get rid of this.  */
+	  break;
+
 	case REG_VTABLE_REF:
 	  /* ??? Should remain with *a particular* memory load.  Given the
 	     nature of vtable data, the last insn seems relatively safe.  */
--- gcc-3.3.1/gcc/config.gcc.hammer-branch	2003-06-27 13:44:22.000000000 +0200
+++ gcc-3.3.1/gcc/config.gcc	2003-08-05 18:22:46.000000000 +0200
@@ -2805,6 +2805,9 @@ i586-*-*)
 	;;
 i686-*-* | i786-*-*)
 	case $target_alias in
+		k8-*)
+			target_cpu_default2=TARGET_CPU_DEFAULT_k8
+			;;
 		athlon_xp-*|athlon_mp-*|athlon_4-*)
 			target_cpu_default2=TARGET_CPU_DEFAULT_athlon_sse
 			;;
@@ -2829,7 +2832,7 @@ x86_64-*-*)
 	# We should have hammer chip here, but it does not exist yet and
 	# thus it is not supported.  Athlon_SSE is probably equivalent feature
 	# wise to hammer from our point of view except for 64bit mode.
-	target_cpu_default2=TARGET_CPU_DEFAULT_athlon_sse
+	target_cpu_default2=TARGET_CPU_DEFAULT_k8
 	;;
 alpha*-*-*)
 	case $machine in
--- gcc-3.3.1/gcc/configure.hammer-branch	2003-07-16 10:27:05.000000000 +0200
+++ gcc-3.3.1/gcc/configure	2003-08-05 18:22:46.000000000 +0200
@@ -8335,7 +8335,7 @@ target_list="all.build all.cross start.e
 	install-normal install-common install-info install-man \
 	uninstall \
 	mostlyclean clean distclean extraclean maintainer-clean \
-	stage1 stage2 stage3 stage4"
+	stage1 stage2 stage3 stage4 stageprofile stagefeedback"
 for t in $target_list
 do
 	x=
@@ -9105,7 +9105,7 @@ if test "$symbolic_link" = "ln -s"; then
    if test $d != ..; then
 	STARTDIR=`${PWDCMD-pwd}`
 	cd $d
-	for t in stage1 stage2 stage3 stage4 include
+	for t in stage1 stage2 stage3 stage4 stageprofile stagefeedback include
 	do
 		rm -f $t
 		$symbolic_link ../$t $t 2>/dev/null
--- gcc-3.3.1/gcc/configure.in.hammer-branch	2003-06-26 01:14:55.000000000 +0200
+++ gcc-3.3.1/gcc/configure.in	2003-08-05 18:22:46.000000000 +0200
@@ -2716,7 +2716,7 @@ target_list="all.build all.cross start.e
 	install-normal install-common install-info install-man \
 	uninstall \
 	mostlyclean clean distclean extraclean maintainer-clean \
-	stage1 stage2 stage3 stage4"
+	stage1 stage2 stage3 stage4 stageprofile stagefeedback"
 for t in $target_list
 do
 	x=
@@ -2943,7 +2943,7 @@ if test "$symbolic_link" = "ln -s"; then
    if test $d != ..; then
 	STARTDIR=`${PWDCMD-pwd}`
 	cd $d
-	for t in stage1 stage2 stage3 stage4 include
+	for t in stage1 stage2 stage3 stage4 stageprofile stagefeedback include
 	do
 		rm -f $t
 		$symbolic_link ../$t $t 2>/dev/null
--- gcc-3.3.1/gcc/convert.c.hammer-branch	2002-07-04 08:38:54.000000000 +0200
+++ gcc-3.3.1/gcc/convert.c	2003-08-05 18:22:46.000000000 +0200
@@ -30,6 +30,7 @@ Software Foundation, 59 Temple Place - S
 #include "convert.h"
 #include "toplev.h"
 #include "langhooks.h"
+#include "real.h"
 
 /* Convert EXPR to some pointer or reference type TYPE.
 
@@ -71,6 +72,51 @@ convert_to_pointer (type, expr)
     }
 }
 
+/* Avoid any floating point extensions from EXP.  */
+tree
+strip_float_extensions (exp)
+     tree exp;
+{
+  tree sub, expt, subt;
+
+  /*  For floating point constant look up the narrowest type that can hold
+      it properly and handle it like (type)(narrowest_type)constant.
+      This way we can optimize for instance a=a*2.0 where "a" is float
+      but 2.0 is double constant.  */
+  if (TREE_CODE (exp) == REAL_CST)
+    {
+      REAL_VALUE_TYPE orig;
+      tree type = NULL;
+
+      orig = TREE_REAL_CST (exp);
+      if (TYPE_PRECISION (TREE_TYPE (exp)) > TYPE_PRECISION (float_type_node)
+	  && exact_real_truncate (TYPE_MODE (float_type_node), &orig))
+	type = float_type_node;
+      else if (TYPE_PRECISION (TREE_TYPE (exp))
+	       > TYPE_PRECISION (double_type_node)
+	       && exact_real_truncate (TYPE_MODE (double_type_node), &orig))
+	type = double_type_node;
+      if (type)
+	return build_real (type, real_value_truncate (TYPE_MODE (type), orig));
+    }
+
+  if (TREE_CODE (exp) != NOP_EXPR)
+    return exp;
+
+  sub = TREE_OPERAND (exp, 0);
+  subt = TREE_TYPE (sub);
+  expt = TREE_TYPE (exp);
+
+  if (!FLOAT_TYPE_P (subt))
+    return exp;
+
+  if (TYPE_PRECISION (subt) > TYPE_PRECISION (expt))
+    return exp;
+
+  return strip_float_extensions (sub);
+}
+
+
 /* Convert EXPR to some floating-point type TYPE.
 
    EXPR must be float, integer, or enumeral;
@@ -80,6 +126,125 @@ tree
 convert_to_real (type, expr)
      tree type, expr;
 {
+  enum built_in_function fcode = builtin_mathfn_code (expr);
+  tree itype = TREE_TYPE (expr);
+
+  /* Disable until we figure out how to decide whether the functions are
+     present in runtime.  */
+  /* Convert (float)sqrt((double)x) where x is float into sqrtf(x) */
+  if ((fcode == BUILT_IN_SQRT
+       || fcode == BUILT_IN_SQRTL
+       || fcode == BUILT_IN_SIN
+       || fcode == BUILT_IN_SINL
+       || fcode == BUILT_IN_COS
+       || fcode == BUILT_IN_COSL
+       || fcode == BUILT_IN_EXP
+       || fcode == BUILT_IN_EXPL)
+      && optimize
+      && (TYPE_MODE (type) == TYPE_MODE (double_type_node)
+          || TYPE_MODE (type) == TYPE_MODE (float_type_node)))
+    {
+      tree arg0 = strip_float_extensions (TREE_VALUE (TREE_OPERAND (expr, 1)));
+      tree newtype = type;
+
+      /* We have (outertype)sqrt((innertype)x).  Choose the wider mode from
+	 the both as the safe type for operation.  */
+      if (TYPE_PRECISION (TREE_TYPE (arg0)) > TYPE_PRECISION (type))
+	newtype = TREE_TYPE (arg0);
+
+      /* Be curefull about integer to fp conversions.
+	 These may overflow still.  */
+      if (FLOAT_TYPE_P (TREE_TYPE (arg0))
+	  && TYPE_PRECISION (newtype) < TYPE_PRECISION (itype)
+	  && (TYPE_MODE (newtype) == TYPE_MODE (double_type_node)
+	      || TYPE_MODE (newtype) == TYPE_MODE (float_type_node)))
+	{
+	  tree arglist;
+	  tree fn = mathfn_built_in (newtype, fcode);
+
+	  if (fn)
+	    {
+	      arglist = build_tree_list (NULL_TREE, fold (convert_to_real (newtype, arg0)));
+	      expr = build_function_call_expr (fn, arglist);
+	      if (newtype == type)
+		return expr;
+	    }
+	}
+    }
+  if (optimize
+      && (((fcode == BUILT_IN_FLOORL
+	   || fcode == BUILT_IN_CEILL
+	   || fcode == BUILT_IN_ROUND
+	   || fcode == BUILT_IN_TRUNC
+	   || fcode == BUILT_IN_NEARBYINT)
+	  && (TYPE_MODE (type) == TYPE_MODE (double_type_node)
+	      || TYPE_MODE (type) == TYPE_MODE (float_type_node)))
+	  || ((fcode == BUILT_IN_FLOOR
+	       || fcode == BUILT_IN_CEIL
+	       || fcode == BUILT_IN_ROUND
+	       || fcode == BUILT_IN_TRUNC
+	       || fcode == BUILT_IN_NEARBYINT)
+	      && (TYPE_MODE (type) == TYPE_MODE (float_type_node)))))
+    {
+      tree fn = mathfn_built_in (type, fcode);
+
+      if (fn)
+	{
+	  tree arg0 = strip_float_extensions (TREE_VALUE (TREE_OPERAND (expr,
+					  				1)));
+	  tree arglist = build_tree_list (NULL_TREE,
+			  		  fold (convert_to_real (type, arg0)));
+
+	  return build_function_call_expr (fn, arglist);
+	}
+    }
+
+  /* Propagate the cast into the operation.  */
+  if (itype != type && FLOAT_TYPE_P (type))
+    switch (TREE_CODE (expr))
+      {
+	/* convert (float)-x into -(float)x.  This is always safe.  */
+	case ABS_EXPR:
+	case NEGATE_EXPR:
+	  if (TYPE_PRECISION (type) < TYPE_PRECISION (TREE_TYPE (expr)))
+	    return build1 (TREE_CODE (expr), type,
+			   fold (convert_to_real (type,
+						  TREE_OPERAND (expr, 0))));
+	  break;
+	/* convert (outertype)((innertype0)a+(innertype1)b)
+	   into ((newtype)a+(newtype)b) where newtype
+	   is the widest mode from all of these.  */
+	case PLUS_EXPR:
+	case MINUS_EXPR:
+	case MULT_EXPR:
+	case RDIV_EXPR:
+	   {
+	     tree arg0 = strip_float_extensions (TREE_OPERAND (expr, 0));
+	     tree arg1 = strip_float_extensions (TREE_OPERAND (expr, 1));
+
+	     if (FLOAT_TYPE_P (TREE_TYPE (arg0))
+		 && FLOAT_TYPE_P (TREE_TYPE (arg1)))
+	       {
+		  tree newtype = type;
+		  if (TYPE_PRECISION (TREE_TYPE (arg0)) > TYPE_PRECISION (newtype))
+		    newtype = TREE_TYPE (arg0);
+		  if (TYPE_PRECISION (TREE_TYPE (arg1)) > TYPE_PRECISION (newtype))
+		    newtype = TREE_TYPE (arg1);
+		  if (TYPE_PRECISION (newtype) < TYPE_PRECISION (itype))
+		    {
+		      expr = build (TREE_CODE (expr), newtype,
+				    fold (convert_to_real (newtype, arg0)),
+				    fold (convert_to_real (newtype, arg1)));
+		      if (newtype == type)
+			return expr;
+		    }
+	       }
+	   }
+	  break;
+	default:
+	  break;
+      }
+
   switch (TREE_CODE (TREE_TYPE (expr)))
     {
     case REAL_TYPE:
--- gcc-3.3.1/gcc/defaults.h.hammer-branch	2002-12-09 18:54:03.000000000 +0100
+++ gcc-3.3.1/gcc/defaults.h	2003-08-05 18:22:46.000000000 +0200
@@ -607,4 +607,10 @@ You Lose!  You must define PREFERRED_DEB
 #define EXTRA_ADDRESS_CONSTRAINT(C) 0
 #endif
 
+/* Determine whether math functions specified by C99 standard, like
+   sinf/sinl are present in the runtime library.  */
+#ifndef TARGET_C99_FUNCTIONS
+#define TARGET_C99_FUNCTIONS 1
+#endif
+
 #endif  /* ! GCC_DEFAULTS_H */
--- gcc-3.3.1/gcc/df.c.hammer-branch	2003-02-01 00:51:21.000000000 +0100
+++ gcc-3.3.1/gcc/df.c	2003-08-05 18:22:46.000000000 +0200
@@ -912,10 +912,10 @@ read_modify_subreg_p (x)
   isize = GET_MODE_SIZE (GET_MODE (SUBREG_REG (x)));
   osize = GET_MODE_SIZE (GET_MODE (x));
   if (isize <= osize)
-    return true;
+    return false;
   if (isize <= UNITS_PER_WORD)
     return false;
-  if (osize >= UNITS_PER_WORD)
+  if (osize > UNITS_PER_WORD)
     return false;
   return true;
 }
--- gcc-3.3.1/gcc/doloop.c.hammer-branch	2002-11-04 21:06:27.000000000 +0100
+++ gcc-3.3.1/gcc/doloop.c	2003-08-05 18:22:46.000000000 +0200
@@ -29,6 +29,7 @@ Software Foundation, 59 Temple Place - S
 #include "basic-block.h"
 #include "toplev.h"
 #include "tm_p.h"
+#include "cfgloop.h"
 
 
 /* This module is used to modify loops with a determinable number of
--- gcc-3.3.1/gcc/emit-rtl.c.hammer-branch	2003-07-16 14:37:53.000000000 +0200
+++ gcc-3.3.1/gcc/emit-rtl.c	2003-08-05 18:22:46.000000000 +0200
@@ -5565,6 +5565,7 @@ emit_copy_of_insn_after (insn, after)
       XEXP (note1, 0) = p;
       XEXP (note2, 0) = new;
     }
+  INSN_CODE (new) = INSN_CODE (insn);
   return new;
 }
 
--- gcc-3.3.1/gcc/expmed.c.hammer-branch	2003-06-13 02:32:28.000000000 +0200
+++ gcc-3.3.1/gcc/expmed.c	2003-08-05 18:22:46.000000000 +0200
@@ -2953,14 +2953,20 @@ expand_divmod (rem_flag, code, mode, op0
   int size;
   rtx insn, set;
   optab optab1, optab2;
-  int op1_is_constant, op1_is_pow2;
+  int op1_is_constant, op1_is_pow2 = 0;
   int max_cost, extra_cost;
   static HOST_WIDE_INT last_div_const = 0;
+  static HOST_WIDE_INT ext_op1;
 
   op1_is_constant = GET_CODE (op1) == CONST_INT;
-  op1_is_pow2 = (op1_is_constant
-		 && ((EXACT_POWER_OF_2_OR_ZERO_P (INTVAL (op1))
-		      || (! unsignedp && EXACT_POWER_OF_2_OR_ZERO_P (-INTVAL (op1))))));
+  if (op1_is_constant)
+    {
+      ext_op1 = INTVAL (op1);
+      if (unsignedp)
+	ext_op1 &= GET_MODE_MASK (mode);
+      op1_is_pow2 = ((EXACT_POWER_OF_2_OR_ZERO_P (ext_op1)
+		     || (! unsignedp && EXACT_POWER_OF_2_OR_ZERO_P (-ext_op1))));
+    }
 
   /*
      This is the structure of expand_divmod:
@@ -3140,7 +3146,8 @@ expand_divmod (rem_flag, code, mode, op0
 		unsigned HOST_WIDE_INT mh, ml;
 		int pre_shift, post_shift;
 		int dummy;
-		unsigned HOST_WIDE_INT d = INTVAL (op1);
+		unsigned HOST_WIDE_INT d = (INTVAL (op1)
+					    & GET_MODE_MASK (compute_mode));
 
 		if (EXACT_POWER_OF_2_OR_ZERO_P (d))
 		  {
--- gcc-3.3.1/gcc/final.c.hammer-branch	2003-01-26 00:40:06.000000000 +0100
+++ gcc-3.3.1/gcc/final.c	2003-08-05 18:22:46.000000000 +0200
@@ -68,7 +68,6 @@ Software Foundation, 59 Temple Place - S
 #include "target.h"
 #include "debug.h"
 #include "expr.h"
-#include "profile.h"
 #include "cfglayout.h"
 
 #ifdef XCOFF_DEBUGGING_INFO
@@ -136,10 +135,6 @@ static unsigned int insn_noperands;
 
 static rtx last_ignored_compare = 0;
 
-/* Flag indicating this insn is the start of a new basic block.  */
-
-static int new_block = 1;
-
 /* Assign a unique number to each insn that is output.
    This can be used to generate unique local labels.  */
 
@@ -203,17 +198,6 @@ static char *line_note_exists;
 rtx current_insn_predicate;
 #endif
 
-struct function_list
-{
-  struct function_list *next; 	/* next function */
-  const char *name; 		/* function name */
-  long cfg_checksum;		/* function checksum */
-  long count_edges;		/* number of intrumented edges in this function */
-};
-
-static struct function_list *functions_head = 0;
-static struct function_list **functions_tail = &functions_head;
-
 #ifdef HAVE_ATTR_length
 static int asm_insn_count	PARAMS ((rtx));
 #endif
@@ -253,272 +237,6 @@ init_final (filename)
 #endif
 }
 
-/* Called at end of source file,
-   to output the arc-profiling table for this entire compilation.  */
-
-void
-end_final (filename)
-     const char *filename;
-{
-  if (profile_arc_flag && profile_info.count_instrumented_edges)
-    {
-      char name[20];
-      tree string_type, string_cst;
-      tree structure_decl, structure_value, structure_pointer_type;
-      tree field_decl, decl_chain, value_chain;
-      tree sizeof_field_value, domain_type;
-
-      /* Build types.  */
-      string_type = build_pointer_type (char_type_node);
-
-      /* Libgcc2 bb structure.  */
-      structure_decl = make_node (RECORD_TYPE);
-      structure_pointer_type = build_pointer_type (structure_decl);
-
-      /* Output the main header, of 7 words:
-         0:  1 if this file is initialized, else 0.
-         1:  address of file name (LPBX1).
-         2:  address of table of counts (LPBX2).
-         3:  number of counts in the table.
-         4:  always 0, libgcc2 uses this as a pointer to next ``struct bb''
-
-         The following are GNU extensions:
-
-         5:  Number of bytes in this header.
-         6:  address of table of function checksums (LPBX7).  */
-
-      /* The zero word.  */
-      decl_chain =
-	build_decl (FIELD_DECL, get_identifier ("zero_word"),
-		    long_integer_type_node);
-      value_chain = build_tree_list (decl_chain,
-				     convert (long_integer_type_node,
-					      integer_zero_node));
-
-      /* Address of filename.  */
-      {
-	char *cwd, *da_filename;
-	int da_filename_len;
-
-	field_decl =
-	  build_decl (FIELD_DECL, get_identifier ("filename"), string_type);
-	TREE_CHAIN (field_decl) = decl_chain;
-	decl_chain = field_decl;
-
-	cwd = getpwd ();
-	da_filename_len = strlen (filename) + strlen (cwd) + 4 + 1;
-	da_filename = (char *) alloca (da_filename_len);
-	strcpy (da_filename, cwd);
-	strcat (da_filename, "/");
-	strcat (da_filename, filename);
-	strcat (da_filename, ".da");
-	da_filename_len = strlen (da_filename);
-	string_cst = build_string (da_filename_len + 1, da_filename);
-	domain_type = build_index_type (build_int_2 (da_filename_len, 0));
-	TREE_TYPE (string_cst)
-	  = build_array_type (char_type_node, domain_type);
-	value_chain = tree_cons (field_decl,
-				 build1 (ADDR_EXPR, string_type, string_cst),
-				 value_chain);
-      }
-
-      /* Table of counts.  */
-      {
-	tree gcov_type_type = make_unsigned_type (GCOV_TYPE_SIZE);
-	tree gcov_type_pointer_type = build_pointer_type (gcov_type_type);
-	tree domain_tree
-	  = build_index_type (build_int_2 (profile_info.
-					   count_instrumented_edges - 1, 0));
-	tree gcov_type_array_type
-	  = build_array_type (gcov_type_type, domain_tree);
-	tree gcov_type_array_pointer_type
-	  = build_pointer_type (gcov_type_array_type);
-	tree counts_table;
-
-	field_decl =
-	  build_decl (FIELD_DECL, get_identifier ("counts"),
-		      gcov_type_pointer_type);
-	TREE_CHAIN (field_decl) = decl_chain;
-	decl_chain = field_decl;
-
-	/* No values.  */
-	counts_table
-	  = build (VAR_DECL, gcov_type_array_type, NULL_TREE, NULL_TREE);
-	TREE_STATIC (counts_table) = 1;
-	ASM_GENERATE_INTERNAL_LABEL (name, "LPBX", 2);
-	DECL_NAME (counts_table) = get_identifier (name);
-	assemble_variable (counts_table, 0, 0, 0);
-
-	value_chain = tree_cons (field_decl,
-				 build1 (ADDR_EXPR,
-					 gcov_type_array_pointer_type,
-					 counts_table), value_chain);
-      }
-
-      /* Count of the # of instrumented arcs.  */
-      field_decl
-	= build_decl (FIELD_DECL, get_identifier ("ncounts"),
-		      long_integer_type_node);
-      TREE_CHAIN (field_decl) = decl_chain;
-      decl_chain = field_decl;
-
-      value_chain = tree_cons (field_decl,
-			       convert (long_integer_type_node,
-					build_int_2 (profile_info.
-						     count_instrumented_edges,
-						     0)), value_chain);
-      /* Pointer to the next bb.  */
-      field_decl
-	= build_decl (FIELD_DECL, get_identifier ("next"),
-		      structure_pointer_type);
-      TREE_CHAIN (field_decl) = decl_chain;
-      decl_chain = field_decl;
-
-      value_chain = tree_cons (field_decl, null_pointer_node, value_chain);
-
-      /* sizeof(struct bb).  We'll set this after entire structure
-	 is laid out.  */
-      field_decl
-	= build_decl (FIELD_DECL, get_identifier ("sizeof_bb"),
-		      long_integer_type_node);
-      TREE_CHAIN (field_decl) = decl_chain;
-      decl_chain = field_decl;
-
-      sizeof_field_value = tree_cons (field_decl, NULL, value_chain);
-      value_chain = sizeof_field_value;
-
-      /* struct bb_function [].  */
-      {
-	struct function_list *item;
-	int num_nodes;
-	tree checksum_field, arc_count_field, name_field;
-	tree domain;
-	tree array_value_chain = NULL_TREE;
-	tree bb_fn_struct_type;
-	tree bb_fn_struct_array_type;
-	tree bb_fn_struct_array_pointer_type;
-	tree bb_fn_struct_pointer_type;
-	tree field_value, field_value_chain;
-
-	bb_fn_struct_type = make_node (RECORD_TYPE);
-
-	checksum_field = build_decl (FIELD_DECL, get_identifier ("checksum"),
-				     long_integer_type_node);
-
-	arc_count_field
-	  = build_decl (FIELD_DECL, get_identifier ("arc_count"),
-		        integer_type_node);
-	TREE_CHAIN (checksum_field) = arc_count_field;
-
-	name_field
-	  = build_decl (FIELD_DECL, get_identifier ("name"), string_type);
-	TREE_CHAIN (arc_count_field) = name_field;
-
-	TYPE_FIELDS (bb_fn_struct_type) = checksum_field;
-
-	num_nodes = 0;
-
-	for (item = functions_head; item != 0; item = item->next)
-	  num_nodes++;
-
-	/* Note that the array contains a terminator, hence no - 1.  */
-	domain = build_index_type (build_int_2 (num_nodes, 0));
-
-	bb_fn_struct_pointer_type = build_pointer_type (bb_fn_struct_type);
-	bb_fn_struct_array_type
-	  = build_array_type (bb_fn_struct_type, domain);
-	bb_fn_struct_array_pointer_type
-	  = build_pointer_type (bb_fn_struct_array_type);
-
-	layout_type (bb_fn_struct_type);
-	layout_type (bb_fn_struct_pointer_type);
-	layout_type (bb_fn_struct_array_type);
-	layout_type (bb_fn_struct_array_pointer_type);
-
-	for (item = functions_head; item != 0; item = item->next)
-	  {
-	    size_t name_len;
-
-	    /* create constructor for structure.  */
-	    field_value_chain
-	      = build_tree_list (checksum_field,
-				 convert (long_integer_type_node,
-					  build_int_2 (item->cfg_checksum, 0)));
-	    field_value_chain
-	      = tree_cons (arc_count_field,
-			   convert (integer_type_node,
-				    build_int_2 (item->count_edges, 0)),
-			   field_value_chain);
-
-	    name_len = strlen (item->name);
-	    string_cst = build_string (name_len + 1, item->name);
-	    domain_type = build_index_type (build_int_2 (name_len, 0));
-	    TREE_TYPE (string_cst)
-	      = build_array_type (char_type_node, domain_type);
-	    field_value_chain = tree_cons (name_field,
-					   build1 (ADDR_EXPR, string_type,
-						   string_cst),
-					   field_value_chain);
-
-	    /* Add to chain.  */
-	    array_value_chain
-	      = tree_cons (NULL_TREE, build (CONSTRUCTOR,
-					     bb_fn_struct_type, NULL_TREE,
-					     nreverse (field_value_chain)),
-			   array_value_chain);
-	  }
-
-	/* Add terminator.  */
-	field_value = build_tree_list (arc_count_field,
-				       convert (integer_type_node,
-						build_int_2 (-1, 0)));
-
-	array_value_chain = tree_cons (NULL_TREE,
-				       build (CONSTRUCTOR, bb_fn_struct_type,
-					      NULL_TREE, field_value),
-				       array_value_chain);
-
-
-	/* Create constructor for array.  */
-	field_decl
-	  = build_decl (FIELD_DECL, get_identifier ("function_infos"),
-		        bb_fn_struct_pointer_type);
-	value_chain = tree_cons (field_decl,
-				 build1 (ADDR_EXPR,
-					 bb_fn_struct_array_pointer_type,
-					 build (CONSTRUCTOR,
-						bb_fn_struct_array_type,
-						NULL_TREE,
-						nreverse
-						(array_value_chain))),
-				 value_chain);
-	TREE_CHAIN (field_decl) = decl_chain;
-	decl_chain = field_decl;
-      }
-
-      /* Finish structure.  */
-      TYPE_FIELDS (structure_decl) = nreverse (decl_chain);
-      layout_type (structure_decl);
-
-      structure_value
-	= build (VAR_DECL, structure_decl, NULL_TREE, NULL_TREE);
-      DECL_INITIAL (structure_value)
-	= build (CONSTRUCTOR, structure_decl, NULL_TREE,
-	         nreverse (value_chain));
-      TREE_STATIC (structure_value) = 1;
-      ASM_GENERATE_INTERNAL_LABEL (name, "LPBX", 0);
-      DECL_NAME (structure_value) = get_identifier (name);
-
-      /* Size of this structure.  */
-      TREE_VALUE (sizeof_field_value)
-	= convert (long_integer_type_node,
-		   build_int_2 (int_size_in_bytes (structure_decl), 0));
-
-      /* Build structure.  */
-      assemble_variable (structure_value, 0, 0, 0);
-    }
-}
-
 /* Default target function prologue and epilogue assembler output.
 
    If not overridden for epilogue code, then the function body itself
@@ -1823,7 +1541,6 @@ final (first, file, optimize, prescan)
   int max_uid = 0;
 
   last_ignored_compare = 0;
-  new_block = 1;
 
   /* Make a map indicating which line numbers appear in this function.
      When producing SDB debugging info, delete troublesome line number
@@ -1904,23 +1621,6 @@ final (first, file, optimize, prescan)
       insn = final_scan_insn (insn, file, optimize, prescan, 0);
     }
 
-  /* Store function names for edge-profiling.  */
-  /* ??? Probably should re-use the existing struct function.  */
-
-  if (cfun->arc_profile)
-    {
-      struct function_list *new_item = xmalloc (sizeof (struct function_list));
-
-      *functions_tail = new_item;
-      functions_tail = &new_item->next;
-
-      new_item->next = 0;
-      new_item->name = xstrdup (IDENTIFIER_POINTER
-				 (DECL_ASSEMBLER_NAME (current_function_decl)));
-      new_item->cfg_checksum = profile_info.current_function_cfg_checksum;
-      new_item->count_edges = profile_info.count_edges_instrumented_now;
-    }
-
   free (line_note_exists);
   line_note_exists = NULL;
 }
@@ -2223,7 +1923,6 @@ final_scan_insn (insn, file, optimize, p
 #endif
       if (prescan > 0)
 	break;
-      new_block = 1;
 
 #ifdef FINAL_PRESCAN_LABEL
       FINAL_PRESCAN_INSN (insn, NULL, 0);
@@ -2765,7 +2464,6 @@ final_scan_insn (insn, file, optimize, p
 
 	    if (prev_nonnote_insn (insn) != last_ignored_compare)
 	      abort ();
-	    new_block = 0;
 
 	    /* We have already processed the notes between the setter and
 	       the user.  Make sure we don't process them again, this is
@@ -2799,7 +2497,6 @@ final_scan_insn (insn, file, optimize, p
 	    abort ();
 #endif
 
-	    new_block = 0;
 	    return new;
 	  }
 
--- gcc-3.3.1/gcc/flags.h.hammer-branch	2003-06-20 23:18:41.000000000 +0200
+++ gcc-3.3.1/gcc/flags.h	2003-08-05 18:22:46.000000000 +0200
@@ -197,6 +197,14 @@ extern int profile_flag;
 
 extern int profile_arc_flag;
 
+/* Nonzero if generating/using loop histograms.  */
+
+extern int flag_loop_histograms;
+
+/* Nonzero if generating/using value histograms.  */
+
+extern int flag_value_histograms;
+
 /* Nonzero if generating info for gcov to calculate line test coverage.  */
 
 extern int flag_test_coverage;
@@ -664,6 +672,8 @@ extern int flag_signaling_nans;
 
 extern const char *flag_random_seed;
 
+extern int flag_unit_at_time;
+
 /* True if the given mode has a NaN representation and the treatment of
    NaN operands is important.  Certain optimizations, such as folding
    x * 0 into x, are not correct for NaN operands, and are normally
--- gcc-3.3.1/gcc/fold-const.c.hammer-branch	2003-07-03 15:18:00.000000000 +0200
+++ gcc-3.3.1/gcc/fold-const.c	2003-08-05 18:22:47.000000000 +0200
@@ -5045,6 +5045,15 @@ fold (expr)
 	}
       else if (TREE_CODE (arg0) == NEGATE_EXPR)
 	return TREE_OPERAND (arg0, 0);
+      /* Convert -((double)float) into (double)(-float).  */
+      else if (TREE_CODE (arg0) == NOP_EXPR
+	       && TREE_CODE (type) == REAL_TYPE)
+	{
+	  tree targ0 = strip_float_extensions (arg0);
+	  if (targ0 != arg0)
+	    return convert (type, build1 (NEGATE_EXPR, TREE_TYPE (targ0), targ0));
+			   
+	}
 
       /* Convert - (a - b) to (b - a) for non-floating-point.  */
       else if (TREE_CODE (arg0) == MINUS_EXPR
@@ -5093,6 +5102,15 @@ fold (expr)
 	}
       else if (TREE_CODE (arg0) == ABS_EXPR || TREE_CODE (arg0) == NEGATE_EXPR)
 	return build1 (ABS_EXPR, type, TREE_OPERAND (arg0, 0));
+      /* Convert fabs((double)float) into (double)fabsf(float).  */
+      else if (TREE_CODE (arg0) == NOP_EXPR
+	       && TREE_CODE (type) == REAL_TYPE)
+	{
+	  tree targ0 = strip_float_extensions (arg0);
+	  if (targ0 != arg0)
+	    return convert (type, build1 (ABS_EXPR, TREE_TYPE (targ0), targ0));
+			   
+	}
       return t;
 
     case CONJ_EXPR:
@@ -5977,6 +5995,18 @@ fold (expr)
 
       if (FLOAT_TYPE_P (TREE_TYPE (arg0)))
 	{
+	  tree targ0 = strip_float_extensions (arg0);
+	  tree targ1 = strip_float_extensions (arg1);
+	  tree newtype = TREE_TYPE (targ0);
+
+	  if (TYPE_PRECISION (TREE_TYPE (targ1)) > TYPE_PRECISION (newtype))
+	    newtype = TREE_TYPE (targ1);
+
+	  /* Fold (double)float1 CMP (double)float2 into float1 CMP float2.  */
+	  if (TYPE_PRECISION (newtype) < TYPE_PRECISION (TREE_TYPE (arg0)))
+	    return fold (build (code, type, convert (newtype, targ0),
+				convert (newtype, targ1)));
+
 	  /* (-a) CMP (-b) -> b CMP a  */
 	  if (TREE_CODE (arg0) == NEGATE_EXPR
 	      && TREE_CODE (arg1) == NEGATE_EXPR)
--- gcc-3.3.1/gcc/function.c.hammer-branch	2003-04-11 00:26:04.000000000 +0200
+++ gcc-3.3.1/gcc/function.c	2003-08-05 18:22:47.000000000 +0200
@@ -291,6 +291,15 @@ static void do_clobber_return_reg PARAMS
 static void do_use_return_reg PARAMS ((rtx, void *));
 static void instantiate_virtual_regs_lossage PARAMS ((rtx));
 
+/* The declarations we know about must not get garbage collected.
+   We do not want callgraph datastructure to be saved via PCH code
+   since it would make it dificult to extend it into untramodule
+   optimizer later, so we store the references into the array to avoid
+   garbage collector from doing it's job.  */
+extern GTY(()) varray_type known_fns;
+varray_type known_fns;
+
+
 /* Pointer to chain of `struct function' for containing functions.  */
 static GTY(()) struct function *outer_function_chain;
 
@@ -7970,7 +7979,6 @@ reposition_prologue_and_epilogue_notes (
     }
 #endif /* HAVE_prologue or HAVE_epilogue */
 }
-
 /* Called once, at initialization, to initialize function.c.  */
 
 void
--- gcc-3.3.1/gcc/gcov-io.h.hammer-branch	2003-04-22 19:18:43.000000000 +0200
+++ gcc-3.3.1/gcc/gcov-io.h	2003-08-05 18:22:47.000000000 +0200
@@ -1,6 +1,7 @@
-/* Machine-independent I/O routines for gcov.
-   Copyright (C) 1996, 1997, 1998, 2000 Free Software Foundation, Inc.
+/* File format for coverage information
+   Copyright (C) 1996, 1997, 1998, 2000, 2002 Free Software Foundation, Inc.
    Contributed by Bob Manson <manson@cygnus.com>.
+   Completely remangled by Nathan Sidwell <nathan@codesourcery.com>.
 
 This file is part of GCC.
 
@@ -19,6 +20,127 @@ along with GCC; see the file COPYING.  I
 Software Foundation, 59 Temple Place - Suite 330, Boston, MA
 02111-1307, USA.  */
 
+/* Coverage information is held in two files.  A basic block graph
+   file, which is generated by the compiler, and a counter file, which
+   is generated by the program under test.  Both files use a similar
+   structure.  We do not attempt to make these files backwards
+   compatible with previous versions, as you only need coverage
+   information when developing a program.  We do hold version
+   information, so that mismatches can be detected, and we use a
+   format that allows tools to skip information they do not understand
+   or are not interested in.
+
+   Numbers are recorded in big endian unsigned binary form.  Either in
+   32 or 64 bits.  Strings are stored with a length count and NUL
+   terminator, and 0 to 3 bytes of zero padding up to the next 4 byte
+   boundary.  Zero length and NULL strings are simply stored as a
+   length of zero (they have no trailing NUL or padding).
+
+   	int32:  byte3 byte2 byte1 byte0
+	int64:  byte7 byte6 byte5 byte4 byte3 byte2 byte1 byte0
+	string: int32:0 | int32:length char* char:0 padding
+	padding: | char:0 | char:0 char:0 | char:0 char:0 char:0
+	item: int32 | int64 | string
+
+   The basic format of the files is
+
+   	file : int32:magic int32:version record*
+
+   The magic ident is different for the bbg and the counter files.
+   The version is the same for both files and is derived from gcc's
+   version number.  Although the ident and version are formally 32 bit
+   numbers, they are derived from 4 character ASCII strings.  The
+   version number consists of the single character major version
+   number, a two character minor version number (leading zero for
+   versions less than 10), and a single character indicating the
+   status of the release.  That will be 'e' experimental, 'p'
+   prerelease and 'r' for release.  Because, by good fortune, these are
+   in alphabetical order, string collating can be used to compare
+   version strings, and because numbers are stored big endian, numeric
+   comparison can be used when it is read as a 32 bit value.  Be aware
+   that the 'e' designation will (naturally) be unstable and might be
+   incompatible with itself.  For gcc 3.4 experimental, it would be
+   '304e' (0x33303465).  When the major version reaches 10, the letters
+   A-Z will be used.  Assuming minor increments releases every 6
+   months, we have to make a major increment every 50 years.  Assuming
+   major increments releases every 5 years, we're ok for the next 155
+   years -- good enough for me.
+
+   A record has a tag, length and variable amount of data.
+
+   	record: header data
+	header: int32:tag int32:length
+	data: item*
+
+   Records are not nested, but there is a record hierarchy.  Tag
+   numbers reflect this hierarchy.  Tags are unique across bbg and da
+   files.  Some record types have a varying amount of data.  The LENGTH
+   is usually used to determine how much data.  The tag value is split
+   into 4 8-bit fields, one for each of four possible levels.  The
+   most significant is allocated first.  Unused levels are zero.
+   Active levels are odd-valued, so that the LSB of the level is one.
+   A sub-level incorporates the values of its superlevels.  This
+   formatting allows you to determine the tag heirarchy, without
+   understanding the tags themselves, and is similar to the standard
+   section numbering used in technical documents.  Level values
+   [1..3f] are used for common tags, values [41..9f] for the graph
+   file and [a1..ff] for the counter file.
+
+   The basic block graph file contains the following records
+   	bbg:  function-graph*
+	function-graph: announce_function basic_blocks {arcs | lines}*
+	announce_function: header string:name int32:checksum
+	basic_block: header int32:flags*
+	arcs: header int32:block_no arc*
+	arc:  int32:dest_block int32:flags
+        lines: header int32:block_no line*
+               int32:0 string:NULL
+	line:  int32:line_no | int32:0 string:filename
+
+   The BASIC_BLOCK record holds per-bb flags.  The number of blocks
+   can be inferred from its data length.  There is one ARCS record per
+   basic block.  The number of arcs from a bb is implicit from the
+   data length.  It enumerates the destination bb and per-arc flags.
+   There is one LINES record per basic block, it enumerates the source
+   lines which belong to that basic block.  Source file names are
+   introduced by a line number of 0, following lines are from the new
+   source file.  The initial source file for the function is NULL, but
+   the current source file should be remembered from one LINES record
+   to the next.  The end of a block is indicated by an empty filename
+   - this does not reset the current source file.  Note there is no
+   ordering of the ARCS and LINES records: they may be in any order,
+   interleaved in any manner.  The current filename follows the order
+   the LINES records are stored in the file, *not* the ordering of the
+   blocks they are for.
+
+   The data file contains the following records.
+        da:   {function-data* summary:object summary:program*}*
+        function-data:	announce_function arc_counts
+	announce_function: header string:name int32:checksum
+	arc_counts: header int64:count*
+	summary: in32:checksum int32:runs int32:arcs int64:sum int64:max \
+		int64:max_sum int64:sum_max
+
+   The ANNOUNCE_FUNCTION record is the same as that in the BBG file.
+   The ARC_COUNTS gives the counter values for those arcs that are
+   instrumented.  The SUMMARY records give information about the whole
+   object file and about the whole program.  The checksum is used for
+   whole program summaries, and disambiguates different programs which
+   include the same instrumented object file.  There may be several
+   program summaries, each with a unique checksum.  The object
+   summary's checkum is zero.  Note that the da file might contain
+   information from several runs concatenated, or the data might be
+   merged.
+
+   This file is included by both the compiler, gcov tools and the
+   library.  The IN_LIBGCC2 define distinguishes these cases.  When
+   IN_LIBGCC2 is nonzero, we're building libgcc2 for the target and
+   know the compiler is (the just built) gcc.  Otherwise we're
+   generating code for the host, and the compiler may or may not be
+   gcc.  In this latter case, you must ensure that 'gcov_type' is
+   typedefed to something suitable (unsigned HOST_WIDEST_INT is
+   usually what you want).  */
+
 /* As a special exception, if you link this library with other files,
    some of which are compiled with GCC, to produce an executable,
    this library does not by itself cause the resulting executable
@@ -29,277 +151,426 @@ Software Foundation, 59 Temple Place - S
 
 #ifndef GCC_GCOV_IO_H
 #define GCC_GCOV_IO_H
-#include <stdio.h>
-#include <sys/types.h>
-
-static int __fetch_long	PARAMS ((long *, char *, size_t))
-	ATTRIBUTE_UNUSED;
-static int __read_long  PARAMS ((long *, FILE *, size_t))
-	ATTRIBUTE_UNUSED;
-static int __write_long PARAMS ((long, FILE *, size_t))
-	ATTRIBUTE_UNUSED;
-static int __fetch_gcov_type PARAMS ((gcov_type *, char *, size_t))
-	ATTRIBUTE_UNUSED;
-static int __store_gcov_type PARAMS ((gcov_type, char *, size_t))
-	ATTRIBUTE_UNUSED;
-static int __read_gcov_type  PARAMS ((gcov_type *, FILE *, size_t))
-	ATTRIBUTE_UNUSED;
-static int __write_gcov_type PARAMS ((gcov_type, FILE *, size_t))
-	ATTRIBUTE_UNUSED;
-static int __write_gcov_string PARAMS ((const char *, size_t, FILE*, long))
-	ATTRIBUTE_UNUSED;
-static int __read_gcov_string PARAMS ((char *, size_t, FILE*, long))
-	ATTRIBUTE_UNUSED;
-
-/* These routines only work for signed values.  */
-
-/* Store a portable representation of VALUE in DEST using BYTES*8-1 bits.
-   Return a nonzero value if VALUE requires more than BYTES*8-1 bits
-   to store.  */
-
-static int
-__store_gcov_type (value, dest, bytes)
-     gcov_type value;
-     char *dest;
-     size_t bytes;
-{
-  int upper_bit = (value < 0 ? 128 : 0);
-  size_t i;
-
-  if (value < 0)
-    {
-      gcov_type oldvalue = value;
-      value = -value;
-      if (oldvalue != -value)
-	return 1;
-    }
-
-  for(i = 0 ; i < (sizeof (value) < bytes ? sizeof (value) : bytes) ; i++) {
-    dest[i] = value & (i == (bytes - 1) ? 127 : 255);
-    value = value / 256;
-  }
-
-  if (value && value != -1)
-    return 1;
 
-  for(; i < bytes ; i++)
-    dest[i] = 0;
-  dest[bytes - 1] |= upper_bit;
-  return 0;
-}
+#if IN_LIBGCC2
+#if LONG_TYPE_SIZE == GCOV_TYPE_SIZE
+typedef long gcov_type;
+#else
+typedef long long gcov_type;
+#endif
+#endif /* IN_LIBGCC2 */
+
+/* File suffixes.  */
+#define GCOV_DATA_SUFFIX ".da"
+#define GCOV_GRAPH_SUFFIX ".bbg"
+
+/* File magic.  */
+#define GCOV_DATA_MAGIC  0x67636f76 /* "gcov" */
+#define GCOV_GRAPH_MAGIC 0x67626267 /* "gbbg" */
+
+/* gcov-iov.h is automatically generated by the makefile from
+   version.c, it looks like
+   	#define GCOV_VERSION ((unsigned)0x89abcdef)
+*/
+#include "gcov-iov.h"
+
+/* The record tags.  Values [1..3f] are for tags which may be in either
+   file.  Values [41..9f] for those in the bbg file and [a1..ff] for
+   the data file.  */
+
+#define GCOV_TAG_FUNCTION	 ((unsigned)0x01000000)
+#define GCOV_TAG_BLOCKS		 ((unsigned)0x01410000)
+#define GCOV_TAG_ARCS		 ((unsigned)0x01430000)
+#define GCOV_TAG_LINES		 ((unsigned)0x01450000)
+#define GCOV_TAG_ARC_COUNTS  	 ((unsigned)0x01a10000)
+#define GCOV_TAG_LOOP_HISTOGRAMS ((unsigned)0x01a30000)
+#define GCOV_TAG_VALUE_HISTOGRAMS ((unsigned)0x01a50000)
+#define GCOV_TAG_SAME_VALUE_HISTOGRAMS ((unsigned)0x01a70000)
+#define GCOV_TAG_OBJECT_SUMMARY  ((unsigned)0xa1000000)
+#define GCOV_TAG_PROGRAM_SUMMARY ((unsigned)0xa3000000)
+#define GCOV_TAG_PLACEHOLDER_SUMMARY ((unsigned)0xa5000000)
+#define GCOV_TAG_INCORRECT_SUMMARY ((unsigned)0xa7000000)
+
+/* The tag level mask has 1's in the position of the inner levels, &
+   the lsb of the current level, and zero on the current and outer
+   levels.  */
+#define GCOV_TAG_MASK(TAG) (((TAG) - 1) ^ (TAG))
+
+/* Return nonzero if SUB is an immediate subtag of TAG.  */
+#define GCOV_TAG_IS_SUBTAG(TAG,SUB)				\
+	(GCOV_TAG_MASK (TAG) >> 8 == GCOV_TAG_MASK (SUB) 	\
+	 && !(((SUB) ^ (TAG)) & ~GCOV_TAG_MASK(TAG)))
+
+/* Return nonzero if SUB is at a sublevel to TAG.  */
+#define GCOV_TAG_IS_SUBLEVEL(TAG,SUB)				\
+     	(GCOV_TAG_MASK (TAG) > GCOV_TAG_MASK (SUB))
+
+/* Basic block flags.  */
+#define GCOV_BLOCK_UNEXPECTED	(1 << 0)
+
+/* Arc flags.  */
+#define GCOV_ARC_ON_TREE 	(1 << 0)
+#define GCOV_ARC_FAKE		(1 << 1)
+#define GCOV_ARC_FALLTHROUGH	(1 << 2)
 
-/* Retrieve a quantity containing BYTES*8-1 bits from SOURCE and store
-   the result in DEST. Returns a nonzero value if the value in SOURCE
-   will not fit in DEST.  */
+/* Structured records.  */
 
-static int
-__fetch_gcov_type (dest, source, bytes)
-     gcov_type *dest;
-     char *source;
-     size_t bytes;
+/* Object & program summary record.  */
+struct gcov_summary
 {
-  gcov_type value = 0;
-  int i;
-
-  for (i = bytes - 1; (size_t) i > (sizeof (*dest) - 1); i--)
-    if (source[i] & ((size_t) i == (bytes - 1) ? 127 : 255 ))
-      return 1;
-
-  for (; i >= 0; i--)
-    value = value * 256 + (source[i] & ((size_t)i == (bytes - 1) ? 127 : 255));
-
-  if ((source[bytes - 1] & 128) && (value > 0))
-    value = - value;
+  unsigned checksum;	  /* checksum of program */
+  unsigned runs;	  /* number of program runs */
+  unsigned arcs;	  /* number of instrumented arcs */
+  gcov_type arc_sum;      /* sum of all arc counters */
+  gcov_type arc_max_one;  /* max counter on any one run */
+  gcov_type arc_max_sum;  /* maximum arc_sum */
+  gcov_type arc_sum_max;  /* sum of max_one */
+};
 
-  *dest = value;
-  return 0;
-}
+/* Structures embedded in coveraged program.  The structures generated
+   by write_profile must match these.  */
 
-static int
-__fetch_long (dest, source, bytes)
-     long *dest;
-     char *source;
-     size_t bytes;
+/* Information about section of counters for a function.  */
+struct counter_section
 {
-  long value = 0;
-  int i;
+  unsigned tag;		/* Tag of the section.  */
+  unsigned n_counters;	/* Number of counters in the section.  */
+};
+
+#if IN_LIBGCC2
+/* Information about section of counters for an object file.  */
+struct counter_section_data
+{
+  unsigned tag;		/* Tag of the section.  */
+  unsigned n_counters;	/* Number of counters in the section.  */
+  gcov_type *counters;	/* The data.  */
+};
 
-  for (i = bytes - 1; (size_t) i > (sizeof (*dest) - 1); i--)
-    if (source[i] & ((size_t) i == (bytes - 1) ? 127 : 255 ))
-      return 1;
+/* Information about a single function.  */
+struct function_info
+{
+  const char *name;	        /* (mangled) name of function */
+  unsigned checksum;		/* function checksum */
+  unsigned n_counter_sections;	/* Number of types of counters */
+  const struct counter_section *counter_sections;
+  				/* The section descriptions */
+};
 
-  for (; i >= 0; i--)
-    value = value * 256 + (source[i] & ((size_t)i == (bytes - 1) ? 127 : 255));
+/* Information about a single object file.  */
+struct gcov_info
+{
+  unsigned long version;        /* expected version number */
+  struct gcov_info *next;	/* link to next, used by libgcc */
 
-  if ((source[bytes - 1] & 128) && (value > 0))
-    value = - value;
+  const char *filename;		/* output file name */
+  long wkspc;	  	        /* libgcc workspace */
 
-  *dest = value;
-  return 0;
-}
+  unsigned n_functions;             /* number of functions */
+  const struct function_info *functions; /* table of functions */
 
-/* Write a BYTES*8-bit quantity to FILE, portably. Returns a nonzero
-   value if the write fails, or if VALUE can't be stored in BYTES*8
-   bits.
+  unsigned n_counter_sections;	/* Number of types of counters */
+  const struct counter_section_data *counter_sections;
+  				/* The data to be put into the sections.  */
+};
+
+/* Register a new object file module.  */
+extern void __gcov_init (struct gcov_info *);
+
+/* Called before fork, to avoid double counting.  */
+extern void __gcov_flush (void);
+
+#endif /* IN_LIBGCC2 */
+
+/* Functions for reading and writing gcov files.  */
+static int gcov_write_unsigned PARAMS((FILE *, unsigned))
+     ATTRIBUTE_UNUSED;
+static int gcov_write_counter PARAMS((FILE *, gcov_type))
+     ATTRIBUTE_UNUSED;
+static int gcov_write_string PARAMS((FILE *, const char *, unsigned))
+     ATTRIBUTE_UNUSED;
+static int gcov_read_unsigned PARAMS((FILE *, unsigned *))
+     ATTRIBUTE_UNUSED;
+static int gcov_read_counter PARAMS((FILE *, gcov_type *, int))
+     ATTRIBUTE_UNUSED;
+#if !IN_LIBGCC2
+static int gcov_read_string PARAMS((FILE *, char **, unsigned *))
+     ATTRIBUTE_UNUSED;
+#endif
+static int gcov_read_summary PARAMS ((FILE *, struct gcov_summary *))
+     ATTRIBUTE_UNUSED;
+#if IN_LIBGCC2
+static int gcov_write_summary PARAMS ((FILE *, unsigned,
+				       const struct gcov_summary *))
+     ATTRIBUTE_UNUSED;
+#endif
+#define gcov_save_position(STREAM) \
+ 	ftell (STREAM)
+#define gcov_reserve_length(STREAM) \
+	(gcov_write_unsigned (STREAM, 0) ? 0 : ftell (STREAM) - 4)
+static int gcov_write_length PARAMS((FILE *, long))
+     ATTRIBUTE_UNUSED;
+#define gcov_resync(STREAM, BASE, LENGTH) \
+	fseek (STREAM, BASE + (long)LENGTH, SEEK_SET)
+#define gcov_skip(STREAM, LENGTH) \
+	fseek (STREAM, LENGTH, SEEK_CUR)
+#define gcov_skip_string(STREAM, LENGTH) \
+	fseek (STREAM, (LENGTH) + 4 - ((LENGTH) & 3), SEEK_CUR)
+typedef int (*merger_function) PARAMS ((FILE *, gcov_type *, unsigned));
+static int same_value_histograms_merger PARAMS ((FILE *, gcov_type *, unsigned))
+     ATTRIBUTE_UNUSED;
+static merger_function profile_merger_for_tag PARAMS ((unsigned))
+     ATTRIBUTE_UNUSED;
 
-   Note that VALUE may not actually be large enough to hold BYTES*8
-   bits, but BYTES characters will be written anyway.
 
-   BYTES may be a maximum of 10.  */
+/* Write VALUE to coverage file FILE.  Return nonzero if failed due to
+   file i/o error, or value error.  */
 
 static int
-__write_gcov_type (value, file, bytes)
-     gcov_type value;
+gcov_write_unsigned (file, value)
      FILE *file;
-     size_t bytes;
+     unsigned value;
 {
-  char c[10];
+  char buffer[4];
+  unsigned ix;
 
-  if (bytes > 10 || __store_gcov_type (value, c, bytes))
-    return 1;
-  else
-    return fwrite(c, 1, bytes, file) != bytes;
+  for (ix = sizeof (buffer); ix--; )
+    {
+      buffer[ix] = value;
+      value >>= 8;
+    }
+  return ((sizeof (value) > sizeof (buffer) && value)
+	  || fwrite (buffer, sizeof (buffer), 1, file) != 1);
 }
 
+/* Write VALUE to coverage file FILE.  Return nonzero if failed due to
+   file i/o error, or value error.  Negative values are not checked
+   here -- they are checked in gcov_read_counter.  */
+
 static int
-__write_long (value, file, bytes)
-     long value;
+gcov_write_counter (file, value)
      FILE *file;
-     size_t bytes;
+     gcov_type value;
 {
-  char c[10];
+  char buffer[8];
+  unsigned ix;
 
-  if (bytes > 10 || __store_gcov_type ((gcov_type)value, c, bytes))
-    return 1;
-  else
-    return fwrite(c, 1, bytes, file) != bytes;
+  for (ix = sizeof (buffer); ix--; )
+    {
+      buffer[ix] = value;
+      value >>= 8;
+    }
+  return ((sizeof (value) > sizeof (buffer) && value != 0 && value != -1)
+	  || fwrite (buffer, sizeof (buffer), 1, file) != 1);
 }
 
-/* Read a quantity containing BYTES bytes from FILE, portably. Return
-   a nonzero value if the read fails or if the value will not fit
-   in DEST.
-
-   Note that DEST may not be large enough to hold all of the requested
-   data, but the function will read BYTES characters anyway.
-
-   BYTES may be a maximum of 10.  */
+/* Write VALUE to coverage file FILE.  Return nonzero if failed due to
+   file i/o error, or value error.  */
 
 static int
-__read_gcov_type (dest, file, bytes)
-     gcov_type *dest;
+gcov_write_string (file, string, length)
      FILE *file;
-     size_t bytes;
+     unsigned length;
+     const char *string;
 {
-  char c[10];
+  unsigned pad = 0;
 
-  if (bytes > 10 || fread(c, 1, bytes, file) != bytes)
-    return 1;
+  if (string)
+    return (gcov_write_unsigned (file, length)
+	    || fwrite (string, length, 1, file) != 1
+	    || fwrite (&pad, 4 - (length & 3), 1, file) != 1);
   else
-    return __fetch_gcov_type (dest, c, bytes);
+    return gcov_write_unsigned (file, 0);
 }
 
+/* Read *VALUE_P from coverage file FILE.  Return nonzero if failed
+   due to file i/o error, or range error.  */
+
 static int
-__read_long (dest, file, bytes)
-     long *dest;
+gcov_read_unsigned (file, value_p)
      FILE *file;
-     size_t bytes;
+     unsigned *value_p;
 {
-  char c[10];
+  unsigned value = 0;
+  unsigned ix;
+  unsigned char buffer[4];
 
-  if (bytes > 10 || fread(c, 1, bytes, file) != bytes)
+  if (fread (buffer, sizeof (buffer), 1, file) != 1)
     return 1;
-  else
-    return __fetch_long (dest, c, bytes);
+  for (ix = sizeof (value); ix < sizeof (buffer); ix++)
+    if (buffer[ix])
+      return 1;
+  for (ix = 0; ix != sizeof (buffer); ix++)
+    {
+      value <<= 8;
+      value |= buffer[ix];
+    }
+  *value_p = value;
+  return 0;
 }
 
-
-/* Writes string in gcov format.  */
+/* Read *VALUE_P from coverage file FILE.  Return nonzero if failed
+   due to file i/o error, or range error.  If not MAY_BE_NEGATIVE,
+   report error if read value is negative.  */
 
 static int
-__write_gcov_string (string, length, file, delim)
-     const char *string;
-     size_t length;
+gcov_read_counter (file, value_p, may_be_negative)
      FILE *file;
-     long delim;
+     gcov_type *value_p;
+     int may_be_negative;
 {
-  size_t temp = length + 1;
+  gcov_type value = 0;
+  unsigned ix;
+  unsigned char buffer[8];
 
-  /* delimiter */
-  if (__write_long (delim, file, 4) != 0)
+  if (fread (buffer, sizeof (buffer), 1, file) != 1)
     return 1;
+  for (ix = sizeof (value); ix < sizeof (buffer); ix++)
+    if (buffer[ix])
+      return 1;
+  for (ix = 0; ix != sizeof (buffer); ix++)
+    {
+      value <<= 8;
+      value |= buffer[ix];
+    }
 
-  if (__write_long (length, file, 4) != 0)
-    return 1;
+  *value_p = value;
+  return !may_be_negative && value < 0;
+}
 
-  if (fwrite (string, temp, 1, file) != 1)
-    return 1;
+static int
+same_value_histograms_merger (da_file, counters, n_counters)
+     FILE *da_file;
+     gcov_type *counters;
+     unsigned n_counters;
+{
+  unsigned i, n_measures;
+  gcov_type value, counter, all;
 
-  temp &= 3;
+  if (n_counters % 3)
+    return 1;
 
-  if (temp)
+  n_measures = n_counters / 3;
+  for (i = 0; i < n_measures; i++, counters += 3)
     {
-      char c[4];
-
-      c[0] = c[1] = c[2] = c[3] = 0;
-
-      if (fwrite (c, sizeof (char), 4 - temp, file) != 4 - temp)
+      if (gcov_read_counter (da_file, &value, 1)
+	  || gcov_read_counter (da_file, &counter, 0)
+	  || gcov_read_counter (da_file, &all, 0))
 	return 1;
-    }
-
-  if (__write_long (delim, file, 4) != 0)
-    return 1;
 
+      if (counters[0] == value)
+	counters[1] += counter;
+      else if (counter > counters[1])
+	{
+	  counters[0] = value;
+	  counters[1] = counter - counters[1];
+	}
+      else
+	counters[1] -= counter;
+      counters[2] += all;
+    }
   return 0;
 }
 
-/* Reads string in gcov format.  */
+static merger_function
+profile_merger_for_tag (tag)
+     unsigned tag;
+{
+  switch (tag)
+    {
+    case GCOV_TAG_SAME_VALUE_HISTOGRAMS:
+      return same_value_histograms_merger;
+    default:
+      return 0;
+    }
+}
+
+#if !IN_LIBGCC2
 
+/* Read string from coverage file FILE.  Length is stored in *LENGTH_P
+   (if non-null), a buffer is allocated and returned in *STRING_P.
+   Return nonzero if failed due to file i/o error, or range
+   error.  Uses xmalloc to allocate the string buffer.  */
 
 static int
-__read_gcov_string (string, max_length, file, delim)
-     char *string;
-     size_t max_length;
+gcov_read_string (file, string_p, length_p)
      FILE *file;
-     long delim;
+     char **string_p;
+     unsigned *length_p;
 {
-  long delim_from_file;
-  long length;
-  long read_length;
-  long tmp;
+  unsigned length;
 
-  if (__read_long (&delim_from_file, file, 4) != 0)
+  if (gcov_read_unsigned (file, &length))
     return 1;
 
-  if (delim_from_file != delim)
-    return 1;
-
-  if (__read_long (&length, file, 4) != 0)
-    return 1;
+  if (length_p)
+    *length_p = length;
+  free (*string_p);
 
-  if (length > (long) max_length)
-    read_length = max_length;
-  else
-    read_length = length;
+  *string_p = NULL;
+  if (!length)
+    return 0;
 
-  tmp = (((length + 1) - 1) / 4 + 1) * 4;
-  /* This is the size occupied by the string in the file */
+  length += 4 - (length & 3);
+  *string_p = (char *) xmalloc (length);
 
-  if (fread (string, read_length, 1, file) != 1)
-    return 1;
+  return fread (*string_p, length, 1, file) != 1;
 
-  string[read_length] = 0;
+}
 
-  if (fseek (file, tmp - read_length, SEEK_CUR) < 0)
-    return 1;
+#endif /* !IN_LIBGCC2 */
 
-  if (__read_long (&delim_from_file, file, 4) != 0)
-    return 1;
+/* Write a record length at PLACE.  The current file position is the
+   end of the record, and is restored before returning.  Returns
+   nonzero on failure.  */
 
-  if (delim_from_file != delim)
-    return 1;
+static int
+gcov_write_length (file, place)
+     FILE *file;
+     long place;
+{
+  long here = ftell (file);
+  int result = (!place || fseek (file, place, SEEK_SET)
+		|| gcov_write_unsigned (file, here - place - 4));
+  if (fseek (file, here, SEEK_SET))
+    result = 1;
+  return result;
+}
 
-  return 0;
+#define GCOV_SUMMARY_LENGTH 44
+static int
+gcov_read_summary (da_file, summary)
+     FILE *da_file;
+     struct gcov_summary *summary;
+{
+  return (gcov_read_unsigned (da_file, &summary->checksum)
+	  || gcov_read_unsigned (da_file, &summary->runs)
+	  || gcov_read_unsigned (da_file, &summary->arcs)
+	  || gcov_read_counter (da_file, &summary->arc_sum, 0)
+	  || gcov_read_counter (da_file, &summary->arc_max_one, 0)
+	  || gcov_read_counter (da_file, &summary->arc_max_sum, 0)
+	  || gcov_read_counter (da_file, &summary->arc_sum_max, 0));
 }
 
+#if IN_LIBGCC2
+static int
+gcov_write_summary (da_file, tag, summary)
+     FILE *da_file;
+     unsigned tag;
+     const struct gcov_summary *summary;
+{
+  long base;
+
+  return (gcov_write_unsigned (da_file, tag)
+	  || !(base = gcov_reserve_length (da_file))
+	  || gcov_write_unsigned (da_file, summary->checksum)
+	  || gcov_write_unsigned (da_file, summary->runs)
+	  || gcov_write_unsigned (da_file, summary->arcs)
+	  || gcov_write_counter (da_file, summary->arc_sum)
+	  || gcov_write_counter (da_file, summary->arc_max_one)
+	  || gcov_write_counter (da_file, summary->arc_max_sum)
+	  || gcov_write_counter (da_file, summary->arc_sum_max)
+	  || gcov_write_length (da_file, base));
+}
+#endif
 
-#endif /* ! GCC_GCOV_IO_H */
+#endif /* GCC_GCOV_IO_H */
--- gcc-3.3.1/gcc/gcov-iov.c.hammer-branch	2003-08-05 18:22:47.000000000 +0200
+++ gcc-3.3.1/gcc/gcov-iov.c	2003-08-05 18:22:47.000000000 +0200
@@ -0,0 +1,68 @@
+/* Generate gcov version string from version.c. See gcov-io.h for
+   description of how the version string is generated.
+   Copyright (C) 2002 Free Software Foundation, Inc.
+   Contributed by Nathan Sidwell <nathan@codesourcery.com>
+   
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+#include "hconfig.h"
+#include "system.h"
+#include "version.c" /* We want the actual string.  */
+
+int main PARAMS ((int, char **));
+
+int
+main (argc, argv)
+     int argc ATTRIBUTE_UNUSED;
+     char **argv;
+{
+  unsigned version = 0;
+  unsigned char v[4];
+  unsigned ix;
+  char const *ptr = version_string;
+  unsigned major, minor = 0;
+  char s = 0;
+
+  major = atoi (ptr);
+  while (*ptr && *ptr != '.')
+    ptr++;
+  if (*ptr)
+    minor = atoi (ptr + 1);
+  while (*ptr)
+    if (*ptr++ == '(')
+      {
+	s = *ptr;
+	break;
+      }
+
+  v[0] = (major < 10 ? '0' : 'A' - 10) + major;
+  v[1] = (minor / 10) + '0';
+  v[2] = (minor % 10) + '0';
+  v[3] = s ? s : '*';
+    
+  for (ix = 0; ix != 4; ix++)
+    version = (version << 8) | v[ix];
+
+  printf ("/* Generated automatically by the program `%s'\n", argv[0]);
+  printf ("   from `%s'.  */\n", version_string);
+  printf ("\n");
+  printf ("#define GCOV_VERSION ((unsigned)%#08x)  /* %.4s */\n",
+	  version, v);
+  
+  return 0;
+}
--- gcc-3.3.1/gcc/gcov.c.hammer-branch	2002-10-08 21:45:17.000000000 +0200
+++ gcc-3.3.1/gcc/gcov.c	2003-08-05 18:22:47.000000000 +0200
@@ -4,6 +4,7 @@
    1999, 2000, 2001, 2002 Free Software Foundation, Inc.
    Contributed by James E. Wilson of Cygnus Support.
    Mangled by Bob Manson of Cygnus Support.
+   Mangled further by Nathan Sidwell <nathan@codesourcery.com>
 
 Gcov is free software; you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
@@ -20,14 +21,6 @@ along with Gcov; see the file COPYING.  
 the Free Software Foundation, 59 Temple Place - Suite 330,
 Boston, MA 02111-1307, USA.  */
 
-/* ??? The code in final.c that produces the struct bb assumes that there is
-   no padding between the fields.  This is not necessary true.  The current
-   code can only be trusted if longs and pointers are the same size.  */
-
-/* ??? No need to print an execution count on every line, could just print
-   it on the first line of each block, and only print it on a subsequent
-   line in the same block if the count changes.  */
-
 /* ??? Print a list of the ten blocks with the highest execution counts,
    and list the line numbers corresponding to those blocks.  Also, perhaps
    list the line numbers with the highest execution counts, only printing
@@ -36,12 +29,17 @@ Boston, MA 02111-1307, USA.  */
 /* ??? Should have an option to print the number of basic blocks, and the
    percent of them that are covered.  */
 
-/* ??? Does not correctly handle the case where two .bb files refer to the
-   same included source file.  For example, if one has a short file containing
-   only inline functions, which is then included in two other files, then
-   there will be two .bb files which refer to the include file, but there
-   is no way to get the total execution counts for the included file, can
-   only get execution counts for one or the other of the including files.  */
+/* ??? Does not correctly handle the case where two .bb files refer to
+   the same included source file.  For example, if one has a short
+   file containing only inline functions, which is then included in
+   two other files, then there will be two .bb files which refer to
+   the include file, but there is no way to get the total execution
+   counts for the included file, can only get execution counts for one
+   or the other of the including files. this can be fixed by --ratios
+   --long-file-names --preserve-paths and perl.  */
+
+/* Need an option to show individual block counts, and show
+   probabilities of fall through arcs.  */
 
 #include "config.h"
 #include "system.h"
@@ -54,120 +52,102 @@ Boston, MA 02111-1307, USA.  */
 typedef HOST_WIDEST_INT gcov_type;
 #include "gcov-io.h"
 
-/* The .bb file format consists of several lists of 4-byte integers
-   which are the line numbers of each basic block in the file.  Each
-   list is terminated by a zero.  These lists correspond to the basic
-   blocks in the reconstructed program flow graph.
-
-   A line number of -1 indicates that a source file name (padded to a
-   long boundary) follows.  The padded file name is followed by
-   another -1 to make it easy to scan past file names.  A -2 indicates
-   that a function name (padded to a long boundary) follows; the name
-   is followed by another -2 to make it easy to scan past the function
-   name.
-
-   The .bbg file contains enough info to enable gcov to reconstruct the
-   program flow graph.  The first word is the number of basic blocks,
-   the second word is the number of arcs, followed by the list of arcs
-   (source bb, dest bb pairs), then a -1, then the number of instrumented
-   arcs followed by the instrumented arcs, followed by another -1.  This
-   is repeated for each function.
-
-   The .da file contains the execution count for each instrumented branch.
-
-   The .bb and .bbg files are created by giving GCC the -ftest-coverage option,
-   and the .da files are created when an executable compiled with
-   -fprofile-arcs is run.  */
+/* The bbg file is generated by -ftest-coverage option. The da file is
+   generated by a program compiled with -fprofile-arcs. Their formats
+   are documented in gcov-io.h.  */
 
 /* The functions in this file for creating and solution program flow graphs
-   are very similar to functions in the gcc source file profile.c.  */
+   are very similar to functions in the gcc source file profile.c.  In
+   some places we make use of the knowledge of how profile.c works to
+   select particular algorithms here.  */
 
 /* This is the size of the buffer used to read in source file lines.  */
 
 #define STRING_SIZE 200
 
-/* One copy of this structure is created for each source file mentioned in the
-   .bb file.  */
+struct function_info;
+struct block_info;
 
-struct sourcefile
-{
-  char *name;
-  int maxlineno;
-  struct sourcefile *next;
-};
-
-/* This points to the head of the sourcefile structure list.  */
+/* Describes an arc between two basic blocks.  */
 
-struct sourcefile *sources;
+typedef struct arc_info
+{
+  /* source and destination blocks. */
+  struct block_info *src;
+  struct block_info *dst;
 
-/* One of these is dynamically created whenever we identify an arc in the
-   function.  */
+  /* transition counts.  */
+  gcov_type count;
 
-struct adj_list
-{
-  int source;
-  int target;
-  gcov_type arc_count;
   unsigned int count_valid : 1;
   unsigned int on_tree : 1;
   unsigned int fake : 1;
   unsigned int fall_through : 1;
-#if 0
-  /* Not needed for gcov, but defined in profile.c.  */
-  rtx branch_insn;
-#endif
-  struct adj_list *pred_next;
-  struct adj_list *succ_next;
-};
 
-/* Count the number of basic blocks, and create an array of these structures,
-   one for each bb in the function.  */
+  /* Arc to a call.  */
+  unsigned int is_call : 1;
+  
+  /* Next branch on line.  */
+  struct arc_info *line_next;
+  
+  /* Links to next arc on src and dst lists.  */
+  struct arc_info *succ_next;
+  struct arc_info *pred_next;
+} arc_t;
+
+/* Describes a basic block. Contains lists of arcs to successor and
+   predecessor blocks.  */
 
-struct bb_info
+typedef struct block_info
 {
-  struct adj_list *succ;
-  struct adj_list *pred;
-  gcov_type succ_count;
-  gcov_type pred_count;
-  gcov_type exec_count;
-  unsigned int count_valid : 1;
-  unsigned int on_tree : 1;
-#if 0
-  /* Not needed for gcov, but defined in profile.c.  */
-  rtx first_insn;
-#endif
-};
+  /* Chain of exit and entry arcs.  */
+  arc_t *succ;
+  arc_t *pred;
 
-/* When outputting branch probabilities, one of these structures is created
-   for each branch/call.  */
+  /* Number of unprocessed exit and entry arcs. */
+  gcov_type num_succ;
+  gcov_type num_pred;
 
-struct arcdata
-{
-  gcov_type hits;
-  gcov_type total;
-  int call_insn;
-  struct arcdata *next;
-};
+  /* Block execution count. */
+  gcov_type count;
+  unsigned count_valid : 1;
+  unsigned valid_chain : 1;
+  unsigned invalid_chain : 1;
 
-/* Used to save the list of bb_graphs, one per function.  */
+  /* Array of line numbers and source files. source files are
+     introduced by a linenumber of zero, the next 'line number' is the
+     number of the source file.  Always starts with a source file.  */
+  unsigned *encoding;
+  unsigned num_encodings;
 
-struct bb_info_list
-{
-  /* Indexed by block number, holds the basic block graph for one function.  */
-  struct bb_info *bb_graph;
-  int num_blocks;
-  struct bb_info_list *next;
-};
+  /* Temporary chain for solving graph. */
+  struct block_info *chain;
+  
+} block_t;
+
+/* Describes a single function. Contains an array of basic blocks.  */
 
-/* Used to hold information about each line.  */
-struct line_info
+typedef struct function_info
 {
-  gcov_type count;	      /* execution count */
-  struct arcdata *branches;   /* list of branch probabilities for line.  */
-  unsigned exists : 1;	      /* has code associated with it.  */
-};
+  /* Name of function.  */
+  char *name;
+  unsigned checksum;
+
+  /* Array of basic blocks.  */
+  block_t *blocks;
+  unsigned num_blocks;
+
+  /* Raw arc coverage counts.  */
+  gcov_type *counts;
+  unsigned num_counts;
   
-struct coverage
+  /* Next function.  */
+  struct function_info *next;
+} function_t;
+
+/* Describes coverage of a file or function.  */
+
+typedef struct coverage_info
 {
   int lines;
   int lines_executed;
@@ -180,104 +160,111 @@ struct coverage
   int calls_executed;
   
   char *name;
-};
+} coverage_t;
 
-/* Holds a list of function basic block graphs.  */
+/* Describes a single line of source. Contains a chain of basic blocks
+   with code on it.  */
 
-static struct bb_info_list *bb_graph_list = 0;
+typedef struct line_info
+{
+  gcov_type count;	   /* execution count */
+  arc_t *branches; 	   /* branches from blocks that end on this
+			      line. */
+  unsigned exists : 1;
+} line_t;
 
-/* Modification time of data files.  */
+/* Describes a file mentioned in the block graph.  Contains an array
+   of line info.  */
 
-static time_t bb_file_time;
+typedef struct source_info
+{
+  /* Name of source file.  */
+  char *name;
+  unsigned index;
 
-/* Name and file pointer of the input file for the basic block graph.  */
+  /* Array of line information. */
+  line_t *lines;
+  unsigned num_lines;
 
-static char *bbg_file_name;
-static FILE *bbg_file;
+  coverage_t coverage;
+  
+  /* Next source file.  */
+  struct source_info *next;
+} source_t;
 
-/* Name and file pointer of the input file for the arc count data.  */
+/* Holds a list of function basic block graphs.  */
 
-static char *da_file_name;
-static FILE *da_file;
+static function_t *functions;
 
-/* Name and file pointer of the input file for the basic block line counts.  */
+/* This points to the head of the sourcefile structure list.  */
 
-static char *bb_file_name;
-static FILE *bb_file;
+static source_t *sources;
 
-/* Holds the entire contents of the bb_file read into memory.  */
+/* Modification time of graph file. */
 
-static char *bb_data;
+static time_t bbg_file_time;
 
-/* Size of bb_data array in longs.  */
+/* Name and file pointer of the input file for the basic block graph.  */
 
-static long bb_data_size;
+static char *bbg_file_name;
 
-/* Name of the file mentioned on the command line.  */
+/* Name and file pointer of the input file for the arc count data.  */
 
-static char *input_file_name = 0;
+static char *da_file_name;
 
-/* Output branch probabilities if true.  */
+/* Output branch probabilities.  */
 
-static int output_branch_probs = 0;
+static int flag_branches = 0;
 
 /* Output a gcov file if this is true.  This is on by default, and can
    be turned off by the -n option.  */
 
-static int output_gcov_file = 1;
+static int flag_gcov_file = 1;
 
-/* For included files, make the gcov output file name include the name of
-   the input source file.  For example, if x.h is included in a.c, then the
-   output file name is a.c.x.h.gcov instead of x.h.gcov.  This works only
-   when a single source file is specified.  */
+/* For included files, make the gcov output file name include the name
+   of the input source file.  For example, if x.h is included in a.c,
+   then the output file name is a.c##x.h.gcov instead of x.h.gcov.  */
 
-static int output_long_names = 0;
+static int flag_long_names = 0;
 
 /* Output summary info for each function.  */
 
-static int output_function_summary = 0;
+static int flag_function_summary = 0;
 
-/* Object directory file prefix.  This is the directory/file
-   where .bb and .bbg files are looked for, if nonzero.  */
+/* Object directory file prefix.  This is the directory/file where the
+   graph and data files are looked for, if nonzero.  */
 
 static char *object_directory = 0;
 
 /* Preserve all pathname components. Needed when object files and
-   source files are in subdirectories.  */
-static int preserve_paths = 0;
+   source files are in subdirectories. '/' is mangled as '#', '.' is
+   elided and '..' mangled to '^'.  */
+
+static int flag_preserve_paths = 0;
 
 /* Output the number of times a branch was taken as opposed to the percentage
-   of times it was taken.  Turned on by the -c option */
+   of times it was taken. */
 
-static int output_branch_counts = 0;
+static int flag_counts = 0;
 
 /* Forward declarations.  */
-static void process_args PARAMS ((int, char **));
-static void open_files PARAMS ((void));
-static void read_files PARAMS ((void));
-static void scan_for_source_files PARAMS ((void));
-static void output_data PARAMS ((struct sourcefile *));
+static void fnotice PARAMS ((FILE *, const char *, ...)) ATTRIBUTE_PRINTF_2;
+static int process_args PARAMS ((int, char **));
 static void print_usage PARAMS ((int)) ATTRIBUTE_NORETURN;
 static void print_version PARAMS ((void)) ATTRIBUTE_NORETURN;
-static void init_arc PARAMS ((struct adj_list *, int, int, struct bb_info *));
-static struct adj_list *reverse_arcs PARAMS ((struct adj_list *));
-static gcov_type *read_profile PARAMS ((char *, long, int));
-static void create_program_flow_graph PARAMS ((struct bb_info_list *));
-static void solve_program_flow_graph PARAMS ((struct bb_info_list *));
-static void accumulate_branch_counts PARAMS ((struct coverage *,
-					      struct arcdata *));
-static void calculate_branch_probs PARAMS ((struct bb_info *,
-					    struct line_info *,
-					    struct coverage *));
-static void function_summary PARAMS ((struct coverage *, const char *));
-static void init_line_info PARAMS ((struct line_info *,
-				    struct coverage *, long));
-static void output_line_info PARAMS ((FILE *, const struct line_info *,
-				      const struct coverage *, long));
-static char *make_gcov_file_name PARAMS ((char *));
-static const char *format_hwint PARAMS ((HOST_WIDEST_INT, HOST_WIDEST_INT,
-					 int));
-
+static void process_file PARAMS ((const char *));
+static void create_file_names PARAMS ((const char *));
+static int read_graph_file PARAMS ((void));
+static int read_count_file PARAMS ((void));
+static void solve_flow_graph PARAMS ((function_t *));
+static void add_branch_counts PARAMS ((coverage_t *, const arc_t *));
+static void add_line_counts PARAMS ((coverage_t *, const function_t *));
+static void function_summary PARAMS ((const coverage_t *, const char *));
+static const char *format_gcov PARAMS ((gcov_type, gcov_type, int));
+static void accumulate_line_counts PARAMS ((source_t *));
+static void output_lines PARAMS ((FILE *, const source_t *));
+static char *make_gcov_file_name PARAMS ((const char *, const char *));
+static void release_structures PARAMS ((void));
 extern int main PARAMS ((int, char **));
 
 int
@@ -285,25 +272,24 @@ main (argc, argv)
      int argc;
      char **argv;
 {
-  struct sourcefile *s_ptr;
+  int argno;
   
   gcc_init_libintl ();
 
-  process_args (argc, argv);
-
-  open_files ();
-
-  read_files ();
-
-  scan_for_source_files ();
-
-  for (s_ptr = sources; s_ptr; s_ptr = s_ptr->next)
-    output_data (s_ptr);
+  argno = process_args (argc, argv);
+  if (optind == argc)
+    print_usage (true);
 
+  for (; argno != argc; argno++)
+    {
+      release_structures ();
+      
+      process_file (argv[argno]);
+    }
+  
   return 0;
 }
 
-static void fnotice PARAMS ((FILE *, const char *, ...)) ATTRIBUTE_PRINTF_2;
 static void
 fnotice VPARAMS ((FILE *file, const char *msgid, ...))
 {
@@ -335,6 +321,7 @@ print_usage (error_p)
 {
   FILE *file = error_p ? stderr : stdout;
   int status = error_p ? FATAL_EXIT_CODE : SUCCESS_EXIT_CODE;
+  
   fnotice (file, "Usage: gcov [OPTION]... SOURCEFILE\n\n");
   fnotice (file, "Print code coverage information.\n\n");
   fnotice (file, "  -h, --help                      Print this help, then exit\n");
@@ -358,8 +345,14 @@ print_usage (error_p)
 static void
 print_version ()
 {
-  fnotice (stdout, "gcov (GCC) %s\n", version_string);
-  fnotice (stdout, "Copyright (C) 2001 Free Software Foundation, Inc.\n");
+  char v[4];
+  unsigned version = GCOV_VERSION;
+  unsigned ix;
+
+  for (ix = 4; ix--; version >>= 8)
+    v[ix] = version;
+  fnotice (stdout, "gcov %.4s (GCC %s)\n", v, version_string);
+  fnotice (stdout, "Copyright (C) 2002 Free Software Foundation, Inc.\n");
   fnotice (stdout,
 	   "This is free software; see the source for copying conditions.  There is NO\n\
 warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n");
@@ -380,9 +373,9 @@ static const struct option options[] =
   { "object-file",          required_argument, NULL, 'o' },
 };
 
-/* Parse the command line.  */
+/* Process args, return index to first non-arg.  */
 
-static void
+static int
 process_args (argc, argv)
      int argc;
      char **argv;
@@ -400,25 +393,25 @@ process_args (argc, argv)
 	  print_version ();
 	  /* print_version will exit.  */
 	case 'b':
-	  output_branch_probs = 1;
+	  flag_branches = 1;
 	  break;
 	case 'c':
-	  output_branch_counts = 1;
+	  flag_counts = 1;
 	  break;
 	case 'n':
-	  output_gcov_file = 0;
+	  flag_gcov_file = 0;
 	  break;
 	case 'l':
-	  output_long_names = 1;
+	  flag_long_names = 1;
 	  break;
 	case 'f':
-	  output_function_summary = 1;
+	  flag_function_summary = 1;
 	  break;
 	case 'o':
 	  object_directory = optarg;
 	  break;
 	case 'p':
-	  preserve_paths = 1;
+	  flag_preserve_paths = 1;
 	  break;
 	default:
 	  print_usage (true);
@@ -426,27 +419,135 @@ process_args (argc, argv)
 	}
     }
 
-  if (optind != argc - 1)
-    print_usage (true);
+  return optind;
+}
+
+/* Process a single source file.  */
 
-  input_file_name = argv[optind];
+static void
+process_file (file_name)
+     const char *file_name;
+{
+  source_t *src;
+  function_t *fn;
+  
+  create_file_names (file_name);
+  if (read_graph_file ())
+    return;
+  
+  if (!functions)
+    {
+      fnotice (stderr, "%s:no functions found\n", bbg_file_name);
+      return;
+    }
+  
+  if (read_count_file ())
+    return;
+  
+  for (fn = functions; fn; fn = fn->next)
+    solve_flow_graph (fn);
+  for (src = sources; src; src = src->next)
+    src->lines = (line_t *) xcalloc (src->num_lines, sizeof (line_t));
+  for (fn = functions; fn; fn = fn->next)
+    {
+      coverage_t coverage;
+      
+      memset (&coverage, 0, sizeof (coverage));
+      coverage.name = fn->name;
+      add_line_counts (flag_function_summary ? &coverage : NULL, fn);
+      if (flag_function_summary)
+	{
+	  function_summary (&coverage, "Function");
+	  fnotice (stdout, "\n");
+	}
+    }
+  
+  for (src = sources; src; src = src->next)
+    {
+      accumulate_line_counts (src);
+      function_summary (&src->coverage, "File");
+      if (flag_gcov_file)
+	{
+	  char *gcov_file_name = make_gcov_file_name (file_name, src->name);
+	  FILE *gcov_file = fopen (gcov_file_name, "w");
+	  
+	  if (gcov_file)
+	    {
+	      fnotice (stdout, "%s:creating `%s'\n",
+		       src->name, gcov_file_name);
+	      output_lines (gcov_file, src);
+	      if (ferror (gcov_file))
+		    fnotice (stderr, "%s:error writing output file `%s'\n",
+			     src->name, gcov_file_name);
+	      fclose (gcov_file);
+	    }
+	  else
+	    fnotice (stderr, "%s:could not open output file `%s'\n",
+		     src->name, gcov_file_name);
+	  free (gcov_file_name);
+	}
+      fnotice (stdout, "\n");
+    }
 }
 
+/* Release all memory used.  */
 
-/* Find and open the .bb, .da, and .bbg files. If OBJECT_DIRECTORY is
-   not specified, these are looked for in the current directory, and
-   named from the basename of the input_file_name sans extension. If
+static void
+release_structures ()
+{
+  function_t *fn;
+  source_t *src;
+  
+  free (bbg_file_name);
+  free (da_file_name);
+  da_file_name = bbg_file_name = NULL;
+  bbg_file_time = 0;
+  
+  while ((src = sources))
+    {
+      sources = src->next;
+
+      free (src->name);
+      free (src->lines);
+    }
+  
+  while ((fn = functions))
+    {
+      unsigned ix;
+      block_t *block;
+      
+      functions = fn->next;
+      for (ix = fn->num_blocks, block = fn->blocks; ix--; block++)
+	{
+	  arc_t *arc, *arc_n;
+
+	  for (arc = block->succ; arc; arc = arc_n)
+	    {
+	      arc_n = arc->succ_next;
+	      free (arc);
+	    }
+	  free (block->encoding);
+	}
+      free (fn->blocks);
+      free (fn->counts);
+    }
+}
+
+/* Generate the names of the graph and data files. If OBJECT_DIRECTORY
+   is not specified, these are looked for in the current directory,
+   and named from the basename of the FILE_NAME sans extension. If
    OBJECT_DIRECTORY is specified and is a directory, the files are in
-   that directory, but named from the basename of the input_file_name,
-   sans extension. Otherwise OBJECT_DIRECTORY is taken to be the name
-   of the object *file*, and the data files are named from that.  */
+   that directory, but named from the basename of the FILE_NAME, sans
+   extension. Otherwise OBJECT_DIRECTORY is taken to be the name of
+   the object *file*, and the data files are named from that.  */
 
 static void
-open_files ()
+create_file_names (file_name)
+     const char *file_name;
 {
   char *cptr;
   char *name;
-  int length = strlen (input_file_name);
+  int length = strlen (file_name);
   int base;
   
   if (object_directory && object_directory[0])
@@ -472,658 +573,640 @@ open_files ()
   if (base)
     {
       /* Append source file name */
-      cptr = strrchr (input_file_name, '/');
-      cptr = cptr ? cptr + 1 : input_file_name;
-
-      strcat (name, cptr);
+      cptr = strrchr (file_name, '/');
+      strcat (name, cptr ? cptr + 1 : file_name);
     }
+  
   /* Remove the extension.  */
   cptr = strrchr (name, '.');
   if (cptr)
     *cptr = 0;
   
   length = strlen (name);
-  da_file_name = xmalloc (length + 4);
-  bb_file_name = xmalloc (length + 4);
-  bbg_file_name = xmalloc (length + 5);
-
-  strcpy (da_file_name, name);
-  strcpy (bb_file_name, name);
-  strcpy (bbg_file_name, name);
-  strcpy (da_file_name + length, ".da");
-  strcpy (bb_file_name + length, ".bb");
-  strcpy (bbg_file_name + length, ".bbg");
-
-  bb_file = fopen (bb_file_name, "rb");
-  if (bb_file == NULL)
-    {
-      fnotice (stderr, "Could not open basic block file %s.\n", bb_file_name);
-      exit (FATAL_EXIT_CODE);
-    }
-
-  bbg_file = fopen (bbg_file_name, "rb");
-  if (bbg_file == NULL)
-    {
-      fnotice (stderr, "Could not open program flow graph file %s.\n",
-	       bbg_file_name);
-      exit (FATAL_EXIT_CODE);
-    }
   
-  {
-    struct stat status;
+  bbg_file_name = xmalloc (length + strlen (GCOV_GRAPH_SUFFIX) + 1);
+  strcpy (bbg_file_name, name);
+  strcpy (bbg_file_name + length, GCOV_GRAPH_SUFFIX);
 
-    if (!fstat (fileno (bb_file), &status))
-      bb_file_time = status.st_mtime;
-  }
+  da_file_name = xmalloc (length + strlen (GCOV_DATA_SUFFIX) + 1);
+  strcpy (da_file_name, name);
+  strcpy (da_file_name + length, GCOV_DATA_SUFFIX);
   
-  /* If none of the functions in the file were executed, then there won't
-     be a .da file.  Just assume that all counts are zero in this case.  */
-  da_file = fopen (da_file_name, "rb");
-  if (da_file == NULL)
-    {
-      fnotice (stderr, "Could not open data file %s.\n", da_file_name);
-      fnotice (stderr, "Assuming that all execution counts are zero.\n");
-    }
-
-  /* Check for empty .bbg file.  This indicates that there is no executable
-     code in this source file.  */
-  /* Set the EOF condition if at the end of file.  */
-  ungetc (getc (bbg_file), bbg_file);
-  if (feof (bbg_file))
-    {
-      fnotice (stderr, "No executable code associated with file %s.\n",
-	       input_file_name);
-      exit (FATAL_EXIT_CODE);
-    }
+  return;
 }
-
-/* Initialize a new arc.  */
 
-static void
-init_arc (arcptr, source, target, bb_graph)
-     struct adj_list *arcptr;
-     int source, target;
-     struct bb_info *bb_graph;
-{
-  arcptr->target = target;
-  arcptr->source = source;
-
-  arcptr->arc_count = 0;
-  arcptr->count_valid = 0;
-  arcptr->on_tree = 0;
-  arcptr->fake = 0;
-  arcptr->fall_through = 0;
-
-  arcptr->succ_next = bb_graph[source].succ;
-  bb_graph[source].succ = arcptr;
-  bb_graph[source].succ_count++;
-
-  arcptr->pred_next = bb_graph[target].pred;
-  bb_graph[target].pred = arcptr;
-  bb_graph[target].pred_count++;
-}
-
-/* Reverse the arcs on an arc list.  */
+/* Read the graph file. Return non-zero on fatal error.  */
 
-static struct adj_list *
-reverse_arcs (arcptr)
-     struct adj_list *arcptr;
+static int
+read_graph_file ()
 {
-  struct adj_list *prev = 0;
-  struct adj_list *next;
+  FILE *file;
+  struct stat status;
+  unsigned magic, version;
+  unsigned current_tag = 0;
+  unsigned tag;
+  struct function_info *fn = NULL;
+  source_t *src = NULL;
+  unsigned ix;
 
-  for ( ; arcptr; arcptr = next)
+  file = fopen (bbg_file_name, "rb");
+  if (!file)
     {
-      next = arcptr->succ_next;
-      arcptr->succ_next = prev;
-      prev = arcptr;
+      fnotice (stderr, "%s:cannot open graph file\n", bbg_file_name);
+      return 1;
     }
-
-  return prev;
-}
-
-/* Reads profiles from the .da file and compute a hybrid profile.  */
-
-static gcov_type *
-read_profile (function_name, cfg_checksum, instr_arcs)
-     char *function_name;
-     long cfg_checksum;
-     int instr_arcs;
-{
-  int i;
-  int okay = 1;
-  gcov_type *profile;
-  char *function_name_buffer;
-  int function_name_buffer_len;
-
-  profile = xmalloc (sizeof (gcov_type) * instr_arcs);
-  function_name_buffer_len = strlen (function_name) + 1;
-  function_name_buffer = xmalloc (function_name_buffer_len + 1);
-
-  for (i = 0; i < instr_arcs; i++)
-    profile[i] = 0;
-
-  if (!da_file)
-    return profile;
-
-  rewind (da_file);
-  while (1)
+  if (!fstat (fileno (file), &status))
+    bbg_file_time = status.st_mtime;
+  if (gcov_read_unsigned (file, &magic) || magic != GCOV_GRAPH_MAGIC)
     {
-      long magic, extra_bytes;
-      long func_count;
-      int i;
-
-      if (__read_long (&magic, da_file, 4) != 0)
-	break;
+      fnotice (stderr, "%s:not a gcov graph file\n", bbg_file_name);
+      fclose (file);
+      return 1;
+    }
 
-      if (magic != -123)
-	{
-	  okay = 0;
-	  break;
-	}
+  if (gcov_read_unsigned (file, &version) || version != GCOV_VERSION)
+    {
+      char v[4], e[4];
 
-      if (__read_long (&func_count, da_file, 4) != 0)
+      magic = GCOV_VERSION;
+      
+      for (ix = 4; ix--; magic >>= 8, version >>= 8)
 	{
-	  okay = 0;
-	  break;
-	}
-
-      if (__read_long (&extra_bytes, da_file, 4) != 0)
-	{
-	  okay = 0;
-	  break;
+	  v[ix] = version;
+	  e[ix] = magic;
 	}
+      fnotice (stderr, "%s:version `%.4s', prefer `%.4s'\n",
+	       bbg_file_name, v, e);
+    }
+  
+  while (!gcov_read_unsigned (file, &tag))
+    {
+      unsigned length;
+      long base;
 
-      /* skip extra data emited by __bb_exit_func.  */
-      fseek (da_file, extra_bytes, SEEK_CUR);
+      if (gcov_read_unsigned (file, &length))
+	goto corrupt;
 
-      for (i = 0; i < func_count; i++)
+      base = gcov_save_position (file);
+      
+      if (tag == GCOV_TAG_FUNCTION)
 	{
-	  long arc_count;
-	  long chksum;
-	  int j;
-
-	  if (__read_gcov_string
-	      (function_name_buffer, function_name_buffer_len, da_file,
-	       -1) != 0)
-	    {
-	      okay = 0;
-	      break;
-	    }
+	  char *function_name = NULL;
+	  unsigned checksum;
 
-	  if (__read_long (&chksum, da_file, 4) != 0)
+	  if (gcov_read_string (file, &function_name, NULL)
+	      || gcov_read_unsigned (file, &checksum))
+	    goto corrupt;
+	  fn = (function_t *)xcalloc (1, sizeof (function_t));
+	  fn->name = function_name;
+	  fn->checksum = checksum;
+
+	  fn->next = functions;
+	  functions = fn;
+	  current_tag = tag;
+	}
+      else if (fn && tag == GCOV_TAG_BLOCKS)
+	{
+	  if (fn->blocks)
+	    fnotice (stderr, "%s:already seen blocks for `%s'\n",
+		     bbg_file_name, fn->name);
+	  else
 	    {
-	      okay = 0;
-	      break;
+	      fn->num_blocks = length / 4;
+	      fn->blocks
+		= (block_t *)xcalloc (fn->num_blocks, sizeof (block_t));
 	    }
-
-	  if (__read_long (&arc_count, da_file, 4) != 0)
+	}
+      else if (fn && tag == GCOV_TAG_ARCS)
+	{
+	  unsigned src;
+	  unsigned num_dests = (length - 4) / 8;
+	  unsigned dest, flags;
+
+	  if (gcov_read_unsigned (file, &src)
+	      || src >= fn->num_blocks
+	      || fn->blocks[src].succ)
+	    goto corrupt;
+	  
+	  while (num_dests--)
 	    {
-	      okay = 0;
-	      break;
+	      struct arc_info *arc;
+	      
+	      if (gcov_read_unsigned (file, &dest)
+		  || gcov_read_unsigned (file, &flags)
+		  || dest >= fn->num_blocks)
+		goto corrupt;
+	      arc = (arc_t *) xcalloc (1, sizeof (arc_t));
+	      
+	      arc->dst = &fn->blocks[dest];
+	      arc->src = &fn->blocks[src];
+	      
+	      arc->count = 0;
+	      arc->count_valid = 0;
+	      arc->on_tree = !!(flags & GCOV_ARC_ON_TREE);
+	      arc->fake = !!(flags & GCOV_ARC_FAKE);
+	      arc->fall_through = !!(flags & GCOV_ARC_FALLTHROUGH);
+	      
+	      arc->succ_next = fn->blocks[src].succ;
+	      fn->blocks[src].succ = arc;
+	      fn->blocks[src].num_succ++;
+	      
+	      arc->pred_next = fn->blocks[dest].pred;
+	      fn->blocks[dest].pred = arc;
+	      fn->blocks[dest].num_pred++;
+
+	      arc->is_call = arc->fake;
+	      
+	      if (!arc->on_tree)
+		fn->num_counts++;
 	    }
-
-	  if (strcmp (function_name_buffer, function_name) != 0
-	      || arc_count != instr_arcs || chksum != cfg_checksum)
+	}
+      else if (fn && tag == GCOV_TAG_LINES)
+	{
+	  unsigned blockno;
+	  unsigned *line_nos
+	    = (unsigned *)xcalloc ((length - 4) / 4, sizeof (unsigned));
+
+	  if (gcov_read_unsigned (file, &blockno)
+	      || blockno >= fn->num_blocks
+	      || fn->blocks[blockno].encoding)
+	    goto corrupt;
+	  
+	  for (ix = 0; ;  )
 	    {
-	      /* skip */
-	      if (fseek (da_file, arc_count * 8, SEEK_CUR) < 0)
+	      unsigned lineno;
+	      
+	      if (gcov_read_unsigned (file, &lineno))
+		goto corrupt;
+	      if (lineno)
 		{
-		  okay = 0;
-		  break;
+		  if (!ix)
+		    {
+		      line_nos[ix++] = 0;
+		      line_nos[ix++] = src->index;
+		    }
+		  line_nos[ix++] = lineno;
+		  if (lineno >= src->num_lines)
+		    src->num_lines = lineno + 1;
 		}
-	    }
-	  else
-	    {
-	      gcov_type tmp;
-
-	      for (j = 0; j < arc_count; j++)
-		if (__read_gcov_type (&tmp, da_file, 8) != 0)
-		  {
-		    okay = 0;
+	      else
+		{
+		  char *file_name = NULL;
+		  
+		  if (gcov_read_string (file, &file_name, NULL))
+		    goto corrupt;
+		  if (!file_name)
 		    break;
-		  }
-		else
-		  {
-		    profile[j] += tmp;
-		  }
+		  for (src = sources; src; src = src->next)
+		    if (!strcmp (file_name, src->name))
+		      {
+			free (file_name);
+			break;
+		      }
+		  if (!src)
+		    {
+		      src = (source_t *)xcalloc (1, sizeof (source_t));
+		      src->name = file_name;
+		      src->coverage.name = file_name;
+		      src->index = sources ? sources->index + 1 : 1;
+		      src->next = sources;
+		      sources = src;
+		    }
+		  line_nos[ix++] = 0;
+		  line_nos[ix++] = src->index;
+		}
 	    }
+	  
+	  fn->blocks[blockno].encoding = line_nos;
+	  fn->blocks[blockno].num_encodings = ix;
+	}
+      else if (current_tag && !GCOV_TAG_IS_SUBTAG (current_tag, tag))
+	{
+	  fn = NULL;
+	  current_tag = 0;
+	}
+      if (gcov_resync (file, base, length))
+	{
+	corrupt:;
+	  fnotice (stderr, "%s:corrupted\n", bbg_file_name);
+	  fclose (file);
+	  return 1;
 	}
-
-      if (!okay)
-	break;
-
     }
+  fclose (file);
+  
+  /* We built everything backwards, so nreverse them all */
+  
+  /* Reverse sources. Not strictly necessary, but we'll then process
+     them in the 'expected' order.  */
+  {
+    source_t *src, *src_p, *src_n;
 
-  free (function_name_buffer);
+    for (src_p = NULL, src = sources; src; src_p = src, src = src_n)
+      {
+	src_n = src->next;
+	src->next = src_p;
+      }
+    sources =  src_p;
+  }
 
-  if (!okay)
-    {
-      fprintf (stderr, ".da file corrupted!\n");
-      free (profile);
-      abort ();
-    }
+  /* Reverse functions.  */
+  {
+    function_t *fn, *fn_p, *fn_n;
+
+    for (fn_p = NULL, fn = functions; fn; fn_p = fn, fn = fn_n)
+      {
+	unsigned ix;
+	
+	fn_n = fn->next;
+	fn->next = fn_p;
 
-  return profile;
+	/* Reverse the arcs */
+	for (ix = fn->num_blocks; ix--;)
+	  {
+	    arc_t *arc, *arc_p, *arc_n;
+	    
+	    for (arc_p = NULL, arc = fn->blocks[ix].succ; arc;
+		 arc_p = arc, arc = arc_n)
+	      {
+		arc_n = arc->succ_next;
+		arc->succ_next = arc_p;
+	      }
+	    fn->blocks[ix].succ = arc_p;
+
+	    for (arc_p = NULL, arc = fn->blocks[ix].pred; arc;
+		 arc_p = arc, arc = arc_n)
+	      {
+		arc_n = arc->pred_next;
+		arc->pred_next = arc_p;
+	      }
+	    fn->blocks[ix].pred = arc_p;
+	  }
+      }
+    functions = fn_p;
+  }
+  return 0;
 }
 
-/* Construct the program flow graph from the .bbg file, and read in the data
-   in the .da file.  */
+/* Reads profiles from the count file and attach to each
+   function. Return non-zero if fatal error.  */
 
-static void
-create_program_flow_graph (bptr)
-     struct bb_info_list *bptr;
+static int
+read_count_file ()
 {
-  long num_blocks, number_arcs, src, dest, flag_bits, num_arcs_per_block;
-  int i;
-  struct adj_list *arcptr;
-  struct bb_info *bb_graph;
-  long cfg_checksum;
-  long instr_arcs = 0;
-  gcov_type *profile;
-  int profile_pos = 0;
-  char *function_name;
-  long function_name_len, tmp;
-
-  /* Read function name.  */
-  __read_long (&tmp, bbg_file, 4);   /* ignore -1.  */
-  __read_long (&function_name_len, bbg_file, 4);
-  function_name = xmalloc (function_name_len + 1);
-  fread (function_name, 1, function_name_len + 1, bbg_file);
+  FILE *file;
+  unsigned ix;
+  char *function_name_buffer = NULL;
+  unsigned magic, version;
+  function_t *fn = NULL;
 
-  /* Skip padding.  */
-  tmp = (function_name_len + 1) % 4;
-
-  if (tmp)
-    fseek (bbg_file, 4 - tmp, SEEK_CUR);
-
-  __read_long (&tmp, bbg_file, 4);   /* ignore -1.  */
-
-  /* Read the cfg checksum.  */
-  __read_long (&cfg_checksum, bbg_file, 4);
-
-  /* Read the number of blocks.  */
-  __read_long (&num_blocks, bbg_file, 4);
-
-  /* Create an array of size bb number of bb_info structs.  */
-  bb_graph = (struct bb_info *) xcalloc (num_blocks, sizeof (struct bb_info));
-
-  bptr->bb_graph = bb_graph;
-  bptr->num_blocks = num_blocks;
-
-  /* Read and create each arc from the .bbg file.  */
-  __read_long (&number_arcs, bbg_file, 4);
-  for (i = 0; i < num_blocks; i++)
+  file = fopen (da_file_name, "rb");
+  if (!file)
     {
-      int j;
-
-      __read_long (&num_arcs_per_block, bbg_file, 4);
-      for (j = 0; j < num_arcs_per_block; j++)
+      fnotice (stderr, "%s:cannot open data file\n", da_file_name);
+      return 1;
+    }
+  if (gcov_read_unsigned (file, &magic) || magic != GCOV_DATA_MAGIC)
+    {
+      fnotice (stderr, "%s:not a gcov data file\n", da_file_name);
+    cleanup:;
+      free (function_name_buffer);
+      fclose (file);
+      return 1;
+    }
+  if (gcov_read_unsigned (file, &version) || version != GCOV_VERSION)
+    {
+      char v[4], e[4];
+      
+      magic = GCOV_VERSION;
+      for (ix = 4; ix--; magic >>= 8, version >>= 8)
 	{
-	  if (number_arcs-- < 0)
-	    abort ();
-
-	  src = i;
-	  __read_long (&dest, bbg_file, 4);
-
-	  arcptr = (struct adj_list *) xmalloc (sizeof (struct adj_list));
-	  init_arc (arcptr, src, dest, bb_graph);
-
-	  __read_long (&flag_bits, bbg_file, 4);
-	  if (flag_bits & 0x1)
-	    arcptr->on_tree++;
-	  else
-	    instr_arcs++;
-	  arcptr->fake = !! (flag_bits & 0x2);
-	  arcptr->fall_through = !! (flag_bits & 0x4);
+	  v[ix] = version;
+	  e[ix] = magic;
 	}
+      fnotice (stderr, "%s:version `%.4s', prefer version `%.4s'\n",
+	       da_file_name, v, e);
     }
-
-  if (number_arcs)
-    abort ();
-
-  /* Read and ignore the -1 separating the arc list from the arc list of the
-     next function.  */
-  __read_long (&src, bbg_file, 4);
-  if (src != -1)
-    abort ();
-
-  /* Must reverse the order of all succ arcs, to ensure that they match
-     the order of the data in the .da file.  */
-
-  for (i = 0; i < num_blocks; i++)
-    if (bb_graph[i].succ)
-      bb_graph[i].succ = reverse_arcs (bb_graph[i].succ);
-
-  /* Read profile from the .da file.  */
-
-  profile = read_profile (function_name, cfg_checksum, instr_arcs);
-
-  /* For each arc not on the spanning tree, set its execution count from
-     the .da file.  */
-
-  /* The first count in the .da file is the number of times that the function
-     was entered.  This is the exec_count for block zero.  */
-
-  /* This duplicates code in branch_prob in profile.c.  */
-
-  for (i = 0; i < num_blocks; i++)
-    for (arcptr = bb_graph[i].succ; arcptr; arcptr = arcptr->succ_next)
-      if (! arcptr->on_tree)
+  
+  while (1)
+    {
+      unsigned tag, length;
+      long base;
+      
+      if (gcov_read_unsigned (file, &tag)
+	  || gcov_read_unsigned (file, &length))
 	{
-	  arcptr->arc_count = profile[profile_pos++];
-	  arcptr->count_valid = 1;
-	  bb_graph[i].succ_count--;
-	  bb_graph[arcptr->target].pred_count--;
+	  if (feof (file))
+	    break;
+	  
+	corrupt:;
+	  fnotice (stderr, "%s:corrupted\n", da_file_name);
+	  goto cleanup;
 	}
-  free (profile);
-  free (function_name);
-}
-
-static void
-solve_program_flow_graph (bptr)
-     struct bb_info_list *bptr;
-{
-  int passes, changes;
-  gcov_type total;
-  int i;
-  struct adj_list *arcptr;
-  struct bb_info *bb_graph;
-  int num_blocks;
-
-  num_blocks = bptr->num_blocks;
-  bb_graph = bptr->bb_graph;
-
-  /* For every block in the file,
-     - if every exit/entrance arc has a known count, then set the block count
-     - if the block count is known, and every exit/entrance arc but one has
-       a known execution count, then set the count of the remaining arc
-
-     As arc counts are set, decrement the succ/pred count, but don't delete
-     the arc, that way we can easily tell when all arcs are known, or only
-     one arc is unknown.  */
-
-  /* The order that the basic blocks are iterated through is important.
-     Since the code that finds spanning trees starts with block 0, low numbered
-     arcs are put on the spanning tree in preference to high numbered arcs.
-     Hence, most instrumented arcs are at the end.  Graph solving works much
-     faster if we propagate numbers from the end to the start.
-
-     This takes an average of slightly more than 3 passes.  */
-
-  changes = 1;
-  passes = 0;
-  while (changes)
-    {
-      passes++;
-      changes = 0;
-
-      for (i = num_blocks - 1; i >= 0; i--)
+      base = gcov_save_position (file);
+      if (tag == GCOV_TAG_FUNCTION)
 	{
-	  if (! bb_graph[i].count_valid)
+	  unsigned checksum;
+	  struct function_info *fn_n = functions;
+	  
+	  if (gcov_read_string (file, &function_name_buffer, NULL)
+	      || gcov_read_unsigned (file, &checksum))
+	    goto corrupt;
+
+	  for (fn = fn ? fn->next : NULL; ; fn = fn->next)
 	    {
-	      if (bb_graph[i].succ_count == 0)
-		{
-		  total = 0;
-		  for (arcptr = bb_graph[i].succ; arcptr;
-		       arcptr = arcptr->succ_next)
-		    total += arcptr->arc_count;
-		  bb_graph[i].exec_count = total;
-		  bb_graph[i].count_valid = 1;
-		  changes = 1;
-		}
-	      else if (bb_graph[i].pred_count == 0)
+	      if (fn)
+		;
+	      else if ((fn = fn_n))
+		fn_n = NULL;
+	      else
 		{
-		  total = 0;
-		  for (arcptr = bb_graph[i].pred; arcptr;
-		       arcptr = arcptr->pred_next)
-		    total += arcptr->arc_count;
-		  bb_graph[i].exec_count = total;
-		  bb_graph[i].count_valid = 1;
-		  changes = 1;
+		  fnotice (stderr, "%s:unknown function `%s'\n",
+			   da_file_name, function_name_buffer);
+		  break;
 		}
+	      if (!strcmp (fn->name, function_name_buffer))
+		break;
 	    }
-	  if (bb_graph[i].count_valid)
-	    {
-	      if (bb_graph[i].succ_count == 1)
-		{
-		  total = 0;
-		  /* One of the counts will be invalid, but it is zero,
-		     so adding it in also doesn't hurt.  */
-		  for (arcptr = bb_graph[i].succ; arcptr;
-		       arcptr = arcptr->succ_next)
-		    total += arcptr->arc_count;
-		  /* Calculate count for remaining arc by conservation.  */
-		  total = bb_graph[i].exec_count - total;
-		  /* Search for the invalid arc, and set its count.  */
-		  for (arcptr = bb_graph[i].succ; arcptr;
-		       arcptr = arcptr->succ_next)
-		    if (! arcptr->count_valid)
-		      break;
-		  if (! arcptr)
-		    abort ();
-		  arcptr->count_valid = 1;
-		  arcptr->arc_count = total;
-		  bb_graph[i].succ_count--;
-
-		  bb_graph[arcptr->target].pred_count--;
-		  changes = 1;
-		}
-	      if (bb_graph[i].pred_count == 1)
-		{
-		  total = 0;
-		  /* One of the counts will be invalid, but it is zero,
-		     so adding it in also doesn't hurt.  */
-		  for (arcptr = bb_graph[i].pred; arcptr;
-		       arcptr = arcptr->pred_next)
-		    total += arcptr->arc_count;
-		  /* Calculate count for remaining arc by conservation.  */
-		  total = bb_graph[i].exec_count - total;
-		  /* Search for the invalid arc, and set its count.  */
-		  for (arcptr = bb_graph[i].pred; arcptr;
-		       arcptr = arcptr->pred_next)
-		    if (! arcptr->count_valid)
-		      break;
-		  if (! arcptr)
-		    abort ();
-		  arcptr->count_valid = 1;
-		  arcptr->arc_count = total;
-		  bb_graph[i].pred_count--;
 
-		  bb_graph[arcptr->source].succ_count--;
-		  changes = 1;
-		}
+	  if (!fn)
+	    ;
+	  else if (checksum != fn->checksum)
+	    {
+	    mismatch:;
+	      fnotice (stderr, "%s:profile mismatch for `%s'\n",
+		       da_file_name, function_name_buffer);
+	      goto cleanup;
+	    }
+	}
+      else if (tag == GCOV_TAG_ARC_COUNTS && fn)
+	{
+	  if (length != 8 * fn->num_counts)
+	    goto mismatch;
+	  
+	  if (!fn->counts)
+	    fn->counts
+	      = (gcov_type *)xcalloc (fn->num_counts, sizeof (gcov_type));
+	  
+	  for (ix = 0; ix != fn->num_counts; ix++)
+	    {
+	      gcov_type count;
+	      
+	      if (gcov_read_counter (file, &count, false))
+		goto corrupt;
+	      fn->counts[ix] += count;
 	    }
 	}
+      gcov_resync (file, base, length);
     }
 
-  /* If the graph has been correctly solved, every block will have a
-     succ and pred count of zero.  */
-  for (i = 0; i < num_blocks; i++)
-    if (bb_graph[i].succ_count || bb_graph[i].pred_count)
-      abort ();
+  fclose (file);
+  free (function_name_buffer);
+  return 0;
 }
 
+/* Solve the flow graph. Propagate counts from the instrumented arcs
+   to the blocks and the uninstrumented arcs.  */
 
 static void
-read_files ()
+solve_flow_graph (fn)
+     function_t *fn;
 {
-  struct stat buf;
-  struct bb_info_list *list_end = 0;
-  struct bb_info_list *b_ptr;
-
-  while (! feof (bbg_file))
+  unsigned ix;
+  arc_t *arc;
+  gcov_type *count_ptr = fn->counts;
+  block_t *valid_blocks = NULL;    /* valid, but unpropagated blocks. */
+  block_t *invalid_blocks = NULL;  /* invalid, but inferable blocks. */
+  
+  if (fn->num_blocks < 2)
+    fnotice (stderr, "%s:`%s' lacks entry and/or exit blocks\n",
+	     bbg_file_name, fn->name);
+  else
     {
-      b_ptr = (struct bb_info_list *) xmalloc (sizeof (struct bb_info_list));
-
-      b_ptr->next = 0;
-      if (list_end)
-	list_end->next = b_ptr;
+      if (fn->blocks[0].num_pred)
+	fnotice (stderr, "%s:`%s' has arcs to entry block\n",
+		 bbg_file_name, fn->name);
       else
-	bb_graph_list = b_ptr;
-      list_end = b_ptr;
-
-      /* Read in the data in the .bbg file and reconstruct the program flow
-	 graph for one function.  */
-      create_program_flow_graph (b_ptr);
-
-      /* Set the EOF condition if at the end of file.  */
-      ungetc (getc (bbg_file), bbg_file);
+	/* We can't deduce the entry block counts from the lack of
+	   predecessors.  */
+	fn->blocks[0].num_pred = ~(unsigned)0;
+      
+      if (fn->blocks[fn->num_blocks - 1].num_succ)
+	fnotice (stderr, "%s:`%s' has arcs from exit block\n",
+		 bbg_file_name, fn->name);
+      else
+	/* Likewise, we can't deduce exit block counts from the lack
+	   of its successors.  */
+	fn->blocks[fn->num_blocks - 1].num_succ = ~(unsigned)0;
     }
 
-  /* Calculate all of the basic block execution counts and branch
-     taken probabilities.  */
-
-  for (b_ptr = bb_graph_list; b_ptr; b_ptr = b_ptr->next)
-    solve_program_flow_graph (b_ptr);
-
-  /* Read in all of the data from the .bb file.   This info will be accessed
-     sequentially twice.  */
-  stat (bb_file_name, &buf);
-  bb_data_size = buf.st_size / 4;
-
-  bb_data = (char *) xmalloc ((unsigned) buf.st_size);
-  fread (bb_data, sizeof (char), buf.st_size, bb_file);
-
-  fclose (bb_file);
-  if (da_file)
-    fclose (da_file);
-  fclose (bbg_file);
-}
-
-
-/* Scan the data in the .bb file to find all source files referenced,
-   and the largest line number mentioned in each one.  */
-
-static void
-scan_for_source_files ()
-{
-  struct sourcefile *s_ptr = NULL;
-  char *ptr;
-  long count;
-  long line_num;
-
-  /* Search the bb_data to find:
-     1) The number of sources files contained herein, and
-     2) The largest line number for each source file.  */
-
-  ptr = bb_data;
-  sources = 0;
-  for (count = 0; count < bb_data_size; count++)
-    {
-      __fetch_long (&line_num, ptr, 4);
-      ptr += 4;
-      if (line_num == -1)
-	{
-	  /* A source file name follows.  Check to see if we already have
-	   a sourcefile structure for this file.  */
-	  s_ptr = sources;
-	  while (s_ptr && strcmp (s_ptr->name, ptr))
-	    s_ptr = s_ptr->next;
-
-	  if (s_ptr == 0)
+  /* Propagate the measured counts, this must be done in the same
+     order as the code in profile.c  */
+  for (ix = 0; ix != fn->num_blocks; ix++)
+    {
+      block_t const *prev_dst = NULL;
+      int out_of_order = 0;
+      
+      for (arc = fn->blocks[ix].succ; arc; arc = arc->succ_next)
+	{
+	  if (!arc->on_tree)
 	    {
-	      /* No sourcefile structure for this file name exists, create
-		 a new one, and append it to the front of the sources list.  */
-	      s_ptr = (struct sourcefile *) xmalloc (sizeof(struct sourcefile));
-	      s_ptr->name = xstrdup (ptr);
-	      s_ptr->maxlineno = 0;
-	      s_ptr->next = sources;
-	      sources = s_ptr;
+	      if (count_ptr)
+		arc->count = *count_ptr++;
+	      arc->count_valid = 1;
+	      fn->blocks[ix].num_succ--;
+	      arc->dst->num_pred--;
 	    }
-
-	  /* Scan past the file name.  */
-	  {
-	    long delim;
-	    do {
-	      count++;
-	      __fetch_long (&delim, ptr, 4);
-	      ptr += 4;
-	    } while (delim != line_num);
-	  }
-	}
-      else if (line_num == -2)
-	{
-	  long delim;
-
-	  /* A function name follows.  Ignore it.  */
-	  do {
-	    count++;
-	    __fetch_long (&delim, ptr, 4);
-	    ptr += 4;
-	  } while (delim != line_num);
+	  if (prev_dst && prev_dst > arc->dst)
+	    out_of_order = 1;
+	  prev_dst = arc->dst;
 	}
-      /* There will be a zero before the first file name, in which case s_ptr
-	 will still be uninitialized.  So, only try to set the maxlineno
-	 field if line_num is nonzero.  */
-      else if (line_num > 0)
+      
+      /* Sort the successor arcs into ascending dst order. profile.c
+	 normally produces arcs in the right order, but sometimes with
+	 one or two out of order.  We're not using a particularly
+	 smart sort. */
+      if (out_of_order)
 	{
-	  if (s_ptr->maxlineno <= line_num)
-	    s_ptr->maxlineno = line_num + 1;
+	  arc_t *start = fn->blocks[ix].succ;
+	  unsigned changes = 1;
+	  
+	  while (changes)
+	    {
+	      arc_t *arc, *arc_p, *arc_n;
+	      
+	      changes = 0;
+	      for (arc_p = NULL, arc = start; (arc_n = arc->succ_next);)
+		{
+		  if (arc->dst > arc_n->dst)
+		    {
+		      changes = 1;
+		      if (arc_p)
+			arc_p->succ_next = arc_n;
+		      else
+			start = arc_n;
+		      arc->succ_next = arc_n->succ_next;
+		      arc_n->succ_next = arc;
+		      arc_p = arc_n;
+		    }
+		  else
+		    {
+		      arc_p = arc;
+		      arc = arc_n;
+		    }
+		}
+	    }
+	  fn->blocks[ix].succ = start;
 	}
-      else if (line_num < 0)
+      
+      /* Place it on the invalid chain, it will be ignored if that's
+	 wrong.  */
+      fn->blocks[ix].invalid_chain = 1;
+      fn->blocks[ix].chain = invalid_blocks;
+      invalid_blocks = &fn->blocks[ix];
+    }
+
+  while (invalid_blocks || valid_blocks)
+    {
+      block_t *blk;
+      
+      while ((blk = invalid_blocks))
 	{
-	  /* Don't know what this is, but it's garbage.  */
-	  abort ();
+	  gcov_type total = 0;
+	  const arc_t *arc;
+	  
+	  invalid_blocks = blk->chain;
+	  blk->invalid_chain = 0;
+	  if (!blk->num_succ)
+	    for (arc = blk->succ; arc; arc = arc->succ_next)
+	      total += arc->count;
+	  else if (!blk->num_pred)
+	    for (arc = blk->pred; arc; arc = arc->pred_next)
+	      total += arc->count;
+	  else
+	    continue;
+	  
+	  blk->count = total;
+	  blk->count_valid = 1;
+	  blk->chain = valid_blocks;
+	  blk->valid_chain = 1;
+	  valid_blocks = blk;
+	}
+      while ((blk = valid_blocks))
+	{
+	  gcov_type total;
+	  arc_t *arc, *inv_arc;
+
+	  valid_blocks = blk->chain;
+	  blk->valid_chain = 0;
+	  if (blk->num_succ == 1)
+	    {
+	      block_t *dst;
+	      
+	      total = blk->count;
+	      inv_arc = NULL;
+	      for (arc = blk->succ; arc; arc = arc->succ_next)
+		{
+		  total -= arc->count;
+		  if (!arc->count_valid)
+		    inv_arc = arc;
+		}
+	      dst = inv_arc->dst;
+	      inv_arc->count_valid = 1;
+	      inv_arc->count = total;
+	      blk->num_succ--;
+	      dst->num_pred--;
+	      if (dst->count_valid)
+		{
+		  if (dst->num_pred == 1 && !dst->valid_chain)
+		    {
+		      dst->chain = valid_blocks;
+		      dst->valid_chain = 1;
+		      valid_blocks = dst;
+		    }
+		}
+	      else
+		{
+		  if (!dst->num_pred && !dst->invalid_chain)
+		    {
+		      dst->chain = invalid_blocks;
+		      dst->invalid_chain = 1;
+		      invalid_blocks = dst;
+		    }
+		}
+	    }
+	  if (blk->num_pred == 1)
+	    {
+	      block_t *src;
+	      
+	      total = blk->count;
+	      inv_arc = NULL;
+	      for (arc = blk->pred; arc; arc = arc->pred_next)
+		{
+		  total -= arc->count;
+		  if (!arc->count_valid)
+		    inv_arc = arc;
+		}
+	      src = inv_arc->src;
+	      inv_arc->count_valid = 1;
+	      inv_arc->count = total;
+	      blk->num_pred--;
+	      src->num_succ--;
+	      if (src->count_valid)
+		{
+		  if (src->num_succ == 1 && !src->valid_chain)
+		    {
+		      src->chain = valid_blocks;
+		      src->valid_chain = 1;
+		      valid_blocks = src;
+		    }
+		}
+	      else
+		{
+		  if (!src->num_succ && !src->invalid_chain)
+		    {
+		      src->chain = invalid_blocks;
+		      src->invalid_chain = 1;
+		      invalid_blocks = src;
+		    }
+		}
+	    }
 	}
     }
+  
+  /* If the graph has been correctly solved, every block will have a
+     valid count.  */
+  for (ix = 0; ix < fn->num_blocks; ix++)
+    if (!fn->blocks[ix].count_valid)
+      {
+	fnotice (stderr, "%s:graph is unsolvable for `%s'\n",
+		 bbg_file_name, fn->name);
+	break;
+      }
 }
+
 
 
-/* Increment totals in FUNCTION according to arc A_PTR.  */
+/* Increment totals in COVERAGE according to arc ARC.  */
 
 static void
-accumulate_branch_counts (function, a_ptr)
-     struct coverage *function;
-     struct arcdata *a_ptr;
-{
-  if (a_ptr->call_insn)
-    {
-      function->calls++;
-      if (a_ptr->total)
-	function->calls_executed++;
+add_branch_counts (coverage, arc)
+     coverage_t *coverage;
+     const arc_t *arc;
+{
+  if (arc->is_call)
+    {
+      coverage->calls++;
+      if (arc->src->count)
+	coverage->calls_executed++;
     }
   else
     {
-      function->branches++;
-      if (a_ptr->total)
-	function->branches_executed++;
-      if (a_ptr->hits)
-	function->branches_taken++;
-    }
-}
-
-/* Calculate the branch taken probabilities for all arcs branches at the
-   end of this block.  */
-
-static void
-calculate_branch_probs (block_ptr, line_info, function)
-     struct bb_info *block_ptr;
-     struct line_info *line_info;
-     struct coverage *function;
-{
-  gcov_type total;
-  struct adj_list *arcptr;
-
-  total = block_ptr->exec_count;
-  for (arcptr = block_ptr->succ; arcptr; arcptr = arcptr->succ_next)
-    {
-      struct arcdata *a_ptr;
-      
-      /* Ignore fall through arcs as they aren't really branches.  */
-      if (arcptr->fall_through)
-	continue;
-
-      a_ptr = (struct arcdata *) xmalloc (sizeof (struct arcdata));
-      a_ptr->total = total;
-      a_ptr->hits = total ? arcptr->arc_count : 0;
-      a_ptr->call_insn = arcptr->fake;
-
-      if (function)
-	accumulate_branch_counts (function, a_ptr);
-      /* Prepend the new branch to the list.  */
-      a_ptr->next = line_info->branches;
-      line_info->branches = a_ptr;
+      coverage->branches++;
+      if (arc->src->count)
+	coverage->branches_executed++;
+      if (arc->count)
+	coverage->branches_taken++;
     }
 }
 
@@ -1134,8 +1217,8 @@ calculate_branch_probs (block_ptr, line_
    format TOP.  Return pointer to a static string.  */
 
 static char const *
-format_hwint (top, bottom, dp)
-     HOST_WIDEST_INT top, bottom;
+format_gcov (top, bottom, dp)
+     gcov_type top, bottom;
      int dp;
 {
   static char buffer[20];
@@ -1169,7 +1252,7 @@ format_hwint (top, bottom, dp)
 	}
     }
   else
-    sprintf (buffer, HOST_WIDEST_INT_PRINT_DEC, top);
+    sprintf (buffer, HOST_WIDEST_INT_PRINT_DEC, (HOST_WIDEST_INT)top);
   
   return buffer;
 }
@@ -1178,42 +1261,40 @@ format_hwint (top, bottom, dp)
 /* Output summary info for a function.  */
 
 static void
-function_summary (function, title)
-     struct coverage *function;
+function_summary (coverage, title)
+     const coverage_t *coverage;
      const char *title;
 {
-  if (function->lines)
-    fnotice (stdout, "%s of %d lines executed in %s %s\n",
-	     format_hwint (function->lines_executed,
-			   function->lines, 2),
-	     function->lines, title, function->name);
+  fnotice (stdout, "%s `%s'\n", title, coverage->name);
+
+  if (coverage->lines)
+    fnotice (stdout, "Lines executed:%s of %d\n",
+	     format_gcov (coverage->lines_executed, coverage->lines, 2),
+	     coverage->lines);
   else
-    fnotice (stdout, "No executable lines in %s %s\n",
-	     title, function->name);
+    fnotice (stdout, "No executable lines");
 
-  if (output_branch_probs)
+  if (flag_branches)
     {
-      if (function->branches)
+      if (coverage->branches)
 	{
-	  fnotice (stdout, "%s of %d branches executed in %s %s\n",
-		   format_hwint (function->branches_executed,
-				 function->branches, 2),
-		   function->branches, title, function->name);
-	  fnotice (stdout,
-		"%s of %d branches taken at least once in %s %s\n",
-		   format_hwint (function->branches_taken,
-				 function->branches, 2),
-		   function->branches, title, function->name);
+	  fnotice (stdout, "Branches executed:%s of %d\n",
+		   format_gcov (coverage->branches_executed,
+				coverage->branches, 2),
+		   coverage->branches);
+	  fnotice (stdout, "Taken at least once:%s of %d\n",
+		   format_gcov (coverage->branches_taken,
+				coverage->branches, 2),
+		   coverage->branches);
 	}
       else
-	fnotice (stdout, "No branches in %s %s\n", title, function->name);
-      if (function->calls)
-	fnotice (stdout, "%s of %d calls executed in %s %s\n",
-		 format_hwint (function->calls_executed,
-			       function->calls, 2),
-		 function->calls, title, function->name);
+	fnotice (stdout, "No branches\n");
+      if (coverage->calls)
+	fnotice (stdout, "Calls executed:%s of %d\n",
+		 format_gcov (coverage->calls_executed, coverage->calls, 2),
+		 coverage->calls);
       else
-	fnotice (stdout, "No calls in %s %s\n", title, function->name);
+	fnotice (stdout, "No calls\n");
     }
 }
 
@@ -1228,28 +1309,27 @@ function_summary (function, title)
    removed and '..'  components are renamed to '^'.  */
 
 static char *
-make_gcov_file_name (src_name)
-     char *src_name;
+make_gcov_file_name (input_name, src_name)
+     const char *input_name;
+     const char *src_name;
 {
   char *cptr;
-  char *name = xmalloc (strlen (src_name) + strlen (input_file_name) + 10);
+  char *name = xmalloc (strlen (src_name) + strlen (input_name) + 10);
   
   name[0] = 0;
-  if (output_long_names && strcmp (src_name, input_file_name))
+  if (flag_long_names && strcmp (src_name, input_name))
     {
       /* Generate the input filename part.  */
-      cptr = preserve_paths ? NULL : strrchr (input_file_name, '/');
-      cptr = cptr ? cptr + 1 : input_file_name;
-      strcat (name, cptr);
+      cptr = flag_preserve_paths ? NULL : strrchr (input_name, '/');
+      strcat (name, cptr ? cptr + 1 : input_name);
       strcat (name, "##");
     }
    
   /* Generate the source filename part.  */
-  cptr = preserve_paths ? NULL : strrchr (src_name, '/');
-  cptr = cptr ? cptr + 1 : src_name;
-  strcat (name, cptr);
+  cptr = flag_preserve_paths ? NULL : strrchr (src_name, '/');
+  strcat (name, cptr ? cptr + 1 : src_name);
   
-  if (preserve_paths)
+  if (flag_preserve_paths)
     {
       /* Convert '/' to '#', remove '/./', convert '/../' to '/^/' */
       char *prev;
@@ -1281,166 +1361,106 @@ make_gcov_file_name (src_name)
  	}
     }
   
-  /* Don't strip off the ending for compatibility with tcov, since
-     this results in confusion if there is more than one file with the
-     same basename, e.g. tmp.c and tmp.h.  */
   strcat (name, ".gcov");
   return name;
 }
 
-/* Scan through the bb_data, and when the file name matches the
-   source file name, then for each following line number, increment
+/* Scan through the bb_data for each line in the block, increment
    the line number execution count indicated by the execution count of
    the appropriate basic block.  */
 
 static void
-init_line_info (line_info, total, maxlineno)
-     struct line_info *line_info;
-     struct coverage *total;
-     long maxlineno;
-{
-  long block_num = 0;		/* current block number */
-  struct bb_info *block_ptr = NULL;	/* current block ptr */
-  struct coverage function;
-  struct coverage *func_ptr = NULL;
-  struct bb_info_list *current_graph = NULL; /* Graph for current function.  */
-  int is_this_file = 0;	/* We're scanning a block from the desired file.  */
-  char *ptr = bb_data;
-  long count;
-  long line_num;
-  struct line_info *line_ptr = 0; /* line info ptr.  */
-   
-  memset (&function, 0, sizeof (function));
-  if (output_function_summary)
-    func_ptr = &function;
-  
-  for (count = 0; count < bb_data_size; count++)
-    {
-      __fetch_long (&line_num, ptr, 4);
-      ptr += 4;
-      if (line_num < 0)
-	{
-	  long delim;
-	  
-	  if (line_num == -1)
-	    {
-	      /* Marks the beginning of a file name.  Check to see
-	     	 whether this is the filename we are currently
-	     	 collecting data for.  */
-	      is_this_file = !strcmp (total->name, ptr);
-	    }
-	  else if (line_num == -2)
-	    {
-	      /* Marks the start of a new function.  Advance to the
-	     	 next program flow graph.  */
-	      if (!current_graph)
-		current_graph = bb_graph_list;
-	      else
-		{
-		  if (block_num == current_graph->num_blocks - 1)
-		    /* Last block falls through to exit.  */
-		    ;
-		  else if (block_num == current_graph->num_blocks - 2)
-		    {
-		      if (output_branch_probs && is_this_file)
-			calculate_branch_probs (block_ptr, line_ptr, func_ptr);
-		    }
-		  else
-		    {
-		      fnotice (stderr,
-			       "didn't use all bb entries of graph, function %s\n",
-			       function.name);
-		      fnotice (stderr, "block_num = %ld, num_blocks = %d\n",
-			       block_num, current_graph->num_blocks);
-		    }
-		  if (func_ptr && is_this_file)
-		    function_summary (func_ptr, "function");
-		  current_graph = current_graph->next;
-		}
-	      block_num = 0;
-	      block_ptr = current_graph->bb_graph;
-	      memset (&function, 0, sizeof (function));
-	      function.name = ptr;
-	    }
-	  else
-	    {
-	      fnotice (stderr, "ERROR: unexpected line number %ld\n", line_num);
-	      abort ();
-	    }
+add_line_counts (coverage, fn)
+     coverage_t *coverage;
+     const function_t *fn;
+{
+  unsigned ix;
+  line_t *line = NULL; /* this is propagated from one iteration to the
+			  next.  */
+
+  /* Scan each basic block. */
+  for (ix = 0; ix != fn->num_blocks; ix++)
+    {
+      const block_t *block = &fn->blocks[ix];
+      unsigned *encoding;
+      const source_t *src = NULL;
+      unsigned jx;
+
+      for (jx = 0, encoding = block->encoding;
+	   jx != block->num_encodings; jx++, encoding++)
+	if (!*encoding)
+	  {
+	    unsigned src_n = *++encoding;
 
-	  /* Scan past the string.  */
-	  for (delim = 0; delim != line_num; count++)
-	    {
-	      __fetch_long (&delim, ptr, 4);
-	      ptr += 4;
-	    }
-	}
-      else if (!line_num)
+	    for (src = sources; src->index != src_n; src = src->next)
+	      continue;
+	    jx++;
+	  }
+	else
+	  {
+	    line = &src->lines[*encoding];
+
+	    if (coverage)
+	      {
+		if (!line->exists)
+		  coverage->lines++;
+		if  (!line->count && block->count)
+		  coverage->lines_executed++;
+	      }
+	    line->exists = 1;
+	    line->count += block->count;
+	  }
+      
+      if (line && flag_branches)
 	{
-	  /* Marks the end of a block.  */
-	  if (block_num >= current_graph->num_blocks)
-	    {
-	      fnotice (stderr, "ERROR: too many basic blocks in function %s\n",
-		       function.name);
-	      abort ();
-	    }
-	  
-	  if (output_branch_probs && is_this_file)
-	    calculate_branch_probs (block_ptr, line_ptr, func_ptr);
+	  arc_t *arc;
 	  
-	  block_num++;
-	  block_ptr++;
-	}
-      else if (is_this_file)
-	{
-	  if (line_num >= maxlineno)
+	  for (arc = block->succ; arc; arc = arc->succ_next)
 	    {
-	      fnotice (stderr, "ERROR: out of range line number in function %s\n",
-		       function.name);
-	      abort ();
+	      /* Ignore fall through arcs as they aren't really branches.  */
+	      if (arc->fall_through)
+		continue;
+	      
+	      arc->line_next = line->branches;
+	      line->branches = arc;
+	      if (coverage)
+		add_branch_counts (coverage, arc);
 	    }
-
-	  line_ptr = &line_info[line_num];
-	  if (func_ptr)
-	    {
-	      if (!line_ptr->exists)
-		func_ptr->lines++;
-	      if (!line_ptr->count && block_ptr->exec_count)
-		func_ptr->lines_executed++;
-	    }
-	  
-	  /* Accumulate execution data for this line number.  */
-	  line_ptr->count += block_ptr->exec_count;
-	  line_ptr->exists = 1;
 	}
     }
+  if (!line)
+    fnotice (stderr, "%s:no lines for `%s'\n", bbg_file_name, fn->name);
+}
+
+/* Accumulate the line counts of a file. */
+
+static void
+accumulate_line_counts (src)
+     source_t *src;
+{
+  line_t *line;
+  unsigned ix;
   
-  if (func_ptr && is_this_file)
-    function_summary (func_ptr, "function");
-  
-  /* Calculate summary test coverage statistics.  */
-  for (line_num = 1, line_ptr = &line_info[line_num];
-       line_num < maxlineno; line_num++, line_ptr++)
+  for (ix = src->num_lines, line = src->lines; ix--; line++)
     {
-      struct arcdata *a_ptr, *prev, *next;
+      arc_t *arc, *arc_p, *arc_n;
       
-      if (line_ptr->exists)
+      /* Total and reverse the branch information. */
+      for (arc = line->branches, arc_p = NULL; arc; arc_p = arc, arc = arc_n)
 	{
-	  total->lines++;
-	  if (line_ptr->count)
-	    total->lines_executed++;
+	  arc_n = arc->line_next;
+	  arc->line_next = arc_p;
+
+	  add_branch_counts (&src->coverage, arc);
 	}
+      line->branches = arc_p;
 
-      /* Total and reverse the branch information.  */
-      for (a_ptr = line_ptr->branches, prev = NULL; a_ptr; a_ptr = next)
+      if (line->exists)
 	{
-	  next = a_ptr->next;
-	  a_ptr->next = prev;
-	  prev = a_ptr;
-
-	  accumulate_branch_counts (total, a_ptr);
+	  src->coverage.lines++;
+	  if (line->count)
+	    src->coverage.lines_executed++;
 	}
-      line_ptr->branches = prev;
     }
 }
 
@@ -1449,25 +1469,24 @@ init_line_info (line_info, total, maxlin
    information.  */
 
 static void
-output_line_info (gcov_file, line_info, total, maxlineno)
+output_lines (gcov_file, src)
      FILE *gcov_file;
-     const struct line_info *line_info;
-     const struct coverage *total;
-     long maxlineno;
+     const source_t *src;
 {
   FILE *source_file;
-  long line_num;                    /* current line number */
-  const struct line_info *line_ptr; /* current line info ptr.  */
-  char string[STRING_SIZE];         /* line buffer.  */
-  char const *retval = "";	    /* status of source file reading.  */
-
-  fprintf (gcov_file, "%9s:%5d:Source:%s\n", "-", 0, total->name);
-  fprintf (gcov_file, "%9s:%5d:Object:%s\n", "-", 0, bb_file_name);
+  unsigned line_num;       	/* current line number.  */
+  const line_t *line;           /* current line info ptr.  */
+  char string[STRING_SIZE];     /* line buffer.  */
+  char const *retval = "";	/* status of source file reading.  */
+
+  fprintf (gcov_file, "%9s:%5d:Source:%s\n", "-", 0, src->name);
+  fprintf (gcov_file, "%9s:%5d:Graph:%s\n", "-", 0, bbg_file_name);
+  fprintf (gcov_file, "%9s:%5d:Data:%s\n", "-", 0, da_file_name);
   
-  source_file = fopen (total->name, "r");
+  source_file = fopen (src->name, "r");
   if (!source_file)
     {
-      fnotice (stderr, "Could not open source file %s.\n", total->name);
+      fnotice (stderr, "%s:cannot open source file\n", src->name);
       retval = NULL;
     }
   else
@@ -1475,17 +1494,17 @@ output_line_info (gcov_file, line_info, 
       struct stat status;
       
       if (!fstat (fileno (source_file), &status)
-	  && status.st_mtime > bb_file_time)
+	  && status.st_mtime > bbg_file_time)
 	{
-	  fnotice (stderr, "Warning: source file %s is newer than %s\n",
-		   total->name, bb_file_name);
-	  fprintf (gcov_file, "%9s:%5d:Source is newer than compiler output\n",
+	  fnotice (stderr, "%s:source file is newer than graph file `%s'\n",
+		   src->name, bbg_file_name);
+	  fprintf (gcov_file, "%9s:%5d:Source is newer than graph\n",
 		   "-", 0);
 	}
     }
 
-  for (line_num = 1, line_ptr = &line_info[line_num];
-       line_num < maxlineno; line_num++, line_ptr++)
+  for (line_num = 1, line = &src->lines[line_num];
+       line_num < src->num_lines; line_num++, line++)
     {
       /* For lines which don't exist in the .bb file, print '-' before
  	 the source line.  For lines which exist but were never
@@ -1493,10 +1512,9 @@ output_line_info (gcov_file, line_info, 
  	 print the execution count before the source line.  There are
  	 16 spaces of indentation added before the source line so that
  	 tabs won't be messed up.  */
-      fprintf (gcov_file, "%9s:%5ld:",
-	       !line_ptr->exists ? "-"
-	       : !line_ptr->count ? "#####"
-	       : format_hwint (line_ptr->count, 0, -1), line_num);
+      fprintf (gcov_file, "%9s:%5u:",
+	       !line->exists ? "-" : !line->count ? "#####"
+	       : format_gcov (line->count, 0, -1), line_num);
       
       if (retval)
 	{
@@ -1506,9 +1524,7 @@ output_line_info (gcov_file, line_info, 
 	      retval = fgets (string, STRING_SIZE, source_file);
 	      if (!retval)
 		{
-		  fnotice (stderr,
-			   "Unexpected EOF while reading source file %s.\n",
-			   total->name);
+		  fnotice (stderr, "%s:unexpected EOF\n", src->name);
 		  break;
 		}
 	      fputs (retval, gcov_file);
@@ -1518,34 +1534,33 @@ output_line_info (gcov_file, line_info, 
       if (!retval)
 	fputs ("??\n", gcov_file);
       
-      if (output_branch_probs)
+      if (flag_branches)
 	{
-	  int i;
-	  struct arcdata *a_ptr;
+	  int ix;
+	  arc_t *arc;
 	  
-	  for (i = 0, a_ptr = line_ptr->branches; a_ptr;
-	       a_ptr = a_ptr->next, i++)
+	  for (ix = 0, arc = line->branches; arc; arc = arc->line_next, ix++)
 	    {
-	      if (a_ptr->call_insn)
+	      if (arc->is_call)
 		{
-		  if (a_ptr->total == 0)
-		    fnotice (gcov_file, "call   %2d never executed\n", i);
-		  else
+		  if (arc->src->count)
 		    fnotice
-		      (gcov_file, "call   %2d returns %s\n", i,
-		       format_hwint (a_ptr->total - a_ptr->hits,
-				     a_ptr->total,
-				     -output_branch_counts));
+		      (gcov_file, "call   %2d returns %s\n", ix,
+		       format_gcov (arc->src->count - arc->count,
+				    arc->src->count,
+				    -flag_counts));
+		  else
+		    fnotice (gcov_file, "call   %2d never executed\n", ix);
 		}
 	      else
 		{
-		  if (a_ptr->total == 0)
-		    fnotice (gcov_file, "branch %2d never executed\n", i);
-		  else
+		  if (arc->src->count)
 		    fnotice
-		      (gcov_file, "branch %2d taken %s\n", i,
-		       format_hwint (a_ptr->hits, a_ptr->total,
-				     -output_branch_counts));
+		      (gcov_file, "branch %2d taken %s\n", ix,
+		       format_gcov (arc->count, arc->src->count,
+				    -flag_counts));
+		  else
+		    fnotice (gcov_file, "branch %2d never executed\n", ix);
 		}
 	    }
 	}
@@ -1557,7 +1572,7 @@ output_line_info (gcov_file, line_info, 
     {
       for (; (retval = fgets (string, STRING_SIZE, source_file)); line_num++)
 	{
-	  fprintf (gcov_file, "%9s:%5ld:%s", "-", line_num, retval);
+	  fprintf (gcov_file, "%9s:%5u:%s", "-", line_num, retval);
 	  
 	  while (!retval[0] || retval[strlen (retval) - 1] != '\n')
 	    {
@@ -1572,63 +1587,3 @@ output_line_info (gcov_file, line_info, 
   if (source_file)
     fclose (source_file);
 }
-
-/* Calculate line execution counts, and output a .gcov file for source
-   file S_PTR. Allocate an array big enough to hold a count for each
-   line.  Scan through the bb_data, and when the file name matches the
-   current file name, then for each following line number, increment
-   the line number execution count indicated by the execution count of
-   the appropriate basic block.  */
-
-static void
-output_data (s_ptr)
-	     struct sourcefile *s_ptr;
-{
-  struct line_info *line_info	/* line info data */
-    = (struct line_info *) xcalloc (s_ptr->maxlineno,
-				    sizeof (struct line_info));
-  long line_num;
-  struct coverage total;
-  
-  memset (&total, 0, sizeof (total));
-  total.name = s_ptr->name;
-  
-  init_line_info (line_info, &total, s_ptr->maxlineno);
-  function_summary (&total, "file");
-
-  if (output_gcov_file)
-    {
-      /* Now the statistics are ready.  Read in the source file one
-	 line at a time, and output that line to the gcov file
-	 preceded by its execution information.  */
-      
-      char *gcov_file_name = make_gcov_file_name (total.name);
-      FILE *gcov_file = fopen (gcov_file_name, "w");
-      
-      if (gcov_file)
-	{
-	  fnotice (stdout, "Creating %s.\n", gcov_file_name);
-	  output_line_info (gcov_file, line_info, &total, s_ptr->maxlineno);
-	  if (ferror (gcov_file))
-	    fnotice (stderr, "Error writing output file %s.\n",
-		     gcov_file_name);
-	  fclose (gcov_file);
-	}
-      else
-	fnotice (stderr, "Could not open output file %s.\n", gcov_file_name);
-      free (gcov_file_name);
-    }
-
-  /* Free data.  */
-  for (line_num = 1; line_num != s_ptr->maxlineno; line_num++)
-    {
-      struct arcdata *branch, *next;
-
-      for (branch = line_info[line_num].branches; branch; branch = next)
-	{
-	  next = branch->next;
-	  free (branch);
-	}
-    }
-  free (line_info);
-}
--- gcc-3.3.1/gcc/genopinit.c.hammer-branch	2002-08-04 01:21:30.000000000 +0200
+++ gcc-3.3.1/gcc/genopinit.c	2003-08-05 18:22:47.000000000 +0200
@@ -112,6 +112,11 @@ static const char * const optabs[] =
     abs_optab->handlers[(int) $A].insn_code = CODE_FOR_$(abs$F$a2$)",
   "absv_optab->handlers[(int) $A].insn_code = CODE_FOR_$(absv$I$a2$)",
   "sqrt_optab->handlers[$A].insn_code = CODE_FOR_$(sqrt$a2$)",
+  "floor_optab->handlers[$A].insn_code = CODE_FOR_$(floor$a2$)",
+  "ceil_optab->handlers[$A].insn_code = CODE_FOR_$(ceil$a2$)",
+  "round_optab->handlers[$A].insn_code = CODE_FOR_$(round$a2$)",
+  "trunc_optab->handlers[$A].insn_code = CODE_FOR_$(trunc$a2$)",
+  "nearbyint_optab->handlers[$A].insn_code = CODE_FOR_$(nearbyint$a2$)",
   "sin_optab->handlers[$A].insn_code = CODE_FOR_$(sin$a2$)",
   "cos_optab->handlers[$A].insn_code = CODE_FOR_$(cos$a2$)",
   "exp_optab->handlers[$A].insn_code = CODE_FOR_$(exp$a2$)",
--- gcc-3.3.1/gcc/langhooks-def.h.hammer-branch	2003-06-10 20:34:07.000000000 +0200
+++ gcc-3.3.1/gcc/langhooks-def.h	2003-08-05 18:22:47.000000000 +0200
@@ -167,6 +167,14 @@ void write_global_declarations PARAMS ((
   LANG_HOOKS_TREE_INLINING_CONVERT_PARM_FOR_INLINING \
 } \
 
+#define LANG_HOOKS_CALLGRAPH_LOWER_FUNCTION NULL
+#define LANG_HOOKS_CALLGRAPH_EXPAND_FUNCTION NULL
+
+#define LANG_HOOKS_CALLGRAPH_INITIALIZER { \
+  LANG_HOOKS_CALLGRAPH_LOWER_FUNCTION, \
+  LANG_HOOKS_CALLGRAPH_EXPAND_FUNCTION, \
+} \
+
 #define LANG_HOOKS_FUNCTION_INITIALIZER {	\
   LANG_HOOKS_FUNCTION_INIT,			\
   LANG_HOOKS_FUNCTION_FINAL,			\
@@ -268,6 +276,7 @@ int lhd_tree_dump_type_quals			PARAMS ((
   LANG_HOOKS_FORMAT_ATTRIBUTE_TABLE, \
   LANG_HOOKS_FUNCTION_INITIALIZER, \
   LANG_HOOKS_TREE_INLINING_INITIALIZER, \
+  LANG_HOOKS_CALLGRAPH_INITIALIZER, \
   LANG_HOOKS_TREE_DUMP_INITIALIZER, \
   LANG_HOOKS_DECLS, \
   LANG_HOOKS_FOR_TYPES_INITIALIZER \
--- gcc-3.3.1/gcc/langhooks.h.hammer-branch	2003-06-10 20:34:08.000000000 +0200
+++ gcc-3.3.1/gcc/langhooks.h	2003-08-05 18:22:47.000000000 +0200
@@ -58,6 +58,15 @@ struct lang_hooks_for_tree_inlining
 							 union tree_node *));
 };
 
+struct lang_hooks_for_callgraph
+{
+  /* Function passed as argument is needed and will be compiled.
+     Lower the representation so the calls are explicit.  */
+  void (*lower_function) PARAMS ((union tree_node *));
+  /* Produce RTL for function passed as argument.  */
+  void (*expand_function) PARAMS ((union tree_node *));
+};
+
 /* Lang hooks for management of language-specific data or status
    when entering / leaving functions etc.  */
 struct lang_hooks_for_functions
@@ -360,6 +369,8 @@ struct lang_hooks
 
   struct lang_hooks_for_tree_inlining tree_inlining;
 
+  struct lang_hooks_for_callgraph callgraph;
+
   struct lang_hooks_for_tree_dump tree_dump;
 
   struct lang_hooks_for_decls decls;
--- gcc-3.3.1/gcc/libgcc2.c.hammer-branch	2002-10-23 12:47:24.000000000 +0200
+++ gcc-3.3.1/gcc/libgcc2.c	2003-08-05 18:22:47.000000000 +0200
@@ -1249,59 +1249,64 @@ __eprintf (const char *string, const cha
 #endif
 #endif
 
-#ifdef L_bb
+#ifdef L_gcov
 
-struct bb_function_info {
-  long checksum;
-  int arc_count;
-  const char *name;
-};
-
-/* Structure emitted by --profile-arcs  */
-struct bb
-{
-  long zero_word;
-  const char *filename;
-  gcov_type *counts;
-  long ncounts;
-  struct bb *next;
-
-  /* Older GCC's did not emit these fields.  */
-  long sizeof_bb;
-  struct bb_function_info *function_infos;
-};
-
-#ifndef inhibit_libc
-
-/* Arc profile dumper. Requires atexit and stdio.  */
+/* Gcov profile dumper. Requires atexit and stdio.  */
 
 #undef NULL /* Avoid errors if stdio.h and our stddef.h mismatch.  */
 #include <stdio.h>
 
 #include "gcov-io.h"
 #include <string.h>
-#ifdef TARGET_HAS_F_SETLKW
+#if defined (TARGET_HAS_F_SETLKW)
 #include <fcntl.h>
 #include <errno.h>
 #endif
 
-/* Chain of per-object file bb structures.  */
-static struct bb *bb_head;
+/* Chain of per-object gcov structures.  */
+static struct gcov_info *gcov_list;
 
-/* Dump the coverage counts. We merge with existing counts when
-   possible, to avoid growing the .da files ad infinitum.  */
+/* A program checksum allows us to distinguish program data for an
+   object file included in multiple programs.  */
+static unsigned gcov_crc32;
 
-void
-__bb_exit_func (void)
+static void
+gcov_version_mismatch (struct gcov_info *ptr, unsigned version)
 {
-  struct bb *ptr;
-  int i;
+  unsigned expected = GCOV_VERSION;
+  unsigned ix;
+  char e[4], v[4];
+
+  for (ix = 4; ix--; expected >>= 8, version >>= 8)
+    {
+      e[ix] = expected;
+      v[ix] = version;
+    }
+  
+  fprintf (stderr,
+	   "profiling:%s:Version mismatch - expected %.4s got %.4s\n",
+	   ptr->filename, e, v);
+}
+
+/* Dump the coverage counts. We merge with existing counts when
+   possible, to avoid growing the .da files ad infinitum. We use this
+   program's checksum to make sure we only accumulate whole program
+   statistics to the correct summary. An object file might be embedded
+   in two separate programs, and we must keep the two program
+   summaries separate. */
+
+static void
+gcov_exit (void)
+{
+  struct gcov_info *ptr;
+  unsigned ix, jx;
+  gcov_type program_max_one = 0;
+  gcov_type program_max_sum = 0;
   gcov_type program_sum = 0;
-  gcov_type program_max = 0;
-  long program_arcs = 0;
-  gcov_type merged_sum = 0;
-  gcov_type merged_max = 0;
-  long merged_arcs = 0;
+  int program_runs = 0;
+  unsigned program_arcs = 0;
+  merger_function merger;
+  struct gcov_summary last_prg;
   
 #if defined (TARGET_HAS_F_SETLKW)
   struct flock s_flock;
@@ -1313,52 +1318,89 @@ __bb_exit_func (void)
   s_flock.l_pid = getpid ();
 #endif
 
-  /* Non-merged stats for this program.  */
-  for (ptr = bb_head; ptr; ptr = ptr->next)
+  last_prg.runs = 0;
+
+  for (ptr = gcov_list; ptr; ptr = ptr->next)
     {
-      for (i = 0; i < ptr->ncounts; i++)
+      unsigned arc_data_index;
+      gcov_type *count_ptr;
+
+      if (!ptr->filename)
+	continue;
+
+      for (arc_data_index = 0;
+	   arc_data_index < ptr->n_counter_sections
+	   && ptr->counter_sections[arc_data_index].tag != GCOV_TAG_ARC_COUNTS;
+	   arc_data_index++)
+	continue;
+
+      for (ix = ptr->counter_sections[arc_data_index].n_counters,
+	   count_ptr = ptr->counter_sections[arc_data_index].counters; ix--;)
 	{
-	  program_sum += ptr->counts[i];
+	  gcov_type count = *count_ptr++;
 
-	  if (ptr->counts[i] > program_max)
-	    program_max = ptr->counts[i];
+	  if (count > program_max_one)
+	    program_max_one = count;
+	  program_sum += count;
 	}
-      program_arcs += ptr->ncounts;
+      program_arcs += ptr->counter_sections[arc_data_index].n_counters;
     }
   
-  for (ptr = bb_head; ptr; ptr = ptr->next)
+  for (ptr = gcov_list; ptr; ptr = ptr->next)
     {
       FILE *da_file;
-      gcov_type object_max = 0;
-      gcov_type object_sum = 0;
-      long object_functions = 0;
+      struct gcov_summary object;
+      struct gcov_summary local_prg;
       int merging = 0;
-      int error = 0;
-      struct bb_function_info *fn_info;
+      long base;
+      const struct function_info *fn_info;
+      gcov_type **counters;
       gcov_type *count_ptr;
+      gcov_type object_max_one = 0;
+      gcov_type count;
+      unsigned tag, length, flength, checksum;
+      unsigned arc_data_index, f_sect_index, sect_index;
+
+      ptr->wkspc = 0;
+      if (!ptr->filename)
+	continue;
+
+      counters = malloc (sizeof (gcov_type *) * ptr->n_counter_sections);
+      for (ix = 0; ix < ptr->n_counter_sections; ix++)
+	counters[ix] = ptr->counter_sections[ix].counters;
+
+      for (arc_data_index = 0;
+	   arc_data_index < ptr->n_counter_sections
+	   && ptr->counter_sections[arc_data_index].tag != GCOV_TAG_ARC_COUNTS;
+	   arc_data_index++)
+	continue;
+
+      if (arc_data_index == ptr->n_counter_sections)
+	{
+	  /* For now; later we may want to just measure other profiles,
+	     but now I am lazy to check for all consequences.  */
+	  abort ();
+	}
+      for (ix = ptr->counter_sections[arc_data_index].n_counters,
+	   count_ptr = ptr->counter_sections[arc_data_index].counters; ix--;)
+	{
+	  gcov_type count = *count_ptr++;
+
+	  if (count > object_max_one)
+	    object_max_one = count;
+	}
       
-      /* Open for modification */
-      da_file = fopen (ptr->filename, "r+b");
+      memset (&local_prg, 0, sizeof (local_prg));
+      memset (&object, 0, sizeof (object));
       
-      if (da_file)
+      /* Open for modification */
+      if ((da_file = fopen (ptr->filename, "r+b")))
 	merging = 1;
+      else if ((da_file = fopen (ptr->filename, "w+b")))
+	;
       else
 	{
-	  /* Try for appending */
-	  da_file = fopen (ptr->filename, "ab");
-	  /* Some old systems might not allow the 'b' mode modifier.
-             Therefore, try to open without it.  This can lead to a
-             race condition so that when you delete and re-create the
-             file, the file might be opened in text mode, but then,
-             you shouldn't delete the file in the first place.  */
-	  if (!da_file)
-	    da_file = fopen (ptr->filename, "a");
-	}
-      
-      if (!da_file)
-	{
-	  fprintf (stderr, "arc profiling: Can't open output file %s.\n",
-		   ptr->filename);
+	  fprintf (stderr, "profiling:%s:Cannot open\n", ptr->filename);
 	  ptr->filename = 0;
 	  continue;
 	}
@@ -1371,193 +1413,317 @@ __bb_exit_func (void)
 	     && errno == EINTR)
 	continue;
 #endif
-      for (fn_info = ptr->function_infos; fn_info->arc_count != -1; fn_info++)
-	object_functions++;
-
       if (merging)
 	{
 	  /* Merge data from file.  */
-	  long tmp_long;
-	  gcov_type tmp_gcov;
-	  
-	  if (/* magic */
-	      (__read_long (&tmp_long, da_file, 4) || tmp_long != -123l)
-	      /* functions in object file.  */
-	      || (__read_long (&tmp_long, da_file, 4)
-		  || tmp_long != object_functions)
-	      /* extension block, skipped */
-	      || (__read_long (&tmp_long, da_file, 4)
-		  || fseek (da_file, tmp_long, SEEK_CUR)))
+	      
+	  if (gcov_read_unsigned (da_file, &tag) || tag != GCOV_DATA_MAGIC)
 	    {
-	    read_error:;
-	      fprintf (stderr, "arc profiling: Error merging output file %s.\n",
+	      fprintf (stderr, "profiling:%s:Not a gcov data file\n",
 		       ptr->filename);
-	      clearerr (da_file);
+	    read_fatal:;
+	      fclose (da_file);
+	      ptr->filename = 0;
+	      continue;
 	    }
-	  else
+	  if (gcov_read_unsigned (da_file, &length) || length != GCOV_VERSION)
 	    {
-	      /* Merge execution counts for each function.  */
-	      count_ptr = ptr->counts;
-	      
-	      for (fn_info = ptr->function_infos; fn_info->arc_count != -1;
-		   fn_info++)
+	      gcov_version_mismatch (ptr, length);
+	      goto read_fatal;
+	    }
+	  
+	  /* Merge execution counts for each function.  */
+	  for (ix = ptr->n_functions, fn_info = ptr->functions;
+	       ix--; fn_info++)
+	    {
+	      if (gcov_read_unsigned (da_file, &tag)
+		  || gcov_read_unsigned (da_file, &length))
 		{
-		  if (/* function name delim */
-		      (__read_long (&tmp_long, da_file, 4)
-		       || tmp_long != -1)
-		      /* function name length */
-		      || (__read_long (&tmp_long, da_file, 4)
-			  || tmp_long != (long) strlen (fn_info->name))
-		      /* skip string */
-		      || fseek (da_file, ((tmp_long + 1) + 3) & ~3, SEEK_CUR)
-		      /* function name delim */
-		      || (__read_long (&tmp_long, da_file, 4)
-			  || tmp_long != -1))
-		    goto read_error;
+		read_error:;
+		  fprintf (stderr, "profiling:%s:Error merging\n",
+			   ptr->filename);
+		  goto read_fatal;
+		}
+
+	      /* Check function */
+	      if (tag != GCOV_TAG_FUNCTION)
+		{
+		read_mismatch:;
+		  fprintf (stderr, "profiling:%s:Merge mismatch at %s\n",
+			   ptr->filename, fn_info->name);
+		  goto read_fatal;
+		}
+
+	      if (gcov_read_unsigned (da_file, &flength)
+		  || gcov_skip_string (da_file, flength)
+		  || gcov_read_unsigned (da_file, &checksum))
+		goto read_error;
+	      if (flength != strlen (fn_info->name)
+		  || checksum != fn_info->checksum)
+		goto read_mismatch;
+
+	      /* Counters.  */
+	      for (f_sect_index = 0;
+		   f_sect_index < fn_info->n_counter_sections;
+		   f_sect_index++)
+		{
+		  unsigned n_counters;
 
-		  if (/* function checksum */
-		      (__read_long (&tmp_long, da_file, 4)
-		       || tmp_long != fn_info->checksum)
-		      /* arc count */
-		      || (__read_long (&tmp_long, da_file, 4)
-			  || tmp_long != fn_info->arc_count))
+		  if (gcov_read_unsigned (da_file, &tag)
+		      || gcov_read_unsigned (da_file, &length))
 		    goto read_error;
-		  
-		  for (i = fn_info->arc_count; i > 0; i--, count_ptr++)
-		    if (__read_gcov_type (&tmp_gcov, da_file, 8))
-		      goto read_error;
-		    else
-		      *count_ptr += tmp_gcov;
+		  for (sect_index = 0;
+		       sect_index < ptr->n_counter_sections;
+		       sect_index++)
+		    if (ptr->counter_sections[sect_index].tag == tag)
+		      break;
+		  if (sect_index == ptr->n_counter_sections
+		      || fn_info->counter_sections[f_sect_index].tag != tag)
+		    goto read_mismatch;
+
+		  n_counters = fn_info->counter_sections[f_sect_index].n_counters;
+		  if (n_counters != length / 8)
+		    goto read_mismatch;
+		 
+		  if ((merger = profile_merger_for_tag (tag)))
+		    {
+		      if ((*merger) (da_file, counters[sect_index], n_counters))
+			goto read_error;
+		    }
+		  else
+		    {
+		      for (jx = 0; jx < n_counters; jx++)
+			if (gcov_read_counter (da_file, &count, 0))
+			  goto read_error;
+			else
+			  counters[sect_index][jx] += count;
+		    }
+		  counters[sect_index] += n_counters;
+		}
+	    }
+
+	  /* Check object summary */
+	  if (gcov_read_unsigned (da_file, &tag)
+	      || gcov_read_unsigned (da_file, &length))
+	    goto read_error;
+	  if (tag != GCOV_TAG_OBJECT_SUMMARY)
+	    goto read_mismatch;
+	  if (gcov_read_summary (da_file, &object))
+	    goto read_error;
+
+	  /* Check program summary */
+	  while (1)
+	    {
+	      long base = ftell (da_file);
+	      
+	      if (gcov_read_unsigned (da_file, &tag)
+		  || gcov_read_unsigned (da_file, &length))
+		{
+		  if (feof (da_file))
+		    break;
+		  goto read_error;
+		}
+	      if (tag != GCOV_TAG_PROGRAM_SUMMARY
+		  && tag != GCOV_TAG_PLACEHOLDER_SUMMARY
+		  && tag != GCOV_TAG_INCORRECT_SUMMARY)
+		goto read_mismatch;
+	      if (gcov_read_summary (da_file, &local_prg))
+		goto read_error;
+	      if (local_prg.checksum != gcov_crc32)
+		{
+		  memset (&local_prg, 0, sizeof (local_prg));
+		  continue;
 		}
+	      if (tag == GCOV_TAG_PLACEHOLDER_SUMMARY)
+		{
+		  fprintf (stderr,
+			   "profiling:%s:Concurrent race detected\n",
+			   ptr->filename);
+		  goto read_fatal;
+		}
+	      merging = -1;
+	      if (tag != GCOV_TAG_PROGRAM_SUMMARY)
+		break;
+	      
+	      /* If everything done correctly, the summaries should be
+	         computed equal for each module.  */
+	      if (last_prg.runs
+#if defined (TARGET_HAS_F_SETLKW)
+	          /* We may be updating the files in parallel.  */
+		  && last_prg.runs == local_prg.runs
+#endif
+		  && memcmp (&last_prg, &local_prg, sizeof (last_prg)))
+		{
+		  fprintf (stderr, "profiling:%s:Invocation mismatch\n",
+			   ptr->filename);
+		  local_prg.runs = 0;
+		}
+	      else
+		memcpy (&last_prg, &local_prg, sizeof (last_prg));
+	      ptr->wkspc = base;
+	      break;
 	    }
 	  fseek (da_file, 0, SEEK_SET);
 	}
-      
-      /* Calculate the per-object statistics.  */
-      for (i = 0; i < ptr->ncounts; i++)
-	{
-	  object_sum += ptr->counts[i];
 
-	  if (ptr->counts[i] > object_max)
-	    object_max = ptr->counts[i];
-	}
-      merged_sum += object_sum;
-      if (merged_max < object_max)
-	merged_max = object_max;
-      merged_arcs += ptr->ncounts;
+      object.runs++;
+      object.arcs = ptr->counter_sections[arc_data_index].n_counters;
+      object.arc_sum = 0;
+      if (object.arc_max_one < object_max_one)
+	object.arc_max_one = object_max_one;
+      object.arc_sum_max += object_max_one;
       
       /* Write out the data.  */
       if (/* magic */
-	  __write_long (-123, da_file, 4)
-	  /* number of functions in object file.  */
-	  || __write_long (object_functions, da_file, 4)
-	  /* length of extra data in bytes.  */
-	  || __write_long ((4 + 8 + 8) + (4 + 8 + 8), da_file, 4)
-
-	  /* whole program statistics. If merging write per-object
-	     now, rewrite later */
-	  /* number of instrumented arcs.  */
-	  || __write_long (merging ? ptr->ncounts : program_arcs, da_file, 4)
-	  /* sum of counters.  */
-	  || __write_gcov_type (merging ? object_sum : program_sum, da_file, 8)
-	  /* maximal counter.  */
-	  || __write_gcov_type (merging ? object_max : program_max, da_file, 8)
-
-	  /* per-object statistics.  */
-	  /* number of counters.  */
-	  || __write_long (ptr->ncounts, da_file, 4)
-	  /* sum of counters.  */
-	  || __write_gcov_type (object_sum, da_file, 8)
-	  /* maximal counter.  */
-	  || __write_gcov_type (object_max, da_file, 8))
+	  gcov_write_unsigned (da_file, GCOV_DATA_MAGIC)
+	  /* version number */
+	  || gcov_write_unsigned (da_file, GCOV_VERSION))
 	{
 	write_error:;
-	  fprintf (stderr, "arc profiling: Error writing output file %s.\n",
-		   ptr->filename);
-	  error = 1;
+	  fclose (da_file);
+	  fprintf (stderr, "profiling:%s:Error writing\n", ptr->filename);
+	  ptr->filename = 0;
+	  continue;
 	}
-      else
-	{
-	  /* Write execution counts for each function.  */
-	  count_ptr = ptr->counts;
-
-	  for (fn_info = ptr->function_infos; fn_info->arc_count != -1;
-	       fn_info++)
+      
+      /* Write execution counts for each function.  */
+      for (ix = 0; ix < ptr->n_counter_sections; ix++)
+	counters[ix] = ptr->counter_sections[ix].counters;
+      for (ix = ptr->n_functions, fn_info = ptr->functions; ix--; fn_info++)
+	{
+	  /* Announce function. */
+	  if (gcov_write_unsigned (da_file, GCOV_TAG_FUNCTION)
+	      || !(base = gcov_reserve_length (da_file))
+	      /* function name */
+	      || gcov_write_string (da_file, fn_info->name,
+				    strlen (fn_info->name))
+	      /* function checksum */
+	      || gcov_write_unsigned (da_file, fn_info->checksum)
+	      || gcov_write_length (da_file, base))
+	    goto write_error;
+
+	  /* counters.  */
+	  for (f_sect_index = 0;
+	       f_sect_index < fn_info->n_counter_sections;
+	       f_sect_index++)
 	    {
-	      if (__write_gcov_string (fn_info->name,
-				       strlen (fn_info->name), da_file, -1)
-		  || __write_long (fn_info->checksum, da_file, 4)
-		  || __write_long (fn_info->arc_count, da_file, 4))
+	      tag = fn_info->counter_sections[f_sect_index].tag;
+	      for (sect_index = 0;
+    		   sect_index < ptr->n_counter_sections;
+		   sect_index++)
+		if (ptr->counter_sections[sect_index].tag == tag)
+		  break;
+	      if (sect_index == ptr->n_counter_sections)
+		abort ();
+
+	      if (gcov_write_unsigned (da_file, tag)
+		  || !(base = gcov_reserve_length (da_file)))
 		goto write_error;
+	  
+    	      for (jx = fn_info->counter_sections[f_sect_index].n_counters; jx--;)
+		{
+		  gcov_type count = *counters[sect_index]++;
 	      
-	      for (i = fn_info->arc_count; i > 0; i--, count_ptr++)
-		if (__write_gcov_type (*count_ptr, da_file, 8))
-		  goto write_error; /* RIP Edsger Dijkstra */
+		  if (tag == GCOV_TAG_ARC_COUNTS)
+		    {
+		      object.arc_sum += count;
+		      if (object.arc_max_sum < count)
+			object.arc_max_sum = count;
+		    }
+		  if (gcov_write_counter (da_file, count))
+		    goto write_error; /* RIP Edsger Dijkstra */
+		}
+	      if (gcov_write_length (da_file, base))
+		goto write_error;
 	    }
 	}
 
+      /* Object file summary. */
+      if (gcov_write_summary (da_file, GCOV_TAG_OBJECT_SUMMARY, &object))
+	goto write_error;
+
+      /* Generate whole program statistics.  */
+      local_prg.runs++;
+      local_prg.checksum = gcov_crc32;
+      local_prg.arcs = program_arcs;
+      local_prg.arc_sum += program_sum;
+      if (local_prg.arc_max_one < program_max_one)
+	local_prg.arc_max_one = program_max_one;
+      local_prg.arc_max_sum += program_max_one;
+      local_prg.arc_sum_max += program_max_one;
+
+      if (merging >= 0)
+	{
+	  if (fseek (da_file, 0, SEEK_END))
+	    goto write_error;
+	  ptr->wkspc = ftell (da_file);
+	  if (gcov_write_summary (da_file, GCOV_TAG_PROGRAM_SUMMARY,
+				  &local_prg))
+	    goto write_error;
+	  ptr->wkspc = ftell (da_file);
+	  if (fseek (da_file, 0, SEEK_END))
+	    goto write_error;
+	}
+      else if (ptr->wkspc)
+	{
+	  if (fseek (da_file, ptr->wkspc, SEEK_SET)
+	      || gcov_write_summary (da_file, GCOV_TAG_PROGRAM_SUMMARY,
+				     &local_prg)
+	      || fflush (da_file))
+	    fprintf (stderr, "profiling:%s:Error writing\n", ptr->filename);
+	}
+      if (fflush (da_file))
+	goto write_error;
+
       if (fclose (da_file))
 	{
-	  fprintf (stderr, "arc profiling: Error closing output file %s.\n",
-		   ptr->filename);
-	  error = 1;
+	  fprintf (stderr, "profiling:%s:Error closing\n", ptr->filename);
+	  ptr->filename = 0;
 	}
-      if (error || !merging)
-	ptr->filename = 0;
+      free(counters);
     }
-
-  /* Upate whole program statistics.  */
-  for (ptr = bb_head; ptr; ptr = ptr->next)
-    if (ptr->filename)
-      {
-	FILE *da_file;
-	
-	da_file = fopen (ptr->filename, "r+b");
-	if (!da_file)
-	  {
-	    fprintf (stderr, "arc profiling: Cannot reopen %s.\n",
-		     ptr->filename);
-	    continue;
-	  }
-	
-#if defined (TARGET_HAS_F_SETLKW)
-	while (fcntl (fileno (da_file), F_SETLKW, &s_flock)
-	       && errno == EINTR)
-	  continue;
-#endif
-	
-	if (fseek (da_file, 4 * 3, SEEK_SET)
-	    /* number of instrumented arcs.  */
-	    || __write_long (merged_arcs, da_file, 4)
-	    /* sum of counters.  */
-	    || __write_gcov_type (merged_sum, da_file, 8)
-	    /* maximal counter.  */
-	    || __write_gcov_type (merged_max, da_file, 8))
-	  fprintf (stderr, "arc profiling: Error updating program header %s.\n",
-		   ptr->filename);
-	if (fclose (da_file))
-	  fprintf (stderr, "arc profiling: Error reclosing %s\n",
-		   ptr->filename);
-      }
 }
 
 /* Add a new object file onto the bb chain.  Invoked automatically
    when running an object file's global ctors.  */
 
 void
-__bb_init_func (struct bb *blocks)
+__gcov_init (struct gcov_info *info)
 {
-  if (blocks->zero_word)
+  if (!info->version)
     return;
+  if (info->version != GCOV_VERSION)
+    gcov_version_mismatch (info, info->version);
+  else
+    {
+      const char *ptr = info->filename;
+      unsigned crc32 = gcov_crc32;
+  
+      do
+	{
+	  unsigned ix;
+	  unsigned value = *ptr << 24;
 
-  /* Initialize destructor and per-thread data.  */
-  if (!bb_head)
-    atexit (__bb_exit_func);
-
-  /* Set up linked list.  */
-  blocks->zero_word = 1;
-  blocks->next = bb_head;
-  bb_head = blocks;
+	  for (ix = 8; ix--; value <<= 1)
+	    {
+	      unsigned feedback;
+
+	      feedback = (value ^ crc32) & 0x80000000 ? 0x04c11db7 : 0;
+	      crc32 <<= 1;
+	      crc32 ^= feedback;
+	    }
+	}
+      while (*ptr++);
+      
+      gcov_crc32 = crc32;
+      
+      if (!gcov_list)
+	atexit (gcov_exit);
+      
+      info->next = gcov_list;
+      gcov_list = info;
+    }
+  info->version = 0;
 }
 
 /* Called before fork or exec - write out profile information gathered so
@@ -1565,21 +1731,22 @@ __bb_init_func (struct bb *blocks)
    profile information gathered so far.  */
 
 void
-__bb_fork_func (void)
+__gcov_flush (void)
 {
-  struct bb *ptr;
+  struct gcov_info *ptr;
 
-  __bb_exit_func ();
-  for (ptr = bb_head; ptr != (struct bb *) 0; ptr = ptr->next)
+  gcov_exit ();
+  for (ptr = gcov_list; ptr; ptr = ptr->next)
     {
-      long i;
-      for (i = ptr->ncounts - 1; i >= 0; i--)
-	ptr->counts[i] = 0;
+      unsigned i, j;
+      
+      for (j = 0; j < ptr->n_counter_sections; j++)
+	for (i = ptr->counter_sections[j].n_counters; i--;)
+	  ptr->counter_sections[j].counters[i] = 0;
     }
 }
 
-#endif /* not inhibit_libc */
-#endif /* L_bb */
+#endif /* L_gcov */
 
 #ifdef L_clear_cache
 /* Clear part of an instruction cache.  */
--- gcc-3.3.1/gcc/libgcc2.h.hammer-branch	2003-04-22 19:17:24.000000000 +0200
+++ gcc-3.3.1/gcc/libgcc2.h	2003-08-05 18:22:47.000000000 +0200
@@ -35,19 +35,6 @@ extern void __clear_cache (char *, char 
 extern void __eprintf (const char *, const char *, unsigned int, const char *)
   __attribute__ ((__noreturn__));
 
-struct bb;
-extern void __bb_exit_func (void);
-extern void __bb_init_func (struct bb *);
-extern void __bb_fork_func (void);
-
-#if LONG_TYPE_SIZE == GCOV_TYPE_SIZE
-typedef long gcov_type;
-#else
-typedef long long gcov_type;
-#endif
-
-extern gcov_type *__bb_find_arc_counters (void);
-
 struct exception_descriptor;
 extern short int __get_eh_table_language (struct exception_descriptor *);
 extern short int __get_eh_table_version (struct exception_descriptor *);
--- gcc-3.3.1/gcc/local-alloc.c.hammer-branch	2002-11-04 17:57:01.000000000 +0100
+++ gcc-3.3.1/gcc/local-alloc.c	2003-08-05 18:22:47.000000000 +0200
@@ -2478,6 +2478,52 @@ dump_local_alloc (file)
      FILE *file;
 {
   int i;
+  int max_regno = max_reg_num ();
+  static const char * const reg_class_names[] = REG_CLASS_NAMES;
+
+  fprintf (file, "%d registers.\n", max_regno);
+  for (i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)
+    if (REG_N_REFS (i))
+      {
+	enum reg_class class, altclass;
+
+	fprintf (file, "\nRegister %d used %d times across %d insns",
+		 i, REG_N_REFS (i), REG_LIVE_LENGTH (i));
+	if (REG_BASIC_BLOCK (i) >= 0)
+	  fprintf (file, " in block %d", REG_BASIC_BLOCK (i));
+	if (REG_N_SETS (i))
+	  fprintf (file, "; set %d time%s", REG_N_SETS (i),
+		   (REG_N_SETS (i) == 1) ? "" : "s");
+	if (regno_reg_rtx[i] != NULL && REG_USERVAR_P (regno_reg_rtx[i]))
+	  fprintf (file, "; user var");
+	if (REG_N_DEATHS (i) != 1)
+	  fprintf (file, "; dies in %d places", REG_N_DEATHS (i));
+	if (REG_N_CALLS_CROSSED (i) == 1)
+	  fprintf (file, "; crosses 1 call");
+	else if (REG_N_CALLS_CROSSED (i))
+	  fprintf (file, "; crosses %d calls", REG_N_CALLS_CROSSED (i));
+	if (regno_reg_rtx[i] != NULL
+	    && PSEUDO_REGNO_BYTES (i) != UNITS_PER_WORD)
+	  fprintf (file, "; %d bytes", PSEUDO_REGNO_BYTES (i));
+
+	class = reg_preferred_class (i);
+	altclass = reg_alternate_class (i);
+	if (class != GENERAL_REGS || altclass != ALL_REGS)
+	  {
+	    if (altclass == ALL_REGS || class == ALL_REGS)
+	      fprintf (file, "; pref %s", reg_class_names[(int) class]);
+	    else if (altclass == NO_REGS)
+	      fprintf (file, "; %s or none", reg_class_names[(int) class]);
+	    else
+	      fprintf (file, "; pref %s, else %s",
+		       reg_class_names[(int) class],
+		       reg_class_names[(int) altclass]);
+	  }
+
+	if (regno_reg_rtx[i] != NULL && REG_POINTER (regno_reg_rtx[i]))
+	  fprintf (file, "; pointer");
+	fprintf (file, ".\n");
+      }
   for (i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)
     if (reg_renumber[i] != -1)
       fprintf (file, ";; Register %d in %d.\n", i, reg_renumber[i]);
--- gcc-3.3.1/gcc/loop-init.c.hammer-branch	2003-08-05 18:22:47.000000000 +0200
+++ gcc-3.3.1/gcc/loop-init.c	2003-08-05 18:22:47.000000000 +0200
@@ -0,0 +1,113 @@
+/* Loop optimizer initialization routines.
+   Copyright (C) 1987, 1988, 1989, 1991, 1992, 1993, 1994, 1995, 1996, 1997,
+   1998, 1999, 2000, 2001, 2002 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+#include "config.h"
+#include "system.h"
+#include "rtl.h"
+#include "hard-reg-set.h"
+#include "basic-block.h"
+#include "cfgloop.h"
+#include "cfglayout.h"
+#include "gcov-io.h"
+#include "profile.h"
+
+/* Initialize loop optimizer.  */
+
+struct loops *
+loop_optimizer_init (dumpfile)
+     FILE *dumpfile;
+{
+  struct loops *loops = xcalloc (1, sizeof (struct loops));
+  edge e;
+
+  /* Avoid anoying special cases of edges going to exit
+     block.  */
+  for (e = EXIT_BLOCK_PTR->pred; e; e = e->pred_next)
+    if ((e->flags & EDGE_FALLTHRU) && e->src->succ->succ_next)
+      split_edge (e);
+
+  /* Find the loops.  */
+
+  if (flow_loops_find (loops, LOOP_TREE) <= 1)
+    {
+      /* No loops.  */
+      flow_loops_free (loops);
+      free (loops);
+      return NULL;
+    }
+
+  /* Initialize structures for layout changes.  */
+  cfg_layout_initialize (loops);
+
+  /* Create pre-headers.  */
+  create_preheaders (loops, CP_SIMPLE_PREHEADERS | CP_INSIDE_CFGLAYOUT);
+
+  /* Force all latches to have only single successor.  */
+  force_single_succ_latches (loops);
+
+  /* Mark irreducible loops.  */
+  mark_irreducible_loops (loops);
+
+  /* Dump loops.  */
+  flow_loops_dump (loops, dumpfile, NULL, 1);
+
+#ifdef ENABLE_CHECKING
+  verify_dominators (loops->cfg.dom);
+  verify_loop_structure (loops);
+#endif
+
+  return loops;
+}
+
+/* Finalize loop optimizer.  */
+void
+loop_optimizer_finalize (loops, dumpfile)
+     struct loops *loops;
+     FILE *dumpfile;
+{
+  basic_block bb;
+
+  /* Finalize layout changes.  */
+  /* Make chain.  */
+  FOR_EACH_BB (bb)
+    if (bb->next_bb != EXIT_BLOCK_PTR)
+      RBI (bb)->next = bb->next_bb;
+
+  /* Another dump.  */
+  free (loops->cfg.rc_order);
+  loops->cfg.rc_order = NULL;
+  free (loops->cfg.dfs_order);
+  loops->cfg.dfs_order = NULL;
+  flow_loops_dump (loops, dumpfile, NULL, 1);
+
+  /* Clean up.  */
+  flow_loops_free (loops);
+  free (loops);
+ 
+  /* Finalize changes.  */
+  cfg_layout_finalize ();
+
+  /* Checking.  */
+#ifdef ENABLE_CHECKING
+  verify_flow_info ();
+#endif
+}
+
--- gcc-3.3.1/gcc/loop-unroll.c.hammer-branch	2003-08-05 18:22:47.000000000 +0200
+++ gcc-3.3.1/gcc/loop-unroll.c	2003-08-05 18:22:47.000000000 +0200
@@ -0,0 +1,1044 @@
+/* Loop unrolling and peeling.
+   Copyright (C) 2002 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+#include "config.h"
+#include "system.h"
+#include "rtl.h"
+#include "hard-reg-set.h"
+#include "basic-block.h"
+#include "cfgloop.h"
+#include "cfglayout.h"
+#include "params.h"
+#include "output.h"
+#include "expr.h"
+
+static void decide_unrolling_and_peeling PARAMS ((struct loops *, int));
+static void peel_loops_completely PARAMS ((struct loops *, int));
+static void decide_peel_simple PARAMS ((struct loops *, struct loop *, int));
+static void decide_peel_once_rolling PARAMS ((struct loops *, struct loop *, int));
+static void decide_peel_completely PARAMS ((struct loops *, struct loop *, int));
+static void decide_unroll_stupid PARAMS ((struct loops *, struct loop *, int));
+static void decide_unroll_constant_iterations PARAMS ((struct loops *, struct loop *, int));
+static void decide_unroll_runtime_iterations PARAMS ((struct loops *, struct loop *, int));
+static void peel_loop_simple PARAMS ((struct loops *, struct loop *));
+static void peel_loop_completely PARAMS ((struct loops *, struct loop *));
+static void unroll_loop_stupid PARAMS ((struct loops *, struct loop *));
+static void unroll_loop_constant_iterations PARAMS ((struct loops *,
+						     struct loop *));
+static void unroll_loop_runtime_iterations PARAMS ((struct loops *,
+						    struct loop *));
+
+/* Unroll and peel (depending on FLAGS) LOOPS.  */
+void
+unroll_and_peel_loops (loops, flags)
+     struct loops *loops;
+     int flags;
+{
+  struct loop *loop, *next;
+  int check;
+
+  /* First perform complete loop peeling (it is almost surely a win,
+     and affects parameters for further decision a lot).  */
+  peel_loops_completely (loops, flags);
+
+  /* Now decide rest of unrolling and peeling.  */
+  decide_unrolling_and_peeling (loops, flags);
+
+  loop = loops->tree_root;
+  while (loop->inner)
+    loop = loop->inner;
+
+  /* Scan the loops, inner ones first.  */
+  while (loop != loops->tree_root)
+    {
+      if (loop->next)
+	{
+	  next = loop->next;
+	  while (next->inner)
+	    next = next->inner;
+	}
+      else
+	next = loop->outer;
+
+      check = 1;
+      switch (loop->lpt_decision.decision)
+	{
+	case LPT_PEEL_COMPLETELY:
+	  /* Already done.  */
+	  abort ();
+	case LPT_PEEL_SIMPLE:
+	  peel_loop_simple (loops, loop);
+	  break;
+	case LPT_UNROLL_CONSTANT:
+	  unroll_loop_constant_iterations (loops, loop);
+	  break;
+	case LPT_UNROLL_RUNTIME:
+	  unroll_loop_runtime_iterations (loops, loop);
+	  break;
+	case LPT_UNROLL_STUPID:
+	  unroll_loop_stupid (loops, loop);
+	  break;
+	case LPT_NONE:
+	  check = 0;
+	  break;
+	default:
+	  abort ();
+	}
+      if (check)
+	{
+#ifdef ENABLE_CHECKING
+	  verify_dominators (loops->cfg.dom);
+	  verify_loop_structure (loops);
+#endif
+	}
+      loop = next;
+    }
+}
+
+/* Check whether to peel loops completely and do so.  */
+static void
+peel_loops_completely (loops, flags)
+     struct loops *loops;
+     int flags;
+{
+  struct loop *loop, *next;
+
+  loop = loops->tree_root;
+  while (loop->inner)
+    loop = loop->inner;
+
+  while (loop != loops->tree_root)
+    {
+      if (loop->next)
+	{
+	  next = loop->next;
+	  while (next->inner)
+	    next = next->inner;
+	}
+      else
+	next = loop->outer;
+
+      loop->lpt_decision.decision = LPT_NONE;
+      loop->has_desc = 0;
+  
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Considering loop %d for complete peeling\n",
+		 loop->num);
+
+      /* Do not peel cold areas.  */
+      if (!maybe_hot_bb_p (loop->header))
+	{
+	  if (rtl_dump_file)
+	    fprintf (rtl_dump_file, ";; Not considering loop, cold area\n");
+	  loop = next;
+	  continue;
+	}
+
+      /* Can the loop be manipulated?  */
+      if (!can_duplicate_loop_p (loop))
+	{
+	  if (rtl_dump_file)
+	    fprintf (rtl_dump_file,
+		     ";; Not considering loop, cannot duplicate\n");
+	  loop = next;
+	  continue;
+	}
+
+      loop->ninsns = num_loop_insns (loop);
+      decide_peel_once_rolling (loops, loop, flags);
+      if (loop->lpt_decision.decision == LPT_NONE)
+	decide_peel_completely (loops, loop, flags);
+
+      if (loop->lpt_decision.decision == LPT_PEEL_COMPLETELY)
+	{
+	  peel_loop_completely (loops, loop);
+#ifdef ENABLE_CHECKING
+	  verify_dominators (loops->cfg.dom);
+	  verify_loop_structure (loops);
+#endif
+	}
+      loop = next;
+    }
+}
+
+/* Decide whether unroll or peel and how much.  */
+static void
+decide_unrolling_and_peeling (loops, flags)
+     struct loops *loops;
+     int flags;
+{
+  struct loop *loop = loops->tree_root, *next;
+
+  while (loop->inner)
+    loop = loop->inner;
+
+  /* Scan the loops, inner ones first.  */
+  while (loop != loops->tree_root)
+    {
+      if (loop->next)
+	{
+	  next = loop->next;
+	  while (next->inner)
+	    next = next->inner;
+	}
+      else
+	next = loop->outer;
+
+      loop->lpt_decision.decision = LPT_NONE;
+
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Considering loop %d\n", loop->num);
+
+      /* Do not peel cold areas.  */
+      if (!maybe_hot_bb_p (loop->header))
+	{
+	  if (rtl_dump_file)
+	    fprintf (rtl_dump_file, ";; Not considering loop, cold area\n");
+	  loop = next;
+	  continue;
+	}
+
+      /* Can the loop be manipulated?  */
+      if (!can_duplicate_loop_p (loop))
+	{
+	  if (rtl_dump_file)
+	    fprintf (rtl_dump_file,
+		     ";; Not considering loop, cannot duplicate\n");
+	  loop = next;
+	  continue;
+	}
+
+      /* Skip non-innermost loops.  */
+      if (loop->inner)
+	{
+	  if (rtl_dump_file)
+	    fprintf (rtl_dump_file, ";; Not considering loop, is not innermost\n");
+	  loop = next;
+	  continue;
+	}
+
+      loop->ninsns = num_loop_insns (loop);
+
+      /* Try transformations one by one in decreasing order of
+	 priority.  */
+
+      decide_unroll_constant_iterations (loops, loop, flags);
+      if (loop->lpt_decision.decision == LPT_NONE)
+	decide_unroll_runtime_iterations (loops, loop, flags);
+      if (loop->lpt_decision.decision == LPT_NONE)
+	decide_unroll_stupid (loops, loop, flags);
+      if (loop->lpt_decision.decision == LPT_NONE)
+	decide_peel_simple (loops, loop, flags);
+
+      loop = next;
+    }
+}
+
+/* Decide whether the loop is once rolling and suitable for complete
+   peeling.  */
+static void
+decide_peel_once_rolling (loops, loop, flags)
+     struct loops *loops;
+     struct loop *loop;
+     int flags ATTRIBUTE_UNUSED;
+{
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Considering peeling once rolling loop\n");
+
+  /* Is the loop small enough?  */
+  if ((unsigned) PARAM_VALUE (PARAM_MAX_ONCE_PEELED_INSNS) < loop->ninsns)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not considering loop, is too big\n");
+      return;
+    }
+
+  /* Check for simple loops.  */
+  loop->simple = simple_loop_p (loops, loop, &loop->desc);
+  loop->has_desc = 1;
+
+  /* Check number of iterations.  */
+  if (!loop->simple || !loop->desc.const_iter || loop->desc.niter !=0)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Unable to prove that the loop rolls exactly once\n");
+      return;
+    }
+
+  /* Success.  */
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Decided to peel exactly once rolling loop\n");
+  loop->lpt_decision.decision = LPT_PEEL_COMPLETELY;
+}
+
+/* Decide whether the loop is suitable for complete peeling.  */
+static void
+decide_peel_completely (loops, loop, flags)
+     struct loops *loops;
+     struct loop *loop;
+     int flags ATTRIBUTE_UNUSED;
+{
+  unsigned npeel;
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Considering peeling completely\n");
+
+  /* Skip non-innermost loops.  */
+  if (loop->inner)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not considering loop, is not innermost\n");
+      return;
+    }
+
+  /* npeel = number of iterations to peel. */
+  npeel = PARAM_VALUE (PARAM_MAX_COMPLETELY_PEELED_INSNS) / loop->ninsns;
+  if (npeel > (unsigned) PARAM_VALUE (PARAM_MAX_COMPLETELY_PEEL_TIMES))
+    npeel = PARAM_VALUE (PARAM_MAX_COMPLETELY_PEEL_TIMES);
+
+  /* Is the loop small enough?  */
+  if (!npeel)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not considering loop, is too big\n");
+      return;
+    }
+
+  /* Check for simple loops.  */
+  if (!loop->has_desc)
+    loop->simple = simple_loop_p (loops, loop, &loop->desc);
+
+  /* Check number of iterations.  */
+  if (!loop->simple || !loop->desc.const_iter)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Unable to prove that the loop iterates constant times\n");
+      return;
+    }
+
+  if (loop->desc.niter > npeel - 1)
+    {
+      if (rtl_dump_file)
+      	{
+	  fprintf (rtl_dump_file, ";; Not peeling loop completely, rolls too much (");
+	  fprintf (rtl_dump_file, HOST_WIDEST_INT_PRINT_DEC,(HOST_WIDEST_INT) loop->desc.niter);
+	  fprintf (rtl_dump_file, "iterations > %d [maximum peelings])\n", npeel);
+	}
+      return;
+    }
+
+  /* Success.  */
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Decided to peel loop completely\n");
+  loop->lpt_decision.decision = LPT_PEEL_COMPLETELY;
+}
+
+/* Peel NPEEL iterations from LOOP, remove exit edges (and cancel the loop
+   completely).  */
+static void
+peel_loop_completely (loops, loop)
+     struct loops *loops;
+     struct loop *loop;
+{
+  sbitmap wont_exit;
+  unsigned HOST_WIDE_INT npeel;
+  edge e;
+  unsigned n_remove_edges, i;
+  edge *remove_edges;
+  struct loop_desc *desc = &loop->desc;
+  
+  npeel = desc->niter;
+
+  wont_exit = sbitmap_alloc (npeel + 2);
+  sbitmap_ones (wont_exit);
+  RESET_BIT (wont_exit, 0);
+  RESET_BIT (wont_exit, npeel + 1);
+  if (desc->may_be_zero)
+    RESET_BIT (wont_exit, 1);
+
+  remove_edges = xcalloc (npeel, sizeof (edge));
+  n_remove_edges = 0;
+
+  if (!duplicate_loop_to_header_edge (loop, loop_preheader_edge (loop),
+	loops, npeel + 1,
+	wont_exit, desc->out_edge, remove_edges, &n_remove_edges,
+	DLTHE_FLAG_UPDATE_FREQ | DLTHE_USE_WONT_EXIT))
+    abort ();
+
+  free (wont_exit);
+
+  /* Remove the exit edges.  */
+  for (i = 0; i < n_remove_edges; i++)
+    remove_path (loops, remove_edges[i]);
+  free (remove_edges);
+
+  /* Now remove the loop.  */
+  for (e = RBI (desc->in_edge->src)->copy->succ;
+       e && e->dest != RBI (desc->in_edge->dest)->copy;
+       e = e->succ_next);
+
+  if (!e)
+    abort ();
+
+  remove_path (loops, e);
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Peeled loop completely, %d times\n", (int) npeel);
+}
+
+/* Decide whether to unroll loop iterating constant number of times and how much.  */
+static void
+decide_unroll_constant_iterations (loops, loop, flags)
+     struct loops *loops;
+     struct loop *loop;
+     int flags;
+{
+  unsigned nunroll, best_copies, best_unroll, n_copies, i;
+
+  if (!(flags & UAP_UNROLL))
+    {
+      /* We were not asked to, just return back silently.  */
+      return;
+    }
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Considering unrolling loop with constant number of iterations\n");
+
+  /* nunroll = total number of copies of the original loop body in
+     unrolled loop (i.e. if it is 2, we have to duplicate loop body once.  */
+  nunroll = PARAM_VALUE (PARAM_MAX_UNROLLED_INSNS) / loop->ninsns;
+  if (nunroll > (unsigned) PARAM_VALUE (PARAM_MAX_UNROLL_TIMES))
+    nunroll = PARAM_VALUE (PARAM_MAX_UNROLL_TIMES);
+
+  /* Skip big loops.  */
+  if (nunroll <= 1)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not considering loop, is too big\n");
+      return;
+    }
+
+  /* Check for simple loops.  */
+  if (!loop->has_desc)
+    loop->simple = simple_loop_p (loops, loop, &loop->desc);
+
+  /* Check number of iterations.  */
+  if (!loop->simple || !loop->desc.const_iter)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Unable to prove that the loop iterates constant times\n");
+      return;
+    }
+
+  /* Check whether the loop rolls enough to consider.  */
+  if (loop->desc.niter < 2 * nunroll)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not unrolling loop, doesn't roll\n");
+      return;
+    }
+
+  /* Success; now compute number of iterations to unroll.  */
+  best_copies = 2 * nunroll + 10;
+
+  i = 2 * nunroll + 2;
+  if ((unsigned) i - 1 >= loop->desc.niter)
+    i = loop->desc.niter - 2;
+
+  for (; i >= nunroll - 1; i--)
+    {
+      unsigned exit_mod = loop->desc.niter % (i + 1);
+
+      if (loop->desc.postincr)
+	n_copies = exit_mod + i + 1;
+      else if (exit_mod != (unsigned) i || loop->desc.may_be_zero)
+	n_copies = exit_mod + i + 2;
+      else
+	n_copies = i + 1;
+
+      if (n_copies < best_copies)
+	{
+	  best_copies = n_copies;
+	  best_unroll = i;
+	}
+    }
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; max_unroll %d (%d copies, initial %d).\n",
+	     best_unroll + 1, best_copies, nunroll);
+
+  loop->lpt_decision.decision = LPT_UNROLL_CONSTANT;
+  loop->lpt_decision.times = best_unroll;
+}
+
+/* Unroll LOOP with constant number of iterations described by DESC.
+   MAX_UNROLL is maximal number of allowed unrollings.  */
+static void
+unroll_loop_constant_iterations (loops, loop)
+     struct loops *loops;
+     struct loop *loop;
+{
+  unsigned HOST_WIDE_INT niter;
+  unsigned exit_mod;
+  sbitmap wont_exit;
+  unsigned n_remove_edges, i;
+  edge *remove_edges;
+  unsigned max_unroll = loop->lpt_decision.times;
+  struct loop_desc *desc = &loop->desc;
+
+  niter = desc->niter;
+
+  if (niter <= (unsigned) max_unroll + 1)
+    abort ();  /* Should not get here.  */
+
+  exit_mod = niter % (max_unroll + 1);
+
+  wont_exit = sbitmap_alloc (max_unroll + 1);
+  sbitmap_ones (wont_exit);
+
+  remove_edges = xcalloc (max_unroll + exit_mod + 1, sizeof (edge));
+  n_remove_edges = 0;
+
+  if (desc->postincr)
+    {
+      /* Counter is incremented after the exit test; leave exit test
+	 in the first copy.  */
+
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Condition on beginning of loop.\n");
+
+      /* Peel exit_mod iterations.  */
+      RESET_BIT (wont_exit, 0);
+      if (desc->may_be_zero)
+	RESET_BIT (wont_exit, 1);
+
+      if (exit_mod
+	  && !duplicate_loop_to_header_edge (loop, loop_preheader_edge (loop),
+		loops, exit_mod,
+		wont_exit, desc->out_edge, remove_edges, &n_remove_edges,
+		DLTHE_FLAG_UPDATE_FREQ | DLTHE_USE_WONT_EXIT))
+	abort ();
+
+      SET_BIT (wont_exit, 1);
+    }
+  else
+    {
+      /* Leave exit test in last copy.  */
+
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Condition on end of loop.\n");
+
+      /* We know that niter >= max_unroll + 2; so we do not need to care of
+	 case when we would exit before reaching the loop.  So just peel
+	 exit_mod + 1 iterations.
+	 */
+      if (exit_mod != (unsigned) max_unroll || desc->may_be_zero)
+	{
+	  RESET_BIT (wont_exit, 0);
+	  if (desc->may_be_zero)
+	    RESET_BIT (wont_exit, 1);
+
+	  if (!duplicate_loop_to_header_edge (loop, loop_preheader_edge (loop),
+		loops, exit_mod + 1,
+		wont_exit, desc->out_edge, remove_edges, &n_remove_edges,
+		DLTHE_FLAG_UPDATE_FREQ | DLTHE_USE_WONT_EXIT))
+	    abort ();
+
+	  SET_BIT (wont_exit, 0);
+	  SET_BIT (wont_exit, 1);
+	}
+
+      RESET_BIT (wont_exit, max_unroll);
+    }
+
+  /* Now unroll the loop.  */
+  if (!duplicate_loop_to_header_edge (loop, loop_latch_edge (loop),
+		loops, max_unroll,
+		wont_exit, desc->out_edge, remove_edges, &n_remove_edges,
+		DLTHE_FLAG_UPDATE_FREQ | DLTHE_USE_WONT_EXIT))
+    abort ();
+
+  free (wont_exit);
+
+  /* Remove the edges.  */
+  for (i = 0; i < n_remove_edges; i++)
+    remove_path (loops, remove_edges[i]);
+  free (remove_edges);
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Unrolled loop %d times, constant # of iterations %i insns\n",max_unroll, num_loop_insns (loop));
+}
+
+/* Decide whether to unroll loop iterating runtime computable number of times
+   and how much.  */
+static void
+decide_unroll_runtime_iterations (loops, loop, flags)
+     struct loops *loops;
+     struct loop *loop;
+     int flags;
+{
+  unsigned nunroll, i;
+
+  if (!(flags & UAP_UNROLL))
+    {
+      /* We were not asked to, just return back silently.  */
+      return;
+    }
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Considering unrolling loop with runtime computable number of iterations\n");
+
+  /* nunroll = total number of copies of the original loop body in
+     unrolled loop (i.e. if it is 2, we have to duplicate loop body once.  */
+  nunroll = PARAM_VALUE (PARAM_MAX_UNROLLED_INSNS) / loop->ninsns;
+  if (nunroll > (unsigned) PARAM_VALUE (PARAM_MAX_UNROLL_TIMES))
+    nunroll = PARAM_VALUE (PARAM_MAX_UNROLL_TIMES);
+
+  /* Skip big loops.  */
+  if (nunroll <= 1)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not considering loop, is too big\n");
+      return;
+    }
+
+  /* Check for simple loops.  */
+  if (!loop->has_desc)
+    loop->simple = simple_loop_p (loops, loop, &loop->desc);
+
+  /* Check simpleness.  */
+  if (!loop->simple)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Unable to prove that the number of iterations can be counted in runtime\n");
+      return;
+    }
+
+  if (loop->desc.const_iter)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Loop iterates constant times\n");
+      return;
+    }
+
+  /* If we have profile feedback, check whether the loop rolls.  */
+  if (loop->header->count && expected_loop_iterations (loop) < 2 * nunroll)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not unrolling loop, doesn't roll\n");
+      return;
+    }
+
+  /* Success; now force nunroll to be power of 2.  */
+  for (i = 1; 2 * i <= nunroll; i *= 2);
+
+  loop->lpt_decision.decision = LPT_UNROLL_RUNTIME;
+  loop->lpt_decision.times = i - 1;
+}
+
+/* Unroll LOOP for that we are able to count number of iterations in runtime.
+   MAX_UNROLL is maximal number of allowed unrollings.  DESC describes the loop.  */
+static void
+unroll_loop_runtime_iterations (loops, loop)
+     struct loops *loops;
+     struct loop *loop;
+{
+  rtx niter, init_code, branch_code, jump, label;
+  unsigned i, j, p;
+  basic_block preheader, *body, *dom_bbs, swtch, ezc_swtch;
+  unsigned n_dom_bbs;
+  sbitmap wont_exit;
+  int may_exit_copy;
+  unsigned n_peel, n_remove_edges;
+  edge *remove_edges, e;
+  bool extra_zero_check, last_may_exit;
+  unsigned max_unroll = loop->lpt_decision.times;
+  struct loop_desc *desc = &loop->desc;
+
+  /* Remember blocks whose dominators will have to be updated.  */
+  dom_bbs = xcalloc (n_basic_blocks, sizeof (basic_block));
+  n_dom_bbs = 0;
+
+  body = get_loop_body (loop);
+  for (i = 0; i < loop->num_nodes; i++)
+    {
+      unsigned nldom;
+      basic_block *ldom;
+
+      nldom = get_dominated_by (loops->cfg.dom, body[i], &ldom);
+      for (j = 0; j < nldom; j++)
+	if (!flow_bb_inside_loop_p (loop, ldom[j]))
+	  dom_bbs[n_dom_bbs++] = ldom[j];
+
+      free (ldom);
+    }
+  free (body);
+
+  if (desc->postincr)
+    {
+      /* Leave exit in first copy.  */
+      may_exit_copy = 0;
+      n_peel = max_unroll - 1;
+      extra_zero_check = true;
+      last_may_exit = false;
+    }
+  else
+    {
+      /* Leave exit in last copy.  */
+      may_exit_copy = max_unroll;
+      n_peel = max_unroll;
+      extra_zero_check = false;
+      last_may_exit = true;
+    }
+
+  /* Normalization.  */
+  start_sequence ();
+  niter = count_loop_iterations (desc, NULL, NULL);
+  if (!niter)
+    abort ();
+  niter = force_operand (niter, NULL);
+
+  /* Count modulo by ANDing it with max_unroll.  */
+  niter = expand_simple_binop (GET_MODE (desc->var), AND,
+			       niter,
+			       GEN_INT (max_unroll),
+			       NULL_RTX, 0, OPTAB_LIB_WIDEN);
+
+  init_code = get_insns ();
+  end_sequence ();
+
+  /* Precondition the loop.  */
+  loop_split_edge_with (loop_preheader_edge (loop), init_code, loops);
+
+  remove_edges = xcalloc (max_unroll + n_peel + 1, sizeof (edge));
+  n_remove_edges = 0;
+
+  wont_exit = sbitmap_alloc (max_unroll + 2);
+
+  /* Peel the first copy of loop body (almost always we must leave exit test
+     here; the only exception is when we have extra_zero_check and the number
+     of iterations is reliable (i.e. comes out of NE condition).  Also record
+     the place of (possible) extra zero check.  */
+  sbitmap_zero (wont_exit);
+  if (extra_zero_check && desc->cond == NE)
+    SET_BIT (wont_exit, 1);
+  ezc_swtch = loop_preheader_edge (loop)->src;
+  if (!duplicate_loop_to_header_edge (loop, loop_preheader_edge (loop),
+		loops, 1,
+		wont_exit, desc->out_edge, remove_edges, &n_remove_edges,
+		DLTHE_FLAG_UPDATE_FREQ | DLTHE_USE_WONT_EXIT))
+    abort ();
+
+  /* Record the place where switch will be built for preconditioning.  */
+  swtch = loop_split_edge_with (loop_preheader_edge (loop),
+				    NULL_RTX, loops);
+
+  for (i = 0; i < n_peel; i++)
+    {
+      /* Peel the copy.  */
+      sbitmap_zero (wont_exit);
+      if (i != n_peel - 1 || !last_may_exit)
+	SET_BIT (wont_exit, 1);
+      if (!duplicate_loop_to_header_edge (loop, loop_preheader_edge (loop),
+		loops, 1,
+		wont_exit, desc->out_edge, remove_edges, &n_remove_edges,
+		DLTHE_FLAG_UPDATE_FREQ | DLTHE_USE_WONT_EXIT))
+    	abort ();
+
+      if (i != n_peel)
+	{
+	  /* Create item for switch.  */
+	  j = n_peel - i - (extra_zero_check ? 0 : 1);
+	  p = REG_BR_PROB_BASE / (i + 2);
+
+	  preheader = loop_split_edge_with (loop_preheader_edge (loop),
+					    NULL_RTX, loops);
+	  label = block_label (preheader);
+	  start_sequence ();
+	  do_compare_rtx_and_jump (copy_rtx (niter), GEN_INT (j), EQ, 0,
+		    		   GET_MODE (desc->var), NULL_RTX, NULL_RTX,
+				   label);
+	  jump = get_last_insn ();
+	  JUMP_LABEL (jump) = label;
+	  REG_NOTES (jump)
+		  = gen_rtx_EXPR_LIST (REG_BR_PROB,
+			    	       GEN_INT (p), REG_NOTES (jump));
+	
+	  LABEL_NUSES (label)++;
+	  branch_code = get_insns ();
+	  end_sequence ();
+
+	  swtch = loop_split_edge_with (swtch->pred, branch_code, loops);
+	  set_immediate_dominator (loops->cfg.dom, preheader, swtch);
+	  swtch->succ->probability = REG_BR_PROB_BASE - p;
+	  e = make_edge (swtch, preheader, 0);
+	  e->probability = p;
+	}
+    }
+
+  if (extra_zero_check)
+    {
+      /* Add branch for zero iterations.  */
+      p = REG_BR_PROB_BASE / (max_unroll + 1);
+      swtch = ezc_swtch;
+      preheader = loop_split_edge_with (loop_preheader_edge (loop),
+					NULL_RTX, loops);
+      label = block_label (preheader);
+      start_sequence ();
+      do_compare_rtx_and_jump (copy_rtx (niter), const0_rtx, EQ, 0,
+			       GET_MODE (desc->var), NULL_RTX, NULL_RTX,
+			       label);
+      jump = get_last_insn ();
+      JUMP_LABEL (jump) = label;
+      REG_NOTES (jump)
+	      = gen_rtx_EXPR_LIST (REG_BR_PROB,
+				   GEN_INT (p), REG_NOTES (jump));
+      
+      LABEL_NUSES (label)++;
+      branch_code = get_insns ();
+      end_sequence ();
+
+      swtch = loop_split_edge_with (swtch->succ, branch_code, loops);
+      set_immediate_dominator (loops->cfg.dom, preheader, swtch);
+      swtch->succ->probability = REG_BR_PROB_BASE - p;
+      e = make_edge (swtch, preheader, 0);
+      e->probability = p;
+    }
+
+  /* Recount dominators for outer blocks.  */
+  iterate_fix_dominators (loops->cfg.dom, dom_bbs, n_dom_bbs);
+
+  /* And unroll loop.  */
+
+  sbitmap_ones (wont_exit);
+  RESET_BIT (wont_exit, may_exit_copy);
+
+  if (!duplicate_loop_to_header_edge (loop, loop_latch_edge (loop),
+	 	loops, max_unroll,
+		wont_exit, desc->out_edge, remove_edges, &n_remove_edges,
+		DLTHE_FLAG_UPDATE_FREQ | DLTHE_USE_WONT_EXIT))
+    abort ();
+
+  free (wont_exit);
+
+  /* Remove the edges.  */
+  for (i = 0; i < n_remove_edges; i++)
+    remove_path (loops, remove_edges[i]);
+  free (remove_edges);
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file,
+	     ";; Unrolled loop %d times, counting # of iterations in runtime, %i insns\n",
+	     max_unroll, num_loop_insns (loop));
+}
+  
+/* Decide whether to simply peel loop and how much.  */
+static void
+decide_peel_simple (loops, loop, flags)
+     struct loops *loops;
+     struct loop *loop;
+     int flags;
+{
+  unsigned npeel;
+
+  if (!(flags & UAP_PEEL))
+    {
+      /* We were not asked to, just return back silently.  */
+      return;
+    }
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Considering simply peeling loop\n");
+
+  /* npeel = number of iterations to peel. */
+  npeel = PARAM_VALUE (PARAM_MAX_PEELED_INSNS) / loop->ninsns;
+  if (npeel > (unsigned) PARAM_VALUE (PARAM_MAX_PEEL_TIMES))
+    npeel = PARAM_VALUE (PARAM_MAX_PEEL_TIMES);
+
+  /* Skip big loops.  */
+  if (!npeel)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not considering loop, is too big\n");
+      return;
+    }
+
+  /* Check for simple loops.  */
+  if (!loop->has_desc)
+    loop->simple = simple_loop_p (loops, loop, &loop->desc);
+
+  /* Check number of iterations.  */
+  if (loop->simple && loop->desc.const_iter)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Loop iterates constant times\n");
+      return;
+    }
+
+  /* Do not simply peel loops with branches inside -- it increases number
+     of mispredicts.  */
+  if (loop->desc.n_branches > 1)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not peeling, contains branches\n");
+      return;
+    }
+
+  if (loop->header->count)
+    {
+      unsigned niter = expected_loop_iterations (loop);
+      if (niter + 1 > npeel)
+	{
+	  if (rtl_dump_file)
+	    {
+	      fprintf (rtl_dump_file, ";; Not peeling loop, rolls too much (");
+	      fprintf (rtl_dump_file, HOST_WIDEST_INT_PRINT_DEC, (HOST_WIDEST_INT) (niter + 1));
+	      fprintf (rtl_dump_file, " iterations > %d [maximum peelings])\n", npeel);
+	    }
+	  return;
+	}
+      npeel = niter + 1;
+    }
+  else
+    {
+      /* For now we have no good heuristics to decide whether loop peeling
+         will be effective, so disable it.  */
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file,
+		 ";; Not peeling loop, no evidence it will be profitable\n");
+      return;
+    }
+
+  /* Success.  */
+  loop->lpt_decision.decision = LPT_PEEL_SIMPLE;
+  loop->lpt_decision.times = npeel;
+}
+
+/* Peel a LOOP.  */
+static void
+peel_loop_simple (loops, loop)
+     struct loops *loops;
+     struct loop *loop;
+{
+  sbitmap wont_exit;
+  unsigned npeel = loop->lpt_decision.times;
+
+  wont_exit = sbitmap_alloc (npeel + 1);
+  sbitmap_zero (wont_exit);
+
+  if (!duplicate_loop_to_header_edge (loop, loop_preheader_edge (loop),
+		loops, npeel, wont_exit, NULL, NULL, NULL,
+		DLTHE_FLAG_UPDATE_FREQ | DLTHE_USE_WONT_EXIT))
+    abort ();
+  
+  free (wont_exit);
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Peeling loop %d times\n", npeel);
+}
+
+/* Decide whether to unroll loop stupidly and how much.  */
+static void
+decide_unroll_stupid (loops, loop, flags)
+     struct loops *loops;
+     struct loop *loop;
+     int flags;
+{
+  unsigned nunroll;
+
+  if (!(flags & UAP_UNROLL_ALL))
+    {
+      /* We were not asked to, just return back silently.  */
+      return;
+    }
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Considering unrolling loop stupidly\n");
+
+  /* nunroll = total number of copies of the original loop body in
+     unrolled loop (i.e. if it is 2, we have to duplicate loop body once.  */
+  nunroll = PARAM_VALUE (PARAM_MAX_UNROLLED_INSNS) / loop->ninsns;
+  if (nunroll > (unsigned) PARAM_VALUE (PARAM_MAX_UNROLL_TIMES))
+    nunroll = PARAM_VALUE (PARAM_MAX_UNROLL_TIMES);
+
+  /* Skip big loops.  */
+  if (nunroll <= 1)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not considering loop, is too big\n");
+      return;
+    }
+
+  /* Check for simple loops.  */
+  if (!loop->has_desc)
+    loop->simple = simple_loop_p (loops, loop, &loop->desc);
+
+  /* Check simpleness.  */
+  if (loop->simple)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; The loop is simple\n");
+      return;
+    }
+
+  /* Do not unroll loops with branches inside -- it increases number
+     of mispredicts.  */
+  if (loop->desc.n_branches > 1)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not unrolling, contains branches\n");
+      return;
+    }
+
+  /* If we have profile feedback, check whether the loop rolls.  */
+  if (loop->header->count && expected_loop_iterations (loop) < 2 * nunroll)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not unrolling loop, doesn't roll\n");
+      return;
+    }
+
+  loop->lpt_decision.decision = LPT_UNROLL_STUPID;
+  loop->lpt_decision.times = nunroll - 1;
+}
+
+/* Unroll a LOOP.  */
+static void
+unroll_loop_stupid (loops, loop)
+     struct loops *loops;
+     struct loop *loop;
+{
+  sbitmap wont_exit;
+  unsigned nunroll = loop->lpt_decision.times;
+
+  wont_exit = sbitmap_alloc (nunroll + 1);
+  sbitmap_zero (wont_exit);
+
+  if (!duplicate_loop_to_header_edge (loop, loop_latch_edge (loop),
+		loops, nunroll, wont_exit, NULL, NULL, NULL,
+		DLTHE_FLAG_UPDATE_FREQ | DLTHE_USE_WONT_EXIT))
+    abort ();
+
+  free (wont_exit);
+  
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Unrolled loop %d times, %i insns\n",
+	     nunroll, num_loop_insns (loop));
+}
--- gcc-3.3.1/gcc/loop-unswitch.c.hammer-branch	2003-08-05 18:22:47.000000000 +0200
+++ gcc-3.3.1/gcc/loop-unswitch.c	2003-08-05 18:22:47.000000000 +0200
@@ -0,0 +1,361 @@
+/* Loop unswitching for GNU compiler.
+   Copyright (C) 2002 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+#include "config.h"
+#include "system.h"
+#include "rtl.h"
+#include "hard-reg-set.h"
+#include "basic-block.h"
+#include "cfgloop.h"
+#include "cfglayout.h"
+#include "params.h"
+#include "output.h"
+#include "expr.h"
+
+static struct loop *unswitch_loop	PARAMS ((struct loops *,
+						struct loop *, basic_block));
+static void unswitch_single_loop	PARAMS ((struct loops *, struct loop *,
+						rtx, int));
+static bool may_unswitch_on_p		PARAMS ((struct loops *, basic_block,
+						struct loop *, basic_block *));
+static rtx reversed_condition		PARAMS ((rtx));
+
+/* Unswitch LOOPS.  */
+void
+unswitch_loops (loops)
+     struct loops *loops;
+{
+  int i, num;
+  struct loop *loop;
+
+  /* Go through inner loops (only original ones).  */
+  num = loops->num;
+  
+  for (i = 1; i < num; i++)
+    {
+      /* Removed loop?  */
+      loop = loops->parray[i];
+      if (!loop)
+	continue;
+      /* Not innermost?  */
+      if (loop->inner)
+	continue;
+
+      unswitch_single_loop (loops, loop, NULL_RTX, 0);
+#ifdef ENABLE_CHECKING
+      verify_dominators (loops->cfg.dom);
+      verify_loop_structure (loops);
+#endif
+    }
+}
+
+/* Checks whether we can unswitch LOOP on basic block BB.  LOOP BODY
+   is provided to save time.  */
+static bool
+may_unswitch_on_p (loops, bb, loop, body)
+     struct loops *loops;
+     basic_block bb;
+     struct loop *loop;
+     basic_block *body;
+{
+  rtx test;
+  unsigned i;
+
+  /* It must be a simple conditional jump.  */
+  if (!bb->succ || !bb->succ->succ_next || bb->succ->succ_next->succ_next)
+    return false;
+  if (!any_condjump_p (bb->end))
+    return false;
+
+  /* Are both branches inside loop?  */
+  if (!flow_bb_inside_loop_p (loop, bb->succ->dest)
+      || !flow_bb_inside_loop_p (loop, bb->succ->succ_next->dest))
+    return false;
+
+  /* It must be executed just once each iteration (because otherwise we
+     are unable to update dominator/irreducible loop information correctly).  */
+  if (!just_once_each_iteration_p (loops, loop, bb))
+    return false;
+
+  /* Condition must be invariant.  */
+  test = get_condition (bb->end, NULL);
+  if (!test)
+    return false;
+
+  for (i = 0; i < loop->num_nodes; i++)
+    if (modified_between_p (test, body[i]->head, NEXT_INSN (body[i]->end)))
+      return false;
+
+  return true;
+}
+
+/* Reverses CONDition; returns NULL if we cannot.  */
+static rtx
+reversed_condition (cond)
+     rtx cond;
+{
+  enum rtx_code reversed;
+  reversed = reversed_comparison_code (cond, NULL);
+  if (reversed == UNKNOWN)
+    return NULL_RTX;
+  else
+    return gen_rtx_fmt_ee (reversed,
+	 		     GET_MODE (cond), XEXP (cond, 0),
+			     XEXP (cond, 1));
+}
+
+/* Unswitch single LOOP.  COND_CHECKED holds list of conditions we already
+   unswitched on and are true in our branch.  NUM is number of unswitchings
+   done; do not allow it to grow too much, it is too easy to create example
+   on that the code would grow exponentially.  */
+static void
+unswitch_single_loop (loops, loop, cond_checked, num)
+     struct loops *loops;
+     struct loop *loop;
+     rtx cond_checked;
+     int num;
+{
+  basic_block *bbs, bb;
+  struct loop *nloop;
+  unsigned i;
+  int true_first;
+  rtx cond, rcond, conds, rconds, acond, split_before;
+  int always_true;
+  int always_false;
+  int repeat;
+  edge e;
+
+  /* Do not unswitch too much.  */
+  if (num > PARAM_VALUE (PARAM_MAX_UNSWITCH_LEVEL))
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not unswitching anymore, hit max level\n");
+      return;
+    }
+
+  /* We only unswitch innermost loops (at least for now).  */
+  if (loop->inner)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not unswitching, not innermost loop\n");
+      return;
+    }
+  
+  /* And we must be able to duplicate loop body.  */
+  if (!can_duplicate_loop_p (loop))
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not unswitching, can't duplicate loop\n");
+      return;
+    }
+
+  /* Check the size of loop.  */
+  if (num_loop_insns (loop) > PARAM_VALUE (PARAM_MAX_UNSWITCH_INSNS))
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not unswitching, loop too big\n");
+      return;
+    }
+  
+  /* Do not unswitch in cold areas.  */
+  if (!maybe_hot_bb_p (loop->header))
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not unswitching, not hot area\n");
+      return;
+    }
+  
+  /* Nor if it usually do not pass.  */
+  if (expected_loop_iterations (loop) < 1)
+    {
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, ";; Not unswitching, loop iterations < 1\n");
+      return;
+    }
+
+  do
+    {
+      repeat = 0;
+    
+      /* Find a bb to unswitch on.  We use just a stupid test of invariantness
+	 of the condition: all used regs must not be modified inside loop body.  */
+      bbs = get_loop_body (loop);
+      for (i = 0; i < loop->num_nodes; i++)
+	if (may_unswitch_on_p (loops, bbs[i], loop, bbs))
+	  break;
+
+      if (i == loop->num_nodes)
+	{
+	  free (bbs);
+	  return;
+	}
+
+      if (!(cond = get_condition (bbs[i]->end, &split_before)))
+	abort ();
+      rcond = reversed_condition (cond);
+      
+      /* Check whether the result can be predicted.  */
+      always_true = 0;
+      always_false = 0;
+      for (acond = cond_checked; acond; acond = XEXP (acond, 1))
+	{
+	  if (rtx_equal_p (cond, XEXP (acond, 0)))
+	    {
+	      /* True.  */
+	      always_true = 1;
+	      break;
+	    }
+	  if (rtx_equal_p (rcond, XEXP (acond, 0)))
+	    {
+	      /* False.  */
+	      always_false = 1;
+	      break;
+	    }
+	}
+
+      if (always_true)
+	{
+	  /* Remove false path.  */
+ 	  for (e = bbs[i]->succ; !(e->flags & EDGE_FALLTHRU); e = e->succ_next);
+	  remove_path (loops, e);
+	  free (bbs);
+	  repeat = 1;
+	}
+      else if (always_false)
+	{
+	  /* Remove true path.  */
+	  for (e = bbs[i]->succ; e->flags & EDGE_FALLTHRU; e = e->succ_next);
+	  remove_path (loops, e);
+	  free (bbs);
+	  repeat = 1;
+	}
+    } while (repeat);
+ 
+  /* We found the condition we can unswitch on.  */
+  conds = alloc_EXPR_LIST (0, cond, cond_checked);
+  if (rcond)
+    rconds = alloc_EXPR_LIST (0, rcond, cond_checked);
+  else
+    rconds = cond_checked;
+
+  /* Separate condition.  */
+  bb = split_loop_bb (loops, bbs[i], PREV_INSN (split_before))->dest;
+  free (bbs);
+  true_first = !(bb->succ->flags & EDGE_FALLTHRU);
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, ";; Unswitching loop\n");
+  /* Unswitch the loop.  */
+  nloop = unswitch_loop (loops, loop, bb);
+  if (!nloop)
+  abort ();
+
+  /* Invoke itself on modified loops.  */
+  unswitch_single_loop (loops, nloop, true_first ? conds : rconds, num + 1);
+  unswitch_single_loop (loops, loop, true_first ? rconds : conds, num + 1);
+
+  free_EXPR_LIST_node (conds);
+  if (rcond)
+    free_EXPR_LIST_node (rconds);
+}
+
+/* Unswitch a LOOP w.r. to given basic block.  We only support unswitching
+   of innermost loops (this is not a principial restriction, I'm just lazy
+   to handle subloops).  UNSWITCH_ON must be executed in every iteration,
+   i.e. it must dominate LOOP latch.  Returns NULL if impossible, new
+   loop otherwise.  */
+static struct loop *
+unswitch_loop (loops, loop, unswitch_on)
+     struct loops *loops;
+     struct loop *loop;
+     basic_block unswitch_on;
+{
+  edge entry, e, latch_edge;
+  basic_block switch_bb, unswitch_on_alt, src;
+  struct loop *nloop;
+  sbitmap zero_bitmap;
+  int irred_flag;
+
+  /* Some sanity checking.  */
+  if (!flow_bb_inside_loop_p (loop, unswitch_on))
+    abort ();
+  if (!unswitch_on->succ || !unswitch_on->succ->succ_next ||
+      unswitch_on->succ->succ_next->succ_next)
+    abort ();
+  if (!just_once_each_iteration_p (loops, loop, unswitch_on))
+    abort ();
+  if (loop->inner)
+    abort ();
+  if (!flow_bb_inside_loop_p (loop, unswitch_on->succ->dest))
+    abort ();
+  if (!flow_bb_inside_loop_p (loop, unswitch_on->succ->succ_next->dest))
+    abort ();
+  
+  /* Will we be able to perform redirection?  */
+  if (!any_condjump_p (unswitch_on->end))
+    return NULL;
+  if (!cfg_layout_can_duplicate_bb_p (unswitch_on))
+    return NULL;
+
+  entry = loop_preheader_edge (loop);
+  
+  /* Make a copy.  */
+  src = entry->src;
+  irred_flag = src->flags & BB_IRREDUCIBLE_LOOP;
+  src->flags &= ~BB_IRREDUCIBLE_LOOP;
+  zero_bitmap = sbitmap_alloc (2);
+  sbitmap_zero (zero_bitmap);
+  if (!duplicate_loop_to_header_edge (loop, entry, loops, 1,
+	zero_bitmap, NULL, NULL, NULL, 0))
+    return NULL;
+  free (zero_bitmap);
+  src->flags |= irred_flag;
+
+  /* Record switch block.  */
+  unswitch_on_alt = RBI (unswitch_on)->copy;
+
+  /* Make a copy of unswitched block.  */
+  switch_bb = cfg_layout_duplicate_bb (unswitch_on, NULL);
+  switch_bb->flags &= ~BB_IRREDUCIBLE_LOOP;
+  switch_bb->flags |= irred_flag;
+  add_to_dominance_info (loops->cfg.dom, switch_bb);
+  RBI (unswitch_on)->copy = unswitch_on_alt;
+
+  /* Loopify the copy.  */
+  for (latch_edge = RBI (loop->latch)->copy->succ;
+       latch_edge->dest != loop->header;
+       latch_edge = latch_edge->succ_next);
+  nloop = loopify (loops, latch_edge,
+		   RBI (loop->header)->copy->pred, switch_bb);
+
+  /* Remove paths from loop copies.  We rely on the fact that
+     cfg_layout_duplicate_bb reverses list of edges here.  */
+  for (e = unswitch_on->succ->succ_next->dest->pred; e; e = e->pred_next)
+    if (e->src != unswitch_on &&
+	!dominated_by_p (loops->cfg.dom, e->src, e->dest))
+      break;
+  remove_path (loops, unswitch_on->succ);
+  remove_path (loops, unswitch_on_alt->succ);
+
+  /* One of created loops do not have to be subloop of the outer loop now.  */
+  fix_loop_placement (loop);
+  fix_loop_placement (nloop);
+
+  return nloop;
+}
--- gcc-3.3.1/gcc/loop.c.hammer-branch	2003-07-16 14:37:54.000000000 +0200
+++ gcc-3.3.1/gcc/loop.c	2003-08-05 18:22:47.000000000 +0200
@@ -54,6 +54,7 @@ Software Foundation, 59 Temple Place - S
 #include "predict.h"
 #include "insn-flags.h"
 #include "optabs.h"
+#include "cfgloop.h"
 
 /* Not really meaningful values, but at least something.  */
 #ifndef SIMULTANEOUS_PREFETCHES
@@ -499,7 +500,7 @@ loop_optimize (f, dumpfile, flags)
 
   /* Allocate and initialize auxiliary loop information.  */
   loops_info = xcalloc (loops->num, sizeof (struct loop_info));
-  for (i = 0; i < loops->num; i++)
+  for (i = 0; i < (int) loops->num; i++)
     loops->array[i].aux = loops_info + i;
 
   /* Now find all register lifetimes.  This must be done after
@@ -5367,18 +5368,6 @@ strength_reduce (loop, flags)
     doloop_optimize (loop);
 #endif  /* HAVE_doloop_end  */
 
-  /* In case number of iterations is known, drop branch prediction note
-     in the branch.  Do that only in second loop pass, as loop unrolling
-     may change the number of iterations performed.  */
-  if (flags & LOOP_BCT)
-    {
-      unsigned HOST_WIDE_INT n
-	= loop_info->n_iterations / loop_info->unroll_number;
-      if (n > 1)
-	predict_insn (prev_nonnote_insn (loop->end), PRED_LOOP_ITERATIONS,
-		      REG_BR_PROB_BASE - REG_BR_PROB_BASE / n);
-    }
-
   if (loop_dump_stream)
     fprintf (loop_dump_stream, "\n");
 
--- gcc-3.3.1/gcc/optabs.c.hammer-branch	2003-07-19 11:32:43.000000000 +0200
+++ gcc-3.3.1/gcc/optabs.c	2003-08-05 18:22:47.000000000 +0200
@@ -4799,15 +4799,26 @@ expand_fix (to, from, unsignedp)
      one plus the highest signed number, convert, and add it back.
 
      We only need to check all real modes, since we know we didn't find
-     anything with a wider integer mode.  */
+     anything with a wider integer mode.  
+
+     This code used to extend FP value into mode wider than the destination.
+     This is not needed.  Consider, for instance conversion from SFmode
+     into DImode.
+
+     The hot path trought the code is dealing with inputs smaller than 2^63
+     and doing just the conversion, so there is no bits to lose.
+
+     In the other path we know the value is positive in the range 2^63..2^64-1
+     inclusive.  (as for other imput overflow happens and result is undefined)
+     So we know that the most important bit set in mantisa corresponds to
+     2^63.  The subtraction of 2^63 should not generate any rounding as it
+     simply clears out that bit.  The rest is trivial.  */
 
   if (unsignedp && GET_MODE_BITSIZE (GET_MODE (to)) <= HOST_BITS_PER_WIDE_INT)
     for (fmode = GET_MODE (from); fmode != VOIDmode;
 	 fmode = GET_MODE_WIDER_MODE (fmode))
-      /* Make sure we won't lose significant bits doing this.  */
-      if (GET_MODE_BITSIZE (fmode) > GET_MODE_BITSIZE (GET_MODE (to))
-	  && CODE_FOR_nothing != can_fix_p (GET_MODE (to), fmode, 0,
-					    &must_trunc))
+      if (CODE_FOR_nothing != can_fix_p (GET_MODE (to), fmode, 0,
+					 &must_trunc))
 	{
 	  int bitsize;
 	  REAL_VALUE_TYPE offset;
@@ -5195,6 +5206,11 @@ init_optabs ()
   one_cmpl_optab = init_optab (NOT);
   ffs_optab = init_optab (FFS);
   sqrt_optab = init_optab (SQRT);
+  floor_optab = init_optab (UNKNOWN);
+  ceil_optab = init_optab (UNKNOWN);
+  round_optab = init_optab (UNKNOWN);
+  trunc_optab = init_optab (UNKNOWN);
+  nearbyint_optab = init_optab (UNKNOWN);
   sin_optab = init_optab (UNKNOWN);
   cos_optab = init_optab (UNKNOWN);
   exp_optab = init_optab (UNKNOWN);
--- gcc-3.3.1/gcc/optabs.h.hammer-branch	2002-08-04 01:21:30.000000000 +0200
+++ gcc-3.3.1/gcc/optabs.h	2003-08-05 18:22:47.000000000 +0200
@@ -135,6 +135,12 @@ enum optab_index
   OTI_exp,
   /* Natural Logarithm */
   OTI_log,
+  /* Rounding functions */
+  OTI_floor,
+  OTI_ceil,
+  OTI_trunc,
+  OTI_round,
+  OTI_nearbyint,
 
   /* Compare insn; two operands.  */
   OTI_cmp,
@@ -204,6 +210,11 @@ extern GTY(()) optab optab_table[OTI_MAX
 #define cos_optab (optab_table[OTI_cos])
 #define exp_optab (optab_table[OTI_exp])
 #define log_optab (optab_table[OTI_log])
+#define floor_optab (optab_table[OTI_floor])
+#define ceil_optab (optab_table[OTI_ceil])
+#define trunc_optab (optab_table[OTI_trunc])
+#define round_optab (optab_table[OTI_round])
+#define nearbyint_optab (optab_table[OTI_nearbyint])
 
 #define cmp_optab (optab_table[OTI_cmp])
 #define ucmp_optab (optab_table[OTI_ucmp])
--- gcc-3.3.1/gcc/params.def.hammer-branch	2003-04-29 16:31:46.000000000 +0200
+++ gcc-3.3.1/gcc/params.def	2003-08-05 18:22:47.000000000 +0200
@@ -169,7 +169,47 @@ DEFPARAM(PARAM_MAX_GCSE_PASSES,
 DEFPARAM(PARAM_MAX_UNROLLED_INSNS,
 	 "max-unrolled-insns",
 	 "The maximum number of instructions to consider to unroll in a loop",
-	 100)
+	 50)
+/* The maximum number of unrollings of a single loop.  */
+DEFPARAM(PARAM_MAX_UNROLL_TIMES,
+	"max-unroll-times",
+	"The maximum number of unrollings of a single loop",
+	8)
+/* The maximum number of insns of a peeled loop.  */
+DEFPARAM(PARAM_MAX_PEELED_INSNS,
+	"max-peeled-insns",
+	"The maximum number of insns of a peeled loop",
+	120)
+/* The maximum number of peelings of a single loop.  */
+DEFPARAM(PARAM_MAX_PEEL_TIMES,
+	"max-peel-times",
+	"The maximum number of peelings of a single loop",
+	16)
+/* The maximum number of insns of a peeled loop.  */
+DEFPARAM(PARAM_MAX_COMPLETELY_PEELED_INSNS,
+	"max-completely-peeled-insns",
+	"The maximum number of insns of a completely peeled loop",
+	120)
+/* The maximum number of peelings of a single loop that is peeled completely.  */
+DEFPARAM(PARAM_MAX_COMPLETELY_PEEL_TIMES,
+	"max-completely-peel-times",
+	"The maximum number of peelings of a single loop that is peeled completely",
+	16)
+/* The maximum number of insns of a peeled loop that rolls only once.  */
+DEFPARAM(PARAM_MAX_ONCE_PEELED_INSNS,
+	"max-once-peeled-insns",
+	"The maximum number of insns of a peeled loop that rolls only once",
+	200)
+/* The maximum number of insns of an unswitched loop.  */
+DEFPARAM(PARAM_MAX_UNSWITCH_INSNS,
+	"max-unswitch-insns",
+	"The maximum number of insns of an unswitched loop",
+	50)
+/* The maximum level of recursion in unswitch_single_loop.  */
+DEFPARAM(PARAM_MAX_UNSWITCH_LEVEL,
+	"max-unswitch-level",
+	"The maximum number of unswitchings in a single loop",
+	3)
 
 DEFPARAM(HOT_BB_COUNT_FRACTION,
 	 "hot-bb-count-fraction",
@@ -204,7 +244,7 @@ DEFPARAM(TRACER_MIN_BRANCH_PROBABILITY_F
 	 "tracer-min-branch-probability-feedback",
 	 "Stop forward growth if the probability of best edge is less than \
 this threshold (in percents). Used when profile feedback is available",
-	 30)
+	 80)
 DEFPARAM(TRACER_MIN_BRANCH_PROBABILITY,
 	 "tracer-min-branch-probability",
 	 "Stop forward growth if the probability of best edge is less than \
--- gcc-3.3.1/gcc/predict.c.hammer-branch	2003-02-22 11:04:08.000000000 +0100
+++ gcc-3.3.1/gcc/predict.c	2003-08-05 18:22:47.000000000 +0200
@@ -50,6 +50,7 @@ Software Foundation, 59 Temple Place - S
 #include "params.h"
 #include "target.h"
 #include "loop.h"
+#include "cfgloop.h"
 
 /* real constants: 0, 1, 1-1/REG_BR_PROB_BASE, REG_BR_PROB_BASE,
 		   1/REG_BR_PROB_BASE, 0.5, BB_FREQ_MAX.  */
@@ -425,7 +426,7 @@ estimate_probability (loops_info)
 {
   dominance_info dominators, post_dominators;
   basic_block bb;
-  int i;
+  unsigned i;
 
   connect_infinite_loops_to_exit ();
   dominators = calculate_dominance_info (CDI_DOMINATORS);
@@ -436,13 +437,33 @@ estimate_probability (loops_info)
   for (i = 1; i < loops_info->num; i++)
     {
       basic_block bb, *bbs;
-      int j;
+      unsigned j;
       int exits;
       struct loop *loop = loops_info->parray[i];
+      struct loop_desc desc;
+      unsigned HOST_WIDE_INT niter;
 
       flow_loop_scan (loops_info, loop, LOOP_EXIT_EDGES);
       exits = loop->num_exits;
 
+      if (simple_loop_p (loops_info, loop, &desc)
+	  && desc.const_iter)
+	{
+	  int prob;
+	  niter = desc.niter + 1;
+	  if (niter == 0)        /* We might overflow here.  */
+	    niter = desc.niter;
+
+	  prob = (REG_BR_PROB_BASE
+		  - (REG_BR_PROB_BASE + niter /2) / niter);
+	  /* Branch prediction algorithm gives 0 frequency for everything
+	     after the end of loop for loop having 0 probability to finish.  */
+	  if (prob == REG_BR_PROB_BASE)
+	    prob = REG_BR_PROB_BASE - 1;
+	  predict_edge (desc.in_edge, PRED_LOOP_ITERATIONS,
+			prob);
+	}
+
       bbs = get_loop_body (loop);
       for (j = 0; j < loop->num_nodes; j++)
 	{
@@ -1060,7 +1081,7 @@ estimate_loops_at_level (first_loop)
     {
       edge e;
       basic_block *bbs;
-      int i;
+      unsigned i;
 
       estimate_loops_at_level (loop->inner);
       
@@ -1084,7 +1105,7 @@ estimate_loops_at_level (first_loop)
 static void
 counts_to_freqs ()
 {
-  HOST_WIDEST_INT count_max = 1;
+  gcov_type count_max = 1;
   basic_block bb;
 
   FOR_EACH_BB (bb)
--- gcc-3.3.1/gcc/profile.c.hammer-branch	2002-12-13 01:17:20.000000000 +0100
+++ gcc-3.3.1/gcc/profile.c	2003-08-05 18:22:47.000000000 +0200
@@ -39,31 +39,12 @@ Software Foundation, 59 Temple Place - S
    edges must be on the spanning tree. We also attempt to place
    EDGE_CRITICAL edges on the spanning tree.
 
-   The two auxiliary files generated are <dumpbase>.bb and
-   <dumpbase>.bbg. The former contains the BB->linenumber
-   mappings, and the latter describes the BB graph.
-
-   The BB file contains line numbers for each block. For each basic
-   block, a zero count is output (to mark the start of a block), then
-   the line numbers of that block are listed. A zero ends the file
-   too.
-
-   The BBG file contains a count of the blocks, followed by edge
-   information, for every edge in the graph. The edge information
-   lists the source and target block numbers, and a bit mask
-   describing the type of edge.
-
-   The BB and BBG file formats are fully described in the gcov
-   documentation.  */
+   The auxiliary file generated is <dumpbase>.bbg. The format is
+   described in full in gcov-io.h.  */
 
 /* ??? Register allocation should use basic block execution counts to
    give preference to the most commonly executed blocks.  */
 
-/* ??? The .da files are not safe.  Changing the program after creating .da
-   files or using different options when compiling with -fbranch-probabilities
-   can result the arc data not matching the program.  Maybe add instrumented
-   arc count to .bbg file?  Maybe check whether PFG matches the .bbg file?  */
-
 /* ??? Should calculate branch probabilities before instrumenting code, since
    then we can use arc counts to help decide which arcs to instrument.  */
 
@@ -81,11 +62,15 @@ Software Foundation, 59 Temple Place - S
 #include "ggc.h"
 #include "hard-reg-set.h"
 #include "basic-block.h"
+#include "cfgloop.h"
+#include "params.h"
 #include "gcov-io.h"
 #include "target.h"
 #include "profile.h"
 #include "libfuncs.h"
 #include "langhooks.h"
+#include "hashtab.h"
+#include "vpt.h"
 
 /* Additional information about the edges we need.  */
 struct edge_info {
@@ -107,6 +92,19 @@ struct bb_info {
   gcov_type pred_count;
 };
 
+struct function_list
+{
+  struct function_list *next; 	/* next function */
+  const char *name; 		/* function name */
+  unsigned cfg_checksum;	/* function checksum */
+  unsigned n_counter_sections;	/* number of counter sections */
+  struct counter_section counter_sections[MAX_COUNTER_SECTIONS];
+  				/* the sections */
+};
+
+static struct function_list *functions_head = 0;
+static struct function_list **functions_tail = &functions_head;
+
 #define EDGE_INFO(e)  ((struct edge_info *) (e)->aux)
 #define BB_INFO(b)  ((struct bb_info *) (b)->aux)
 
@@ -123,18 +121,24 @@ struct profile_info profile_info;
 /* Name and file pointer of the output file for the basic block graph.  */
 
 static FILE *bbg_file;
+static char *bbg_file_name;
 
 /* Name and file pointer of the input file for the arc count data.  */
 
 static FILE *da_file;
 static char *da_file_name;
 
-/* Pointer of the output file for the basic block/line number map.  */
-static FILE *bb_file;
+/* The name of the count table. Used by the edge profiling code.  */
+static GTY(()) rtx profiler_label;
+
+/* The name of the loop histograms table.  */
+static GTY(()) rtx loop_histograms_label;
 
-/* Last source file name written to bb_file.  */
+/* The name of the value histograms table.  */
+static GTY(()) rtx value_histograms_label;
 
-static char *last_bb_file_name;
+/* The name of the same value histograms table.  */
+static GTY(()) rtx same_value_histograms_label;
 
 /* Collect statistics on the performance of this pass for the entire source
    file.  */
@@ -152,19 +156,38 @@ static int total_num_branches;
 
 /* Forward declarations.  */
 static void find_spanning_tree PARAMS ((struct edge_list *));
-static void init_edge_profiler PARAMS ((void));
 static rtx gen_edge_profiler PARAMS ((int));
+static rtx gen_interval_profiler PARAMS ((struct histogram_value *, rtx, int));
+static rtx gen_range_profiler PARAMS ((struct histogram_value *, rtx, int));
+static rtx gen_pow2_profiler PARAMS ((struct histogram_value *, rtx, int));
+static rtx gen_one_value_profiler PARAMS ((struct histogram_value *, rtx, int));
 static void instrument_edges PARAMS ((struct edge_list *));
-static void output_gcov_string PARAMS ((const char *, long));
+static void instrument_loops PARAMS ((struct loops *));
+static void instrument_values PARAMS ((unsigned, struct histogram_value *));
 static void compute_branch_probabilities PARAMS ((void));
+static void compute_loop_histograms PARAMS ((struct loops *));
+static void compute_value_histograms PARAMS ((unsigned, struct histogram_value *));
+static hashval_t htab_counts_index_hash PARAMS ((const void *));
+static int htab_counts_index_eq PARAMS ((const void *, const void *));
+static void htab_counts_index_del PARAMS ((void *));
+static void cleanup_counts_index PARAMS ((int));
+static int index_counts_file PARAMS ((void));
 static gcov_type * get_exec_counts PARAMS ((void));
-static long compute_checksum PARAMS ((void));
+static gcov_type * get_histogram_counts PARAMS ((unsigned, unsigned));
+static unsigned compute_checksum PARAMS ((void));
 static basic_block find_group PARAMS ((basic_block));
 static void union_groups PARAMS ((basic_block, basic_block));
+static void set_purpose PARAMS ((tree, tree));
+static rtx label_for_tag PARAMS ((unsigned));
+static tree build_counter_section_fields PARAMS ((void));
+static tree build_counter_section_value PARAMS ((unsigned, unsigned));
+static tree build_counter_section_data_fields PARAMS ((void));
+static tree build_counter_section_data_value PARAMS ((unsigned, unsigned));
+static tree build_function_info_fields PARAMS ((void));
+static tree build_function_info_value PARAMS ((struct function_list *));
+static tree build_gcov_info_fields PARAMS ((tree));
+static tree build_gcov_info_value PARAMS ((void));
 
-/* If nonzero, we need to output a constructor to set up the
-   per-object-file data.  */
-static int need_func_profiler = 0;
 
 /* Add edge instrumentation code to the entire insn chain.
 
@@ -178,6 +201,7 @@ instrument_edges (el)
   int num_instr_edges = 0;
   int num_edges = NUM_EDGES (el);
   basic_block bb;
+  struct section_info *section_info;
   remove_fake_edges ();
 
   FOR_BB_BETWEEN (bb, ENTRY_BLOCK_PTR, NULL, next_bb)
@@ -194,7 +218,6 @@ instrument_edges (el)
 		fprintf (rtl_dump_file, "Edge %d to %d instrumented%s\n",
 			 e->src->index, e->dest->index,
 			 EDGE_CRITICAL_P (e) ? " (and split)" : "");
-	      need_func_profiler = 1;
 	      insert_insn_on_edge (
 			 gen_edge_profiler (total_num_edges_instrumented
 					    + num_instr_edges++), e);
@@ -203,48 +226,374 @@ instrument_edges (el)
 	}
     }
 
-  profile_info.count_edges_instrumented_now = num_instr_edges;
+  section_info = find_counters_section (GCOV_TAG_ARC_COUNTS);
+  section_info->n_counters_now = num_instr_edges;
   total_num_edges_instrumented += num_instr_edges;
-  profile_info.count_instrumented_edges = total_num_edges_instrumented;
+  section_info->n_counters = total_num_edges_instrumented;
 
   total_num_blocks_created += num_edges;
   if (rtl_dump_file)
     fprintf (rtl_dump_file, "%d edges instrumented\n", num_instr_edges);
+}
+
+/* Add code that counts histograms of first iterations of LOOPS.  */
+static void
+instrument_loops (loops)
+     struct loops *loops;
+{
+  enum machine_mode mode = mode_for_size (GCOV_TYPE_SIZE, MODE_INT, 0);
+  rtx *loop_counters;
+  rtx sequence;
+  unsigned i, histogram_steps;
+  basic_block bb;
+  edge e;
+  int n_histogram_counters;
+  struct section_info *section_info;
+  
+  histogram_steps = PARAM_VALUE (PARAM_MAX_PEEL_TIMES);
+  if (histogram_steps < (unsigned) PARAM_VALUE (PARAM_MAX_UNROLL_TIMES))
+    histogram_steps = PARAM_VALUE (PARAM_MAX_UNROLL_TIMES);
+
+  loop_counters = xmalloc (sizeof (rtx) * loops->num);
+  for (i = 1; i < loops->num; i++)
+    loop_counters[i] = gen_reg_rtx (mode);
+
+  section_info = find_counters_section (GCOV_TAG_LOOP_HISTOGRAMS);
+  /* First the easy part -- code to initalize counter on preheader edge &
+     to increase it on latch one.  */
+  for (i = 1; i < loops->num; i++)
+    {
+      rtx tmp;
+
+      start_sequence ();
+      emit_move_insn (loop_counters[i], const0_rtx);
+      sequence = get_insns ();
+      end_sequence ();
+      insert_insn_on_edge (sequence, loop_preheader_edge (loops->parray[i]));
+
+      start_sequence ();
+      tmp = expand_simple_binop (mode, PLUS, loop_counters[i], const1_rtx,
+				 loop_counters[i], 0, OPTAB_WIDEN);
+      if (tmp != loop_counters[i])
+	emit_move_insn (loop_counters[i], tmp);
+      sequence = get_insns ();
+      end_sequence ();
+      insert_insn_on_edge (sequence, loop_latch_edge (loops->parray[i]));
+    }
+ 
+  /* And now emit code to generate the histogram on exit edges. The trouble
+     is that there may be more than one edge leaving the loop and the single
+     edge may exit multiple loops.  The other problem is that the exit edge
+     may be abnormal & critical; in this case we just ignore it.  */
+
+  FOR_EACH_BB (bb)
+    {
+      for (e = bb->succ; e; e = e->succ_next)
+	{
+	  struct loop *src_loop, *dest_loop, *loop;
+
+	  if ((e->flags & EDGE_ABNORMAL) && EDGE_CRITICAL_P (e))
+	    continue;
+
+	  src_loop = e->src->loop_father;
+	  dest_loop = find_common_loop (src_loop, e->dest->loop_father);
+
+	  for (loop = src_loop; loop != dest_loop; loop = loop->outer)
+	    {
+	      struct histogram_value cdesc;
+
+	      cdesc.value = loop_counters[loop->num];
+	      cdesc.mode = mode;
+	      cdesc.seq = NULL_RTX;
+	      cdesc.hdata.intvl.int_start = 0;
+	      cdesc.hdata.intvl.steps = histogram_steps;
+	      cdesc.hdata.intvl.may_be_less = 0;
+	      cdesc.hdata.intvl.may_be_more = 1;
+	      insert_insn_on_edge (
+			gen_interval_profiler (&cdesc, loop_histograms_label,
+			section_info->n_counters
+				+ (loop->num - 1) * (histogram_steps + 1)),
+			e);
+	    }
+	}
+    }
+  free (loop_counters);
+
+  n_histogram_counters = (loops->num - 1) * (histogram_steps + 1);
+  section_info->n_counters_now = n_histogram_counters;
+  section_info->n_counters += n_histogram_counters;
+}
 
-  commit_edge_insertions_watch_calls ();
+/* Add code to measure histograms of VALUES.  */
+static void
+instrument_values (n_values, values)
+     unsigned n_values;
+     struct histogram_value *values;
+{
+  rtx sequence;
+  unsigned i;
+  edge e;
+  int n_histogram_counters = 0, n_sv_histogram_counters = 0;
+  struct section_info *section_info, *sv_section_info;
+ 
+  sv_section_info = find_counters_section (GCOV_TAG_SAME_VALUE_HISTOGRAMS);
+  section_info = find_counters_section (GCOV_TAG_VALUE_HISTOGRAMS);
+
+  /* Emit code to generate the histograms before the insns.  */
+
+  for (i = 0; i < n_values; i++)
+    {
+      e = split_block (BLOCK_FOR_INSN (values[i].insn),
+		       PREV_INSN (values[i].insn));
+
+      switch (values[i].type)
+	{
+	case HIST_TYPE_INTERVAL:
+	  sequence = 
+	      gen_interval_profiler (values + i, value_histograms_label,
+			section_info->n_counters + n_histogram_counters);
+	  n_histogram_counters += values[i].n_counters;
+	  break;
+
+	case HIST_TYPE_RANGE:
+	  sequence = 
+	      gen_range_profiler (values + i, value_histograms_label,
+			section_info->n_counters + n_histogram_counters);
+	  n_histogram_counters += values[i].n_counters;
+	  break;
+
+	case HIST_TYPE_POW2:
+	  sequence = 
+	      gen_pow2_profiler (values + i, value_histograms_label,
+			section_info->n_counters + n_histogram_counters);
+	  n_histogram_counters += values[i].n_counters;
+	  break;
+
+	case HIST_TYPE_ONE_VALUE:
+	  sequence = 
+	      gen_one_value_profiler (values + i, same_value_histograms_label,
+			sv_section_info->n_counters + n_sv_histogram_counters);
+	  n_sv_histogram_counters += values[i].n_counters;
+	  break;
+
+	default:
+	  abort ();
+	}
+
+      insert_insn_on_edge (sequence, e);
+    }
+
+  section_info->n_counters_now = n_histogram_counters;
+  section_info->n_counters += n_histogram_counters;
+  sv_section_info->n_counters_now = n_sv_histogram_counters;
+  sv_section_info->n_counters += n_sv_histogram_counters;
+}
+
+struct section_reference
+{
+  long offset;
+  int owns_summary;
+  long *summary;
+};
+
+struct da_index_entry
+{
+  /* We hash by  */
+  char *function_name;
+  unsigned section;
+  /* and store  */
+  unsigned checksum;
+  unsigned n_offsets;
+  struct section_reference *offsets;
+};
+
+static hashval_t
+htab_counts_index_hash (of)
+     const void *of;
+{
+  const struct da_index_entry *entry = of;
+
+  return htab_hash_string (entry->function_name) ^ entry->section;
 }
 
-/* Output STRING to bb_file, surrounded by DELIMITER.  */
+static int
+htab_counts_index_eq (of1, of2)
+     const void *of1;
+     const void *of2;
+{
+  const struct da_index_entry *entry1 = of1;
+  const struct da_index_entry *entry2 = of2;
+
+  return !strcmp (entry1->function_name, entry2->function_name)
+	  && entry1->section == entry2->section;
+}
 
 static void
-output_gcov_string (string, delimiter)
-     const char *string;
-     long delimiter;
+htab_counts_index_del (what)
+     void *what;
 {
-  size_t temp;
+  struct da_index_entry *entry = what;
+  unsigned i;
 
-  /* Write a delimiter to indicate that a file name follows.  */
-  __write_long (delimiter, bb_file, 4);
+  for (i = 0; i < entry->n_offsets; i++)
+    {
+      struct section_reference *act = entry->offsets + i;
+      if (act->owns_summary)
+	free (act->summary);
+    }
+  free (entry->function_name);
+  free (entry->offsets);
+  free (entry);
+}
 
-  /* Write the string.  */
-  temp = strlen (string) + 1;
-  fwrite (string, temp, 1, bb_file);
+static char *counts_file_name;
+static htab_t counts_file_index = NULL;
 
-  /* Append a few zeros, to align the output to a 4 byte boundary.  */
-  temp = temp & 0x3;
-  if (temp)
+static void
+cleanup_counts_index (close_file)
+     int close_file;
+{
+  if (da_file && close_file)
     {
-      char c[4];
+      fclose (da_file);
+      da_file = NULL;
+    }
+  if (counts_file_name)
+    free (counts_file_name);
+  counts_file_name = NULL;
+  if (counts_file_index)
+    htab_delete (counts_file_index);
+  counts_file_index = NULL;
+}
+
+static int
+index_counts_file ()
+{
+  char *function_name_buffer = NULL;
+  unsigned magic, version, ix, checksum;
+  long *summary;
+
+  if (!da_file)
+    return 0;
+  counts_file_index = htab_create (10, htab_counts_index_hash, htab_counts_index_eq, htab_counts_index_del);
+
+  /* No .da file, no data.  */
+  if (!da_file)
+    return 0;
+
+  /* Now index all profile sections.  */
+
+  rewind (da_file);
+
+  summary = NULL;
 
-      c[0] = c[1] = c[2] = c[3] = 0;
-      fwrite (c, sizeof (char), 4 - temp, bb_file);
+  if (gcov_read_unsigned (da_file, &magic) || magic != GCOV_DATA_MAGIC)
+    {
+      warning ("`%s' is not a gcov data file", da_file_name);
+      goto cleanup;
+    }
+  if (gcov_read_unsigned (da_file, &version) || version != GCOV_VERSION)
+    {
+      char v[4], e[4];
+      magic = GCOV_VERSION;
+      
+      for (ix = 4; ix--; magic >>= 8, version >>= 8)
+	{
+	  v[ix] = version;
+	  e[ix] = magic;
+	}
+      warning ("`%s' is version `%.4s', expected version `%.4s'",
+	       da_file_name, v, e);
+      goto cleanup;
+    }
+  
+  while (1)
+    {
+      unsigned tag, length;
+      long offset;
+      
+      offset = gcov_save_position (da_file);
+      if (gcov_read_unsigned (da_file, &tag)
+	  || gcov_read_unsigned (da_file, &length))
+	{
+	  if (feof (da_file))
+	    break;
+	corrupt:;
+	  warning ("`%s' is corrupted", da_file_name);
+	  goto cleanup;
+	}
+      if (tag == GCOV_TAG_FUNCTION)
+	{
+	  if (gcov_read_string (da_file, &function_name_buffer, NULL)
+	      || gcov_read_unsigned (da_file, &checksum))
+	    goto corrupt;
+	  continue;
+	}
+      if (tag == GCOV_TAG_PROGRAM_SUMMARY)
+	{
+	  if (length != GCOV_SUMMARY_LENGTH)
+	    goto corrupt;
+
+	  if (summary)
+	    *summary = offset;
+	  summary = NULL;
+	}
+      else
+	{
+	  if (function_name_buffer)
+	    {
+	      struct da_index_entry **slot, elt;
+	      elt.function_name = function_name_buffer;
+	      elt.section = tag;
+
+	      slot = (struct da_index_entry **)
+		htab_find_slot (counts_file_index, &elt, INSERT);
+	      if (*slot)
+		{
+		  if ((*slot)->checksum != checksum)
+		    {
+		      warning ("profile mismatch for `%s'", function_name_buffer);
+		      goto cleanup;
+		    }
+		  (*slot)->n_offsets++;
+		  (*slot)->offsets = xrealloc ((*slot)->offsets,
+					       sizeof (struct section_reference) * (*slot)->n_offsets);
+		}
+	      else
+		{
+		  *slot = xmalloc (sizeof (struct da_index_entry));
+		  (*slot)->function_name = xstrdup (function_name_buffer);
+		  (*slot)->section = tag;
+		  (*slot)->checksum = checksum;
+		  (*slot)->n_offsets = 1;
+		  (*slot)->offsets = xmalloc (sizeof (struct section_reference));
+		}
+	      (*slot)->offsets[(*slot)->n_offsets - 1].offset = offset;
+	      if (summary)
+		(*slot)->offsets[(*slot)->n_offsets - 1].owns_summary = 0;
+	      else
+		{
+		  summary = xmalloc (sizeof (long));
+		  *summary = -1;
+		  (*slot)->offsets[(*slot)->n_offsets - 1].owns_summary = 1;
+		}
+	      (*slot)->offsets[(*slot)->n_offsets - 1].summary = summary;
+	    }
+	}
+      if (gcov_skip (da_file, length))
+	goto corrupt;
     }
 
-  /* Store another delimiter in the .bb file, just to make it easy to find
-     the end of the file name.  */
-  __write_long (delimiter, bb_file, 4);
+  free (function_name_buffer);
+
+  return 1;
+
+cleanup:
+  cleanup_counts_index (1);
+  if (function_name_buffer)
+    free (function_name_buffer);
+  return 0;
 }
-
 
 /* Computes hybrid profile for all matching entries in da_file.
    Sets max_counter_in_program as a side effect.  */
@@ -252,23 +601,25 @@ output_gcov_string (string, delimiter)
 static gcov_type *
 get_exec_counts ()
 {
-  int num_edges = 0;
+  unsigned num_edges = 0;
   basic_block bb;
-  int okay = 1, i;
-  int mismatch = 0;
   gcov_type *profile;
-  char *function_name_buffer;
-  int function_name_buffer_len;
-  gcov_type max_counter_in_run;
-  const char *name = IDENTIFIER_POINTER
-		      (DECL_ASSEMBLER_NAME (current_function_decl));
+  gcov_type max_count;
+  unsigned ix, i, tag, length, num;
+  const char *name = IDENTIFIER_POINTER (DECL_ASSEMBLER_NAME (current_function_decl));
+  struct da_index_entry *entry, what;
+  struct section_reference *act;
+  gcov_type count;
+  struct gcov_summary summ;
 
   profile_info.max_counter_in_program = 0;
   profile_info.count_profiles_merged = 0;
 
   /* No .da file, no execution counts.  */
   if (!da_file)
-    return 0;
+    return NULL;
+  if (!counts_file_index)
+    abort ();
 
   /* Count the edges to be (possibly) instrumented.  */
 
@@ -283,134 +634,289 @@ get_exec_counts ()
   /* now read and combine all matching profiles.  */
 
   profile = xmalloc (sizeof (gcov_type) * num_edges);
-  rewind (da_file);
-  function_name_buffer_len = strlen (name) + 1;
-  function_name_buffer = xmalloc (function_name_buffer_len + 1);
 
-  for (i = 0; i < num_edges; i++)
-    profile[i] = 0;
+  for (ix = 0; ix < num_edges; ix++)
+    profile[ix] = 0;
 
-  while (1)
+  what.function_name = (char *) name;
+  what.section = GCOV_TAG_ARC_COUNTS;
+  entry = htab_find (counts_file_index, &what);
+  if (!entry)
+    {
+      warning ("No profile for function '%s' found.", name);
+      goto cleanup;
+    }
+  
+  if (entry->checksum != profile_info.current_function_cfg_checksum)
+    {
+      warning ("profile mismatch for `%s'", current_function_name);
+      goto cleanup;
+    }
+
+  for (i = 0; i < entry->n_offsets; i++)
     {
-      long magic, extra_bytes;
-      long func_count;
-      int i;
+      act = entry->offsets + i;
 
-      if (__read_long (&magic, da_file, 4) != 0)
-	break;
+      /* Read arc counters.  */
+      max_count = 0;
+      gcov_resync (da_file, act->offset, 0);
 
-      if (magic != -123)
+      if (gcov_read_unsigned (da_file, &tag)
+	  || gcov_read_unsigned (da_file, &length)
+	  || tag != GCOV_TAG_ARC_COUNTS)
 	{
-	  okay = 0;
-	  break;
+	  /* We have already passed through file, so any error means
+	     something is rotten.  */
+	  abort ();
 	}
+      num = length / 8;
 
-      if (__read_long (&func_count, da_file, 4) != 0)
+      if (num != num_edges)
 	{
-	  okay = 0;
-	  break;
+	  warning ("profile mismatch for `%s'", current_function_name);
+	  goto cleanup;
+	}
+	  
+      for (ix = 0; ix != num; ix++)
+	{
+	  if (gcov_read_counter (da_file, &count, false))
+	    abort ();
+	  if (count > max_count)
+	    max_count = count;
+	  profile[ix] += count;
 	}
 
-      if (__read_long (&extra_bytes, da_file, 4) != 0)
+      /* Read program summary.  */
+      if (*act->summary != -1)
 	{
-	  okay = 0;
-	  break;
+	  gcov_resync (da_file, *act->summary, 0);
+	  if (gcov_read_unsigned (da_file, &tag)
+	      || gcov_read_unsigned (da_file, &length)
+	      || tag != GCOV_TAG_PROGRAM_SUMMARY
+	      || gcov_read_summary (da_file, &summ))
+	    abort ();
+	  profile_info.count_profiles_merged += summ.runs;
+	  profile_info.max_counter_in_program += summ.arc_sum_max;
+	}
+      else
+	summ.runs = 0;
+      if (!summ.runs)
+	{
+	  profile_info.count_profiles_merged++;
+	  profile_info.max_counter_in_program += max_count;
 	}
+    }
 
-      fseek (da_file, 4 + 8, SEEK_CUR);
+  if (rtl_dump_file)
+    {
+      fprintf(rtl_dump_file, "Merged %i profiles with maximal count %i.\n",
+	      profile_info.count_profiles_merged,
+	      (int)profile_info.max_counter_in_program);
+    }
 
-      /* read the maximal counter.  */
-      __read_gcov_type (&max_counter_in_run, da_file, 8);
+  return profile;
 
-      /* skip the rest of "statistics" emited by __bb_exit_func.  */
-      fseek (da_file, extra_bytes - (4 + 8 + 8), SEEK_CUR);
+cleanup:;
+  free (profile);
+  cleanup_counts_index (1);
+  return 0;
+}
 
-      for (i = 0; i < func_count; i++)
-	{
-	  long arc_count;
-	  long chksum;
-	  int j;
+/* Get histogram counters.  */
+static gcov_type *
+get_histogram_counts (section_tag, n_counters)
+     unsigned section_tag;
+     unsigned n_counters;
+{
+  gcov_type *profile;
+  unsigned ix, i, tag, length, num;
+  const char *name = IDENTIFIER_POINTER (DECL_ASSEMBLER_NAME (current_function_decl));
+  struct da_index_entry *entry, what;
+  struct section_reference *act;
+  gcov_type count;
+  merger_function merger;
 
-	  if (__read_gcov_string
-	      (function_name_buffer, function_name_buffer_len, da_file,
-	       -1) != 0)
-	    {
-	      okay = 0;
-	      break;
-	    }
+  /* No .da file, no execution counts.  */
+  if (!da_file)
+    return NULL;
+  if (!counts_file_index)
+    abort ();
 
-	  if (__read_long (&chksum, da_file, 4) != 0)
-	    {
-	      okay = 0;
-	      break;
-	    }
+  /* No counters to read.  */
+  if (!n_counters)
+    return NULL;
+
+  /* now read and combine all matching profiles.  */
+
+  profile = xmalloc (sizeof (gcov_type) * n_counters);
+
+  for (ix = 0; ix < n_counters; ix++)
+    profile[ix] = 0;
+
+  what.function_name = (char *) name;
+  what.section = section_tag;
+  entry = htab_find (counts_file_index, &what);
+  if (!entry)
+    {
+      warning ("No profile for function '%s' found.", name);
+      goto cleanup;
+    }
+  
+  if (entry->checksum != profile_info.current_function_cfg_checksum)
+    {
+      warning ("profile mismatch for `%s'", current_function_name);
+      goto cleanup;
+    }
+
+  for (i = 0; i < entry->n_offsets; i++)
+    {
+      act = entry->offsets + i;
+
+      /* Read arc counters.  */
+      gcov_resync (da_file, act->offset, 0);
+
+      if (gcov_read_unsigned (da_file, &tag)
+	  || gcov_read_unsigned (da_file, &length)
+	  || tag != section_tag)
+	{
+	  /* We have already passed through file, so any error means
+	     something is rotten.  */
+	  abort ();
+	}
+      num = length / 8;
+
+      if (num != n_counters)
+	{
+	  warning ("profile mismatch for `%s'", current_function_name);
+	  goto cleanup;
+	}
 
-	  if (__read_long (&arc_count, da_file, 4) != 0)
+      if ((merger = profile_merger_for_tag (tag)))
+	{
+	  if ((*merger) (da_file, profile, n_counters))
 	    {
-	      okay = 0;
-	      break;
+	      warning ("profile mismatch for `%s'", current_function_name);
+	      goto cleanup;
 	    }
-
-	  if (strcmp (function_name_buffer, name) != 0)
+	}
+      else
+	{
+	  for (ix = 0; ix != num; ix++)
 	    {
-	      /* skip */
-	      if (fseek (da_file, arc_count * 8, SEEK_CUR) < 0)
+	      if (gcov_read_counter (da_file, &count, false))
 		{
-		  okay = 0;
-		  break;
+		  warning ("profile mismatch for `%s'", current_function_name);
+		  goto cleanup;
 		}
+	      profile[ix] += count;
 	    }
-	  else if (arc_count != num_edges
-		   || chksum != profile_info.current_function_cfg_checksum)
-	    okay = 0, mismatch = 1;
-	  else
-	    {
-	      gcov_type tmp;
+	}
+    }
 
-	      profile_info.max_counter_in_program += max_counter_in_run;
-	      profile_info.count_profiles_merged++;
+  return profile;
 
-	      for (j = 0; j < arc_count; j++)
-		if (__read_gcov_type (&tmp, da_file, 8) != 0)
-		  {
-		    okay = 0;
-		    break;
-		  }
-		else
-		  {
-		    profile[j] += tmp;
-		  }
-	    }
-	}
+cleanup:;
+  free (profile);
+  cleanup_counts_index (1);
+  return 0;
+}
+
+/* Load loop histograms from .da file.  */
+static void
+compute_loop_histograms (loops)
+     struct loops *loops;
+{
+  unsigned histogram_steps;
+  unsigned i;
+  gcov_type *histogram_counts, *act_count;
+ 
+  histogram_steps = PARAM_VALUE (PARAM_MAX_PEEL_TIMES);
+  if (histogram_steps < (unsigned) PARAM_VALUE (PARAM_MAX_UNROLL_TIMES))
+    histogram_steps = PARAM_VALUE (PARAM_MAX_UNROLL_TIMES);
+
+  histogram_counts = get_histogram_counts (GCOV_TAG_LOOP_HISTOGRAMS,
+					   (loops->num - 1) * (histogram_steps + 1));
+  if (!histogram_counts)
+    return;
 
-      if (!okay)
-	break;
+  act_count = histogram_counts;
+  for (i = 1; i < loops->num; i++)
+    {
+      edge latch = loop_latch_edge (loops->parray[i]);
 
+      latch->loop_histogram = xmalloc (sizeof (struct loop_histogram));
+      latch->loop_histogram->steps = histogram_steps;
+      latch->loop_histogram->counts = xmalloc (histogram_steps * sizeof (gcov_type));
+      memcpy (latch->loop_histogram->counts, act_count,
+	      histogram_steps * sizeof (gcov_type));
+      latch->loop_histogram->more = act_count[histogram_steps];
+      act_count += histogram_steps + 1;
     }
 
-  free (function_name_buffer);
+  free (histogram_counts);
+  
+  find_counters_section (GCOV_TAG_LOOP_HISTOGRAMS)->present = 1;
+}
 
-  if (!okay)
+/* Load value histograms from .da file.  */
+static void
+compute_value_histograms (n_values, values)
+     unsigned n_values;
+     struct histogram_value *values;
+{
+  unsigned i, j, n_histogram_counters, n_sv_histogram_counters;
+  gcov_type *histogram_counts, *act_count;
+  gcov_type *sv_histogram_counts, *sv_act_count;
+  gcov_type *aact_count;
+  
+  n_histogram_counters = 0;
+  n_sv_histogram_counters = 0;
+  for (i = 0; i < n_values; i++)
     {
-      if (mismatch)
-	error
-	  ("Profile does not match flowgraph of function %s (out of date?)",
-	   current_function_name);
+      if (values[i].type == HIST_TYPE_ONE_VALUE)
+	n_sv_histogram_counters += values[i].n_counters;
       else
-	error (".da file corrupted");
-      free (profile);
-      return 0;
+	n_histogram_counters += values[i].n_counters;
     }
-  if (rtl_dump_file)
+
+  histogram_counts = get_histogram_counts (GCOV_TAG_VALUE_HISTOGRAMS,
+					   n_histogram_counters);
+  sv_histogram_counts = get_histogram_counts (GCOV_TAG_SAME_VALUE_HISTOGRAMS,
+   					      n_sv_histogram_counters);
+  if (!histogram_counts && !sv_histogram_counts)
+    return;
+
+  act_count = histogram_counts;
+  sv_act_count = sv_histogram_counts;
+  for (i = 0; i < n_values; i++)
     {
-      fprintf(rtl_dump_file, "Merged %i profiles with maximal count %i.\n",
-	      profile_info.count_profiles_merged,
-	      (int)profile_info.max_counter_in_program);
-    }
+      rtx hist_list = NULL_RTX;
 
-  return profile;
+      
+      if (values[i].type == HIST_TYPE_ONE_VALUE)
+	{
+	  aact_count = sv_act_count;
+	  sv_act_count += values[i].n_counters;
+	}
+      else
+	{
+	  aact_count = act_count;
+	  act_count += values[i].n_counters;
+	}
+      for (j = values[i].n_counters; j > 0; j--)
+	hist_list = alloc_EXPR_LIST (0, GEN_INT (aact_count[j - 1]), hist_list);
+      hist_list = alloc_EXPR_LIST (0, copy_rtx (values[i].value), hist_list);
+      hist_list = alloc_EXPR_LIST (0, GEN_INT (values[i].type), hist_list);
+      REG_NOTES (values[i].insn) =
+	      alloc_EXPR_LIST (REG_VALUE_HISTOGRAM, hist_list,
+			       REG_NOTES (values[i].insn));
+    }
+
+  free (histogram_counts);
+  free (sv_histogram_counts);
+  find_counters_section (GCOV_TAG_VALUE_HISTOGRAMS)->present = 1;
+  find_counters_section (GCOV_TAG_SAME_VALUE_HISTOGRAMS)->present = 1;
 }
-
 
 /* Compute the branch probabilities for the various branches.
    Annotate them accordingly.  */
@@ -628,11 +1134,32 @@ compute_branch_probabilities ()
 	{
 	  for (e = bb->succ; e; e = e->succ_next)
 	    {
+		/* Function may return twice in the cased the called fucntion is
+		   setjmp or calls fork, but we can't represent this by extra
+		   edge from the entry, since extra edge from the exit is
+		   already present.  We get negative frequency from the entry
+		   point.  */
+		if ((e->count < 0
+		     && e->dest == EXIT_BLOCK_PTR)
+		    || (e->count > total
+		        && e->dest != EXIT_BLOCK_PTR))
+		  {
+		    rtx insn = bb->end;
+
+		    while (GET_CODE (insn) != CALL_INSN
+			   && insn != bb->head
+			   && keep_with_call_p (insn))
+		      insn = PREV_INSN (insn);
+		    if (GET_CODE (insn) == CALL_INSN)
+		      e->count = e->count < 0 ? 0 : total;
+		  }
+
 		e->probability = (e->count * REG_BR_PROB_BASE + total / 2) / total;
 		if (e->probability < 0 || e->probability > REG_BR_PROB_BASE)
 		  {
-		    error ("corrupted profile info: prob for %d-%d thought to be %d",
-			   e->src->index, e->dest->index, e->probability);
+		    error ("corrupted profile info: prob for %d-%d thought to be %f",
+			   e->src->index, e->dest->index,
+			   e->probability/(double) REG_BR_PROB_BASE);
 		    e->probability = REG_BR_PROB_BASE / 2;
 		  }
 	    }
@@ -669,10 +1196,11 @@ compute_branch_probabilities ()
 	      num_branches++;
 	    }
 	}
-      /* Otherwise distribute the probabilities evenly so we get sane sum.
-	 Use simple heuristics that if there are normal edges, give all abnormals
-	 frequency of 0, otherwise distribute the frequency over abnormals
-	 (this is the case of noreturn calls).  */
+      /* Otherwise distribute the probabilities evenly so we get sane
+	 sum.  Use simple heuristics that if there are normal edges,
+	 give all abnormals frequency of 0, otherwise distribute the
+	 frequency over abnormals (this is the case of noreturn
+	 calls).  */
       else
 	{
 	  for (e = bb->succ; e; e = e->succ_next)
@@ -723,32 +1251,46 @@ compute_branch_probabilities ()
   free_aux_for_blocks ();
   if (exec_counts)
     free (exec_counts);
+  find_counters_section (GCOV_TAG_ARC_COUNTS)->present = 1;
 }
 
-/* Compute checksum for the current function.  */
+/* Compute checksum for the current function.  We generate a CRC32.  */
 
-#define CHSUM_HASH	500000003
-#define CHSUM_SHIFT	2
-
-static long
+static unsigned
 compute_checksum ()
 {
-  long chsum = 0;
+  unsigned chksum = 0;
   basic_block bb;
-
+  
   FOR_EACH_BB (bb)
     {
-      edge e;
+      edge e = NULL;
+      
+      do
+	{
+	  unsigned value = BB_TO_GCOV_INDEX (e ? e->dest : bb);
+	  unsigned ix;
+
+	  /* No need to use all bits in value identically, nearly all
+	     functions have less than 256 blocks.  */
+	  value ^= value << 16;
+	  value ^= value << 8;
+	  
+	  for (ix = 8; ix--; value <<= 1)
+	    {
+	      unsigned feedback;
 
-      for (e = bb->succ; e; e = e->succ_next)
-	{
-	  chsum = ((chsum << CHSUM_SHIFT) + (BB_TO_GCOV_INDEX (e->dest) + 1)) % CHSUM_HASH;
+	      feedback = (value ^ chksum) & 0x80000000 ? 0x04c11db7 : 0;
+	      chksum <<= 1;
+	      chksum ^= feedback;
+	    }
+	  
+	  e = e ? e->succ_next : bb->succ;
 	}
-
-      chsum = (chsum << CHSUM_SHIFT) % CHSUM_HASH;
+      while (e);
     }
 
-  return chsum;
+  return chksum;
 }
 
 /* Instrument and/or analyze program behavior based on program flow graph.
@@ -771,22 +1313,26 @@ void
 branch_prob ()
 {
   basic_block bb;
-  int i;
-  int num_edges, ignored_edges;
+  unsigned i;
+  unsigned num_edges, ignored_edges;
   struct edge_list *el;
+  struct loops loops;
+  unsigned n_values;
+  struct histogram_value *values;
   const char *name = IDENTIFIER_POINTER
-		       (DECL_ASSEMBLER_NAME (current_function_decl));
+		      (DECL_ASSEMBLER_NAME (current_function_decl));
 
   profile_info.current_function_cfg_checksum = compute_checksum ();
+  for (i = 0; i < profile_info.n_sections; i++)
+    {
+      profile_info.section_info[i].n_counters_now = 0;
+      profile_info.section_info[i].present = 0;
+    }
 
   if (rtl_dump_file)
-    fprintf (rtl_dump_file, "CFG checksum is %ld\n",
+    fprintf (rtl_dump_file, "CFG checksum is %u\n",
 	profile_info.current_function_cfg_checksum);
 
-  /* Start of a function.  */
-  if (flag_test_coverage)
-    output_gcov_string (name, (long) -2);
-
   total_num_times_called++;
 
   flow_call_edges_add (NULL);
@@ -808,33 +1354,11 @@ branch_prob ()
       rtx insn;
       edge e;
 
-      /* Add fake edges from entry block to the call insns that may return
-	 twice.  The CFG is not quite correct then, as call insn plays more
-	 role of CODE_LABEL, but for our purposes, everything should be OK,
-	 as we never insert code to the beggining of basic block.  */
-      for (insn = bb->head; insn != NEXT_INSN (bb->end);
-	   insn = NEXT_INSN (insn))
-	{
-	  if (GET_CODE (insn) == CALL_INSN
-	      && find_reg_note (insn, REG_SETJMP, NULL))
-	    {
-	      if (GET_CODE (bb->head) == CODE_LABEL
-		  || insn != NEXT_INSN (bb->head))
-		{
-		  e = split_block (bb, PREV_INSN (insn));
-		  make_edge (ENTRY_BLOCK_PTR, e->dest, EDGE_FAKE);
-		  break;
-		}
-	      else
-		{
-		  /* We should not get abort here, as call to setjmp should not
-		     be the very first instruction of function.  */
-		  if (bb == ENTRY_BLOCK_PTR)
-		    abort ();
-		  make_edge (ENTRY_BLOCK_PTR, bb, EDGE_FAKE);
-		}
-	    }
-	}
+      /* Functions returning multiple times are not handled by extra edges.
+         Instead we simply allow negative counts on edges from exit to the
+         block past call and corresponding probabilities.  We can't go
+         with the extra edges because that would result in flowgraph that
+	 needs to have fake edges outside the spanning tree.  */
 
       for (e = bb->succ; e; e = e->succ_next)
 	{
@@ -869,6 +1393,19 @@ branch_prob ()
 	}
     }
 
+  if (flag_loop_histograms)
+    {
+      /* Find loops and bring them into canonical shape.  */
+      flow_loops_find (&loops, LOOP_TREE);
+      create_preheaders (&loops, 0);
+      /* Release dominators -- we aren't going to need them nor update them.  */
+      if (loops.cfg.dom)
+	{
+	  free_dominance_info (loops.cfg.dom);
+	  loops.cfg.dom = NULL;
+	}
+    }
+
   el = create_edge_list ();
   num_edges = NUM_EDGES (el);
   alloc_aux_for_edges (sizeof (struct edge_info));
@@ -895,75 +1432,11 @@ branch_prob ()
   verify_flow_info ();
 #endif
 
-  /* Output line number information about each basic block for
-     GCOV utility.  */
-  if (flag_test_coverage)
-    {
-      FOR_EACH_BB (bb)
-	{
-	  rtx insn = bb->head;
-	  static int ignore_next_note = 0;
-
-	  /* We are looking for line number notes.  Search backward before
-	     basic block to find correct ones.  */
-	  insn = prev_nonnote_insn (insn);
-	  if (!insn)
-	    insn = get_insns ();
-	  else
-	    insn = NEXT_INSN (insn);
+  /* Create spanning tree from basic block graph, mark each edge that is
+     on the spanning tree.  We insert as many abnormal and critical edges
+     as possible to minimize number of edge splits necessary.  */
 
-	  /* Output a zero to the .bb file to indicate that a new
-	     block list is starting.  */
-	  __write_long (0, bb_file, 4);
-
-	  while (insn != bb->end)
-	    {
-	      if (GET_CODE (insn) == NOTE)
-		{
-		  /* Must ignore the line number notes that immediately
-		     follow the end of an inline function to avoid counting
-		     it twice.  There is a note before the call, and one
-		     after the call.  */
-		  if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_REPEATED_LINE_NUMBER)
-		    ignore_next_note = 1;
-		  else if (NOTE_LINE_NUMBER (insn) > 0)
-		    {
-		      if (ignore_next_note)
-			ignore_next_note = 0;
-		      else
-			{
-			  /* If this is a new source file, then output the
-			     file's name to the .bb file.  */
-			  if (! last_bb_file_name
-			      || strcmp (NOTE_SOURCE_FILE (insn),
-					 last_bb_file_name))
-			    {
-			      if (last_bb_file_name)
-				free (last_bb_file_name);
-			      last_bb_file_name
-				= xstrdup (NOTE_SOURCE_FILE (insn));
-			      output_gcov_string (NOTE_SOURCE_FILE (insn),
-						  (long)-1);
-			    }
-			  /* Output the line number to the .bb file.  Must be
-			     done after the output_bb_profile_data() call, and
-			     after the file name is written, to ensure that it
-			     is correctly handled by gcov.  */
-			  __write_long (NOTE_LINE_NUMBER (insn), bb_file, 4);
-			}
-		    }
-		}
-	      insn = NEXT_INSN (insn);
-	    }
-	}
-      __write_long (0, bb_file, 4);
-    }
-
-  /* Create spanning tree from basic block graph, mark each edge that is
-     on the spanning tree.  We insert as many abnormal and critical edges
-     as possible to minimize number of edge splits necessary.  */
-
-  find_spanning_tree (el);
+  find_spanning_tree (el);
 
   /* Fake edges that are not on the tree will not be instrumented, so
      mark them ignored.  */
@@ -995,77 +1468,216 @@ branch_prob ()
      edge output the source and target basic block numbers.
      NOTE: The format of this file must be compatible with gcov.  */
 
-  if (flag_test_coverage)
+  if (flag_test_coverage && bbg_file)
     {
-      int flag_bits;
-
-      __write_gcov_string (name, strlen (name), bbg_file, -1);
-
-      /* write checksum.  */
-      __write_long (profile_info.current_function_cfg_checksum, bbg_file, 4);
-
-      /* The plus 2 stands for entry and exit block.  */
-      __write_long (n_basic_blocks + 2, bbg_file, 4);
-      __write_long (num_edges - ignored_edges + 1, bbg_file, 4);
-
+      long offset;
+      
+      /* Announce function */
+      if (gcov_write_unsigned (bbg_file, GCOV_TAG_FUNCTION)
+	  || !(offset = gcov_reserve_length (bbg_file))
+	  || gcov_write_string (bbg_file, name,
+			     strlen (name))
+	  || gcov_write_unsigned (bbg_file,
+			    profile_info.current_function_cfg_checksum)
+	  || gcov_write_length (bbg_file, offset))
+	goto bbg_error;
+
+      /* Basic block flags */
+      if (gcov_write_unsigned (bbg_file, GCOV_TAG_BLOCKS)
+	  || !(offset = gcov_reserve_length (bbg_file)))
+	goto bbg_error;
+      for (i = 0; i != (unsigned) (n_basic_blocks + 2); i++)
+	if (gcov_write_unsigned (bbg_file, 0))
+	  goto bbg_error;
+      if (gcov_write_length (bbg_file, offset))
+	goto bbg_error;
+      
+      /* Arcs */
       FOR_BB_BETWEEN (bb, ENTRY_BLOCK_PTR, EXIT_BLOCK_PTR, next_bb)
 	{
 	  edge e;
-	  long count = 0;
 
-	  for (e = bb->succ; e; e = e->succ_next)
-	    if (!EDGE_INFO (e)->ignore)
-	      count++;
-	  __write_long (count, bbg_file, 4);
+	  if (gcov_write_unsigned (bbg_file, GCOV_TAG_ARCS)
+	      || !(offset = gcov_reserve_length (bbg_file))
+	      || gcov_write_unsigned (bbg_file, BB_TO_GCOV_INDEX (bb)))
+	    goto bbg_error;
 
 	  for (e = bb->succ; e; e = e->succ_next)
 	    {
 	      struct edge_info *i = EDGE_INFO (e);
 	      if (!i->ignore)
 		{
-		  flag_bits = 0;
+		  unsigned flag_bits = 0;
+		  
 		  if (i->on_tree)
-		    flag_bits |= 0x1;
+		    flag_bits |= GCOV_ARC_ON_TREE;
 		  if (e->flags & EDGE_FAKE)
-		    flag_bits |= 0x2;
+		    flag_bits |= GCOV_ARC_FAKE;
 		  if (e->flags & EDGE_FALLTHRU)
-		    flag_bits |= 0x4;
+		    flag_bits |= GCOV_ARC_FALLTHROUGH;
 
-		  __write_long (BB_TO_GCOV_INDEX (e->dest), bbg_file, 4);
-		  __write_long (flag_bits, bbg_file, 4);
+		  if (gcov_write_unsigned (bbg_file,
+					   BB_TO_GCOV_INDEX (e->dest))
+		      || gcov_write_unsigned (bbg_file, flag_bits))
+		    goto bbg_error;
 	        }
 	    }
+
+	  if (gcov_write_length (bbg_file, offset))
+	    goto bbg_error;
 	}
-      /* Emit fake loopback edge for EXIT block to maintain compatibility with
-         old gcov format.  */
-      __write_long (1, bbg_file, 4);
-      __write_long (0, bbg_file, 4);
-      __write_long (0x1, bbg_file, 4);
-
-      /* Emit a -1 to separate the list of all edges from the list of
-	 loop back edges that follows.  */
-      __write_long (-1, bbg_file, 4);
+
+      /* Output line number information about each basic block for
+     	 GCOV utility.  */
+      {
+	char const *prev_file_name = NULL;
+	
+	FOR_EACH_BB (bb)
+	  {
+	    rtx insn = bb->head;
+	    int ignore_next_note = 0;
+	    
+	    offset = 0;
+	    
+	    /* We are looking for line number notes.  Search backward
+	       before basic block to find correct ones.  */
+	    insn = prev_nonnote_insn (insn);
+	    if (!insn)
+	      insn = get_insns ();
+	    else
+	      insn = NEXT_INSN (insn);
+
+	    while (insn != bb->end)
+	      {
+		if (GET_CODE (insn) == NOTE)
+		  {
+		     /* Must ignore the line number notes that immediately
+		     	follow the end of an inline function to avoid counting
+		     	it twice.  There is a note before the call, and one
+		     	after the call.  */
+		    if (NOTE_LINE_NUMBER (insn)
+			== NOTE_INSN_REPEATED_LINE_NUMBER)
+		      ignore_next_note = 1;
+		    else if (NOTE_LINE_NUMBER (insn) <= 0)
+		      /*NOP*/;
+		    else if (ignore_next_note)
+		      ignore_next_note = 0;
+		    else
+		      {
+			if (offset)
+			  /*NOP*/;
+			else if (gcov_write_unsigned (bbg_file, GCOV_TAG_LINES)
+				 || !(offset = gcov_reserve_length (bbg_file))
+				 || gcov_write_unsigned (bbg_file,
+						   BB_TO_GCOV_INDEX (bb)))
+			  goto bbg_error;
+			/* If this is a new source file, then output
+			   the file's name to the .bb file.  */
+			if (!prev_file_name
+			    || strcmp (NOTE_SOURCE_FILE (insn),
+				       prev_file_name))
+			  {
+			    prev_file_name = NOTE_SOURCE_FILE (insn);
+			    if (gcov_write_unsigned (bbg_file, 0)
+				|| gcov_write_string (bbg_file, prev_file_name,
+						      strlen (prev_file_name)))
+			      goto bbg_error;
+			  }
+			if (gcov_write_unsigned (bbg_file, NOTE_LINE_NUMBER (insn)))
+			  goto bbg_error;
+		      }
+		  }
+		insn = NEXT_INSN (insn);
+	      }
+
+	    if (offset)
+	      {
+		if (gcov_write_unsigned (bbg_file, 0)
+		    || gcov_write_string (bbg_file, NULL, 0)
+		    || gcov_write_length (bbg_file, offset))
+		  {
+		  bbg_error:;
+		    warning ("error writing `%s'", bbg_file_name);
+		    fclose (bbg_file);
+		    bbg_file = NULL;
+		  }
+	      }
+	  }
+      }
+    }
+
+  if (flag_value_histograms)
+    {
+      find_values_to_profile (&n_values, &values);
+      allocate_reg_info (max_reg_num (), FALSE, FALSE);
     }
 
   if (flag_branch_probabilities)
-    compute_branch_probabilities ();
+    {
+      compute_branch_probabilities ();
+      if (flag_loop_histograms)
+	compute_loop_histograms (&loops);
+      if (flag_value_histograms)
+	compute_value_histograms (n_values, values);
+    }
 
   /* For each edge not on the spanning tree, add counting code as rtl.  */
 
-  if (profile_arc_flag)
+  if (cfun->arc_profile && profile_arc_flag)
     {
+      struct function_list *item;
+      
       instrument_edges (el);
+      if (flag_loop_histograms)
+	instrument_loops (&loops);
+      if (flag_value_histograms)
+	instrument_values (n_values, values);
+
+      /* Commit changes done by instrumentation.  */
+      commit_edge_insertions_watch_calls ();
       allocate_reg_info (max_reg_num (), FALSE, FALSE);
+
+      /* ??? Probably should re-use the existing struct function.  */
+      item = xmalloc (sizeof (struct function_list));
+      
+      *functions_tail = item;
+      functions_tail = &item->next;
+      
+      item->next = 0;
+      item->name = xstrdup (name);
+      item->cfg_checksum = profile_info.current_function_cfg_checksum;
+      item->n_counter_sections = 0;
+      for (i = 0; i < profile_info.n_sections; i++)
+	if (profile_info.section_info[i].n_counters_now)
+	  {
+	    item->counter_sections[item->n_counter_sections].tag = 
+		    profile_info.section_info[i].tag;
+	    item->counter_sections[item->n_counter_sections].n_counters =
+		    profile_info.section_info[i].n_counters_now;
+	    item->n_counter_sections++;
+	  }
+    }
+
+  if (flag_loop_histograms)
+    {
+      /* Free the loop datastructure.  */
+      flow_loops_free (&loops);
+    }
+
+  if (flag_value_histograms)
+    {
+      /* Free list of interesting values.  */
+      free_profiled_values (n_values, values);
     }
 
   remove_fake_edges ();
+  free_aux_for_edges ();
   /* Re-merge split basic blocks and the mess introduced by
      insert_insn_on_edge.  */
   cleanup_cfg (profile_arc_flag ? CLEANUP_EXPENSIVE : 0);
   if (rtl_dump_file)
     dump_flow_info (rtl_dump_file);
 
-  free_aux_for_edges ();
   free_edge_list (el);
 }
 
@@ -1194,30 +1806,25 @@ init_branch_prob (filename)
 
   if (flag_test_coverage)
     {
-      char *data_file, *bbg_file_name;
-
-      /* Open an output file for the basic block/line number map.  */
-      data_file = (char *) alloca (len + 4);
-      strcpy (data_file, filename);
-      strcat (data_file, ".bb");
-      if ((bb_file = fopen (data_file, "wb")) == 0)
-	fatal_io_error ("can't open %s", data_file);
-
-      /* Open an output file for the program flow graph.  */
-      bbg_file_name = (char *) alloca (len + 5);
+      /* Open the bbg output file.  */
+      bbg_file_name = (char *) xmalloc (len + strlen (GCOV_GRAPH_SUFFIX) + 1);
       strcpy (bbg_file_name, filename);
-      strcat (bbg_file_name, ".bbg");
-      if ((bbg_file = fopen (bbg_file_name, "wb")) == 0)
-	fatal_io_error ("can't open %s", bbg_file_name);
+      strcat (bbg_file_name, GCOV_GRAPH_SUFFIX);
+      bbg_file = fopen (bbg_file_name, "wb");
+      if (!bbg_file)
+	fatal_io_error ("cannot open %s", bbg_file_name);
 
-      /* Initialize to zero, to ensure that the first file name will be
-	 written to the .bb file.  */
-      last_bb_file_name = 0;
+      if (gcov_write_unsigned (bbg_file, GCOV_GRAPH_MAGIC)
+	  || gcov_write_unsigned (bbg_file, GCOV_VERSION))
+	{
+	  fclose (bbg_file);
+	  fatal_io_error ("cannot write `%s'", bbg_file_name);
+	}
     }
 
-  da_file_name = (char *) xmalloc (len + 4);
+  da_file_name = (char *) xmalloc (len + strlen (GCOV_DATA_SUFFIX) + 1);
   strcpy (da_file_name, filename);
-  strcat (da_file_name, ".da");
+  strcat (da_file_name, GCOV_DATA_SUFFIX);
   
   if (flag_branch_probabilities)
     {
@@ -1225,11 +1832,30 @@ init_branch_prob (filename)
       if (!da_file)
 	warning ("file %s not found, execution counts assumed to be zero",
 		 da_file_name);
+      if (counts_file_index && strcmp (da_file_name, counts_file_name))
+       	cleanup_counts_index (0);
+      if (index_counts_file ())
+	counts_file_name = xstrdup (da_file_name);
     }
 
   if (profile_arc_flag)
-    init_edge_profiler ();
+    {
+      /* Generate and save a copy of this so it can be shared.  */
+      char buf[20];
+      
+      ASM_GENERATE_INTERNAL_LABEL (buf, "LPBX", 2);
+      profiler_label = gen_rtx_SYMBOL_REF (Pmode, ggc_strdup (buf));
+
+      ASM_GENERATE_INTERNAL_LABEL (buf, "LPBX", 3);
+      loop_histograms_label = gen_rtx_SYMBOL_REF (Pmode, ggc_strdup (buf));
+
+      ASM_GENERATE_INTERNAL_LABEL (buf, "LPBX", 4);
+      value_histograms_label = gen_rtx_SYMBOL_REF (Pmode, ggc_strdup (buf));
 
+      ASM_GENERATE_INTERNAL_LABEL (buf, "LPBX", 5);
+      same_value_histograms_label = gen_rtx_SYMBOL_REF (Pmode, ggc_strdup (buf));
+    }
+  
   total_num_blocks = 0;
   total_num_edges = 0;
   total_num_edges_ignored = 0;
@@ -1251,12 +1877,32 @@ end_branch_prob ()
 {
   if (flag_test_coverage)
     {
-      fclose (bb_file);
-      fclose (bbg_file);
-      unlink (da_file_name);
+      if (bbg_file)
+	{
+#if __GNUC__ && !CROSS_COMPILE && SUPPORTS_WEAK
+	  /* If __gcov_init has a value in the compiler, it means we
+	     are instrumenting ourselves. We should not remove the
+	     counts file, because we might be recompiling
+	     ourselves. The .da files are all removed during copying
+	     the stage1 files.  */
+	  extern void __gcov_init (void *)
+	    __attribute__ ((weak));
+	  
+	  if (!__gcov_init)
+	    unlink (da_file_name);
+#else
+	  unlink (da_file_name);
+#endif
+	  fclose (bbg_file);
+	}
+      else
+	{
+	  unlink (bbg_file_name);
+	  unlink (da_file_name);
+	}
     }
 
-  if (flag_branch_probabilities && da_file)
+  if (da_file)
     fclose (da_file);
 
   if (rtl_dump_file)
@@ -1292,22 +1938,635 @@ end_branch_prob ()
 	}
     }
 }
-
-/* The label used by the edge profiling code.  */
 
-static GTY(()) rtx profiler_label;
+/* Find (and create if not present) a section with TAG.  */
+struct section_info *
+find_counters_section (tag)
+     unsigned tag;
+{
+  unsigned i;
 
-/* Initialize the profiler_label.  */
+  for (i = 0; i < profile_info.n_sections; i++)
+    if (profile_info.section_info[i].tag == tag)
+      return profile_info.section_info + i;
+
+  if (i == MAX_COUNTER_SECTIONS)
+    abort ();
 
+  profile_info.section_info[i].tag = tag;
+  profile_info.section_info[i].present = 0;
+  profile_info.section_info[i].n_counters = 0;
+  profile_info.section_info[i].n_counters_now = 0;
+  profile_info.n_sections++;
+
+  return profile_info.section_info + i;
+}
+
+/* Set FIELDS as purpose to VALUE.  */
 static void
-init_edge_profiler ()
+set_purpose (value, fields)
+     tree value;
+     tree fields;
+{
+  tree act_field, act_value;
+  
+  for (act_field = fields, act_value = value;
+       act_field;
+       act_field = TREE_CHAIN (act_field), act_value = TREE_CHAIN (act_value))
+    TREE_PURPOSE (act_value) = act_field;
+}
+
+/* Returns label for base of counters inside TAG section.  */
+static rtx
+label_for_tag (tag)
+     unsigned tag;
+{
+  switch (tag)
+    {
+    case GCOV_TAG_ARC_COUNTS:
+      return profiler_label;
+    case GCOV_TAG_LOOP_HISTOGRAMS:
+      return loop_histograms_label;
+    case GCOV_TAG_VALUE_HISTOGRAMS:
+      return value_histograms_label;
+    case GCOV_TAG_SAME_VALUE_HISTOGRAMS:
+      return same_value_histograms_label;
+    default:
+      abort ();
+    }
+}
+
+/* Creates fields of struct counter_section (in gcov-io.h).  */
+static tree
+build_counter_section_fields ()
 {
-  /* Generate and save a copy of this so it can be shared.  */
-  char buf[20];
-  ASM_GENERATE_INTERNAL_LABEL (buf, "LPBX", 2);
-  profiler_label = gen_rtx_SYMBOL_REF (Pmode, ggc_strdup (buf));
+  tree field, fields;
+
+  /* tag */
+  fields = build_decl (FIELD_DECL, NULL_TREE, unsigned_type_node);
+
+  /* n_counters */
+  field = build_decl (FIELD_DECL, NULL_TREE, unsigned_type_node);
+  TREE_CHAIN (field) = fields;
+  fields = field;
+
+  return fields;
+}
+
+/* Creates value of struct counter_section (in gcov-io.h).  */
+static tree
+build_counter_section_value (tag, n_counters)
+     unsigned tag;
+     unsigned n_counters;
+{
+  tree value = NULL_TREE;
+
+  /* tag */
+  value = tree_cons (NULL_TREE,
+		     convert (unsigned_type_node,
+			      build_int_2 (tag, 0)),
+		     value);
+  
+  /* n_counters */
+  value = tree_cons (NULL_TREE,
+		     convert (unsigned_type_node,
+			      build_int_2 (n_counters, 0)),
+		     value);
+
+  return value;
 }
 
+/* Creates fields of struct counter_section_data (in gcov-io.h).  */
+static tree
+build_counter_section_data_fields ()
+{
+  tree field, fields, gcov_type, gcov_ptr_type;
+
+  gcov_type = make_signed_type (GCOV_TYPE_SIZE);
+  gcov_ptr_type =
+	  build_pointer_type (build_qualified_type (gcov_type,
+						    TYPE_QUAL_CONST));
+
+  /* tag */
+  fields = build_decl (FIELD_DECL, NULL_TREE, unsigned_type_node);
+
+  /* n_counters */
+  field = build_decl (FIELD_DECL, NULL_TREE, unsigned_type_node);
+  TREE_CHAIN (field) = fields;
+  fields = field;
+
+  /* counters */
+  field = build_decl (FIELD_DECL, NULL_TREE, gcov_ptr_type);
+  TREE_CHAIN (field) = fields;
+  fields = field;
+
+  return fields;
+}
+
+/* Creates value of struct counter_section_data (in gcov-io.h).  */
+static tree
+build_counter_section_data_value (tag, n_counters)
+     unsigned tag;
+     unsigned n_counters;
+{
+  tree value = NULL_TREE, counts_table, gcov_type, gcov_ptr_type;
+
+  gcov_type = make_signed_type (GCOV_TYPE_SIZE);
+  gcov_ptr_type
+    = build_pointer_type (build_qualified_type
+			  (gcov_type, TYPE_QUAL_CONST));
+
+  /* tag */
+  value = tree_cons (NULL_TREE,
+		     convert (unsigned_type_node,
+			      build_int_2 (tag, 0)),
+		     value);
+  
+  /* n_counters */
+  value = tree_cons (NULL_TREE,
+		     convert (unsigned_type_node,
+			      build_int_2 (n_counters, 0)),
+		     value);
+
+  /* counters */
+  if (n_counters)
+    {
+      tree gcov_type_array_type =
+	      build_array_type (gcov_type,
+				build_index_type (build_int_2 (n_counters - 1,
+							       0)));
+      counts_table =
+	      build (VAR_DECL, gcov_type_array_type, NULL_TREE, NULL_TREE);
+      TREE_STATIC (counts_table) = 1;
+      DECL_NAME (counts_table) = get_identifier (XSTR (label_for_tag (tag), 0));
+      assemble_variable (counts_table, 0, 0, 0);
+      counts_table = build1 (ADDR_EXPR, gcov_ptr_type, counts_table);
+    }
+  else
+    counts_table = null_pointer_node;
+
+  value = tree_cons (NULL_TREE, counts_table, value);
+
+  return value;
+}
+
+/* Creates fields for struct function_info type (in gcov-io.h).  */
+static tree
+build_function_info_fields ()
+{
+  tree field, fields, counter_section_fields, counter_section_type;
+  tree counter_sections_ptr_type;
+  tree string_type =
+	  build_pointer_type (build_qualified_type (char_type_node,
+						    TYPE_QUAL_CONST));
+  /* name */
+  fields = build_decl (FIELD_DECL, NULL_TREE, string_type);
+
+  /* checksum */
+  field = build_decl (FIELD_DECL, NULL_TREE, unsigned_type_node);
+  TREE_CHAIN (field) = fields;
+  fields = field;
+
+  /* n_counter_sections */
+  field = build_decl (FIELD_DECL, NULL_TREE, unsigned_type_node);
+  TREE_CHAIN (field) = fields;
+  fields = field;
+
+  /* counter_sections */
+  counter_section_fields = build_counter_section_fields ();
+  counter_section_type = (*lang_hooks.types.make_type) (RECORD_TYPE);
+  finish_builtin_struct (counter_section_type, "__counter_section",
+			 counter_section_fields, NULL_TREE);
+  counter_sections_ptr_type =
+	  build_pointer_type
+	  	(build_qualified_type (counter_section_type,
+				       TYPE_QUAL_CONST));
+  field = build_decl (FIELD_DECL, NULL_TREE, counter_sections_ptr_type);
+  TREE_CHAIN (field) = fields;
+  fields = field;
+
+  return fields;
+}
+
+/* Creates value for struct function_info (in gcov-io.h).  */
+static tree
+build_function_info_value (function)
+     struct function_list *function;
+{
+  tree value = NULL_TREE;
+  size_t name_len = strlen (function->name);
+  tree fname = build_string (name_len + 1, function->name);
+  tree string_type =
+	  build_pointer_type (build_qualified_type (char_type_node,
+						    TYPE_QUAL_CONST));
+  tree counter_section_fields, counter_section_type, counter_sections_value;
+  tree counter_sections_ptr_type, counter_sections_array_type;
+  unsigned i;
+
+  /* name */
+  TREE_TYPE (fname) =
+	  build_array_type (char_type_node,
+			    build_index_type (build_int_2 (name_len, 0)));
+  value = tree_cons (NULL_TREE,
+		     build1 (ADDR_EXPR,
+			     string_type,
+			     fname),
+		     value);
+
+  /* checksum */
+  value = tree_cons (NULL_TREE,
+		     convert (unsigned_type_node,
+			      build_int_2 (function->cfg_checksum, 0)),
+		     value);
+
+  /* n_counter_sections */
+
+  value = tree_cons (NULL_TREE,
+		     convert (unsigned_type_node,
+			      build_int_2 (function->n_counter_sections, 0)),
+	    	    value);
+
+  /* counter_sections */
+  counter_section_fields = build_counter_section_fields ();
+  counter_section_type = (*lang_hooks.types.make_type) (RECORD_TYPE);
+  counter_sections_ptr_type =
+	  build_pointer_type
+	  	(build_qualified_type (counter_section_type,
+				       TYPE_QUAL_CONST));
+  counter_sections_array_type =
+	  build_array_type (counter_section_type,
+			    build_index_type (
+      				build_int_2 (function->n_counter_sections - 1,
+		  			     0)));
+
+  counter_sections_value = NULL_TREE;
+  for (i = 0; i < function->n_counter_sections; i++)
+    {
+      tree counter_section_value =
+	      build_counter_section_value (function->counter_sections[i].tag,
+					   function->counter_sections[i].n_counters);
+      set_purpose (counter_section_value, counter_section_fields);
+      counter_sections_value = tree_cons (NULL_TREE,
+					  build (CONSTRUCTOR,
+						 counter_section_type,
+						 NULL_TREE,
+						 nreverse (counter_section_value)),
+					  counter_sections_value);
+    }
+  finish_builtin_struct (counter_section_type, "__counter_section",
+			 counter_section_fields, NULL_TREE);
+
+  if (function->n_counter_sections)
+    {
+      counter_sections_value = 
+	      build (CONSTRUCTOR,
+ 		     counter_sections_array_type,
+		     NULL_TREE,
+		     nreverse (counter_sections_value)),
+      counter_sections_value = build1 (ADDR_EXPR,
+				       counter_sections_ptr_type,
+				       counter_sections_value);
+    }
+  else
+    counter_sections_value = null_pointer_node;
+
+  value = tree_cons (NULL_TREE, counter_sections_value, value);
+
+  return value;
+}
+
+/* Creates fields of struct gcov_info type (in gcov-io.h).  */
+static tree
+build_gcov_info_fields (gcov_info_type)
+     tree gcov_info_type;
+{
+  tree field, fields;
+  char *filename;
+  int filename_len;
+  tree string_type =
+	  build_pointer_type (build_qualified_type (char_type_node,
+						    TYPE_QUAL_CONST));
+  tree function_info_fields, function_info_type, function_info_ptr_type;
+  tree counter_section_data_fields, counter_section_data_type;
+  tree counter_section_data_ptr_type;
+
+  /* Version ident */
+  fields = build_decl (FIELD_DECL, NULL_TREE, long_unsigned_type_node);
+
+  /* next -- NULL */
+  field = build_decl (FIELD_DECL, NULL_TREE,
+		      build_pointer_type (build_qualified_type (gcov_info_type,
+								TYPE_QUAL_CONST)));
+  TREE_CHAIN (field) = fields;
+  fields = field;
+  
+  /* Filename */
+  filename = getpwd ();
+  filename = (filename && da_file_name[0] != '/'
+	      ? concat (filename, "/", da_file_name, NULL)
+	      : da_file_name);
+  filename_len = strlen (filename);
+  if (filename != da_file_name)
+    free (filename);
+
+  field = build_decl (FIELD_DECL, NULL_TREE, string_type);
+  TREE_CHAIN (field) = fields;
+  fields = field;
+  
+  /* Workspace */
+  field = build_decl (FIELD_DECL, NULL_TREE, long_integer_type_node);
+  TREE_CHAIN (field) = fields;
+  fields = field;
+
+  /* number of functions */
+  field = build_decl (FIELD_DECL, NULL_TREE, unsigned_type_node);
+  TREE_CHAIN (field) = fields;
+  fields = field;
+      
+  /* function_info table */
+  function_info_fields = build_function_info_fields ();
+  function_info_type = (*lang_hooks.types.make_type) (RECORD_TYPE);
+  finish_builtin_struct (function_info_type, "__function_info",
+			 function_info_fields, NULL_TREE);
+  function_info_ptr_type =
+	  build_pointer_type
+	  	(build_qualified_type (function_info_type,
+				       TYPE_QUAL_CONST));
+  field = build_decl (FIELD_DECL, NULL_TREE, function_info_ptr_type);
+  TREE_CHAIN (field) = fields;
+  fields = field;
+    
+  /* n_counter_sections  */
+  field = build_decl (FIELD_DECL, NULL_TREE, unsigned_type_node);
+  TREE_CHAIN (field) = fields;
+  fields = field;
+  
+  /* counter sections */
+  counter_section_data_fields = build_counter_section_data_fields ();
+  counter_section_data_type = (*lang_hooks.types.make_type) (RECORD_TYPE);
+  finish_builtin_struct (counter_section_data_type, "__counter_section_data",
+			 counter_section_data_fields, NULL_TREE);
+  counter_section_data_ptr_type =
+	  build_pointer_type
+	  	(build_qualified_type (counter_section_data_type,
+				       TYPE_QUAL_CONST));
+  field = build_decl (FIELD_DECL, NULL_TREE, counter_section_data_ptr_type);
+  TREE_CHAIN (field) = fields;
+  fields = field;
+
+  return fields;
+}
+
+/* Creates struct gcov_info value (in gcov-io.h).  */
+static tree
+build_gcov_info_value ()
+{
+  tree value = NULL_TREE;
+  tree filename_string;
+  char *filename;
+  int filename_len;
+  unsigned n_functions, i;
+  struct function_list *item;
+  tree string_type =
+	  build_pointer_type (build_qualified_type (char_type_node,
+						    TYPE_QUAL_CONST));
+  tree function_info_fields, function_info_type, function_info_ptr_type;
+  tree functions;
+  tree counter_section_data_fields, counter_section_data_type;
+  tree counter_section_data_ptr_type, counter_sections;
+
+  /* Version ident */
+  value = tree_cons (NULL_TREE,
+		     convert (long_unsigned_type_node,
+			      build_int_2 (GCOV_VERSION, 0)),
+		     value);
+
+  /* next -- NULL */
+  value = tree_cons (NULL_TREE, null_pointer_node, value);
+  
+  /* Filename */
+  filename = getpwd ();
+  filename = (filename && da_file_name[0] != '/'
+	      ? concat (filename, "/", da_file_name, NULL)
+	      : da_file_name);
+  filename_len = strlen (filename);
+  filename_string = build_string (filename_len + 1, filename);
+  if (filename != da_file_name)
+    free (filename);
+  TREE_TYPE (filename_string) =
+	  build_array_type (char_type_node,
+			    build_index_type (build_int_2 (filename_len, 0)));
+  value = tree_cons (NULL_TREE,
+		     build1 (ADDR_EXPR,
+			     string_type,
+		       	     filename_string),
+		     value);
+  
+  /* Workspace */
+  value = tree_cons (NULL_TREE,
+		     convert (long_integer_type_node, integer_zero_node),
+		     value);
+      
+  /* number of functions */
+  n_functions = 0;
+  for (item = functions_head; item != 0; item = item->next, n_functions++)
+    continue;
+  value = tree_cons (NULL_TREE,
+		     convert (unsigned_type_node,
+			      build_int_2 (n_functions, 0)),
+		     value);
+
+  /* function_info table */
+  function_info_fields = build_function_info_fields ();
+  function_info_type = (*lang_hooks.types.make_type) (RECORD_TYPE);
+  function_info_ptr_type =
+	  build_pointer_type (
+		build_qualified_type (function_info_type,
+	       			      TYPE_QUAL_CONST));
+  functions = NULL_TREE;
+  for (item = functions_head; item != 0; item = item->next)
+    {
+      tree function_info_value = build_function_info_value (item);
+      set_purpose (function_info_value, function_info_fields);
+      functions = tree_cons (NULL_TREE,
+    			     build (CONSTRUCTOR,
+			    	    function_info_type,
+				    NULL_TREE,
+				    nreverse (function_info_value)),
+			     functions);
+    }
+  finish_builtin_struct (function_info_type, "__function_info",
+			 function_info_fields, NULL_TREE);
+
+  /* Create constructor for array.  */
+  if (n_functions)
+    {
+      tree array_type;
+
+      array_type = build_array_type (
+			function_info_type,
+   			build_index_type (build_int_2 (n_functions - 1, 0)));
+      functions = build (CONSTRUCTOR,
+      			 array_type,
+			 NULL_TREE,
+			 nreverse (functions));
+      functions = build1 (ADDR_EXPR,
+			  function_info_ptr_type,
+			  functions);
+    }
+  else
+    functions = null_pointer_node;
+
+  value = tree_cons (NULL_TREE, functions, value);
+
+  /* n_counter_sections  */
+  value = tree_cons (NULL_TREE,
+		     convert (unsigned_type_node,
+			      build_int_2 (profile_info.n_sections, 0)),
+		     value);
+  
+  /* counter sections */
+  counter_section_data_fields = build_counter_section_data_fields ();
+  counter_section_data_type = (*lang_hooks.types.make_type) (RECORD_TYPE);
+  counter_sections = NULL_TREE;
+  for (i = 0; i < profile_info.n_sections; i++)
+    {
+      tree counter_sections_value =
+	      build_counter_section_data_value (
+		profile_info.section_info[i].tag,
+		profile_info.section_info[i].n_counters);
+      set_purpose (counter_sections_value, counter_section_data_fields);
+      counter_sections = tree_cons (NULL_TREE,
+		       		    build (CONSTRUCTOR,
+		       			   counter_section_data_type,
+		       			   NULL_TREE,
+		       			   nreverse (counter_sections_value)),
+		       		    counter_sections);
+    }
+  finish_builtin_struct (counter_section_data_type, "__counter_section_data",
+			 counter_section_data_fields, NULL_TREE);
+  counter_section_data_ptr_type =
+	  build_pointer_type
+	  	(build_qualified_type (counter_section_data_type,
+				       TYPE_QUAL_CONST));
+
+  if (profile_info.n_sections)
+    {
+      counter_sections =
+    	      build (CONSTRUCTOR,
+    		     build_array_type (
+	       			       counter_section_data_type,
+		       		       build_index_type (build_int_2 (profile_info.n_sections - 1, 0))),
+		     NULL_TREE,
+		     nreverse (counter_sections));
+      counter_sections = build1 (ADDR_EXPR,
+				 counter_section_data_ptr_type,
+				 counter_sections);
+    }
+  else
+    counter_sections = null_pointer_node;
+  value = tree_cons (NULL_TREE, counter_sections, value);
+
+  return value;
+}
+
+/* Write out the structure which libgcc uses to locate all the arc
+   counters.  The structures used here must match those defined in
+   gcov-io.h.  Write out the constructor to call __gcov_init.  */
+
+void
+create_profiler ()
+{
+  tree gcov_info_fields, gcov_info_type, gcov_info_value, gcov_info;
+  char name[20];
+  char *ctor_name;
+  tree ctor;
+  rtx gcov_info_address;
+  int save_flag_inline_functions = flag_inline_functions;
+  unsigned i;
+
+  for (i = 0; i < profile_info.n_sections; i++)
+    if (profile_info.section_info[i].n_counters_now)
+      break;
+  if (i == profile_info.n_sections)
+    return;
+  
+  gcov_info_type = (*lang_hooks.types.make_type) (RECORD_TYPE);
+  gcov_info_fields = build_gcov_info_fields (gcov_info_type);
+  gcov_info_value = build_gcov_info_value ();
+  set_purpose (gcov_info_value, gcov_info_fields);
+  finish_builtin_struct (gcov_info_type, "__gcov_info",
+			 gcov_info_fields, NULL_TREE);
+
+  gcov_info = build (VAR_DECL, gcov_info_type, NULL_TREE, NULL_TREE);
+  DECL_INITIAL (gcov_info) =
+	  build (CONSTRUCTOR, gcov_info_type, NULL_TREE,
+		 nreverse (gcov_info_value));
+
+  TREE_STATIC (gcov_info) = 1;
+  ASM_GENERATE_INTERNAL_LABEL (name, "LPBX", 0);
+  DECL_NAME (gcov_info) = get_identifier (name);
+  
+  /* Build structure.  */
+  assemble_variable (gcov_info, 0, 0, 0);
+
+  /* Build the constructor function to invoke __gcov_init. */
+  ctor_name = concat (IDENTIFIER_POINTER (get_file_function_name ('I')),
+		      "_GCOV", NULL);
+  ctor = build_decl (FUNCTION_DECL, get_identifier (ctor_name),
+		     build_function_type (void_type_node, NULL_TREE));
+  free (ctor_name);
+  DECL_EXTERNAL (ctor) = 0;
+
+  /* It can be a static function as long as collect2 does not have
+     to scan the object file to find its ctor/dtor routine.  */
+  TREE_PUBLIC (ctor) = ! targetm.have_ctors_dtors;
+  TREE_USED (ctor) = 1;
+  DECL_RESULT (ctor) = build_decl (RESULT_DECL, NULL_TREE, void_type_node);
+
+  ctor = (*lang_hooks.decls.pushdecl) (ctor);
+  rest_of_decl_compilation (ctor, 0, 1, 0);
+  announce_function (ctor);
+  current_function_decl = ctor;
+  DECL_INITIAL (ctor) = error_mark_node;
+  make_decl_rtl (ctor, NULL);
+  init_function_start (ctor, input_filename, lineno);
+  (*lang_hooks.decls.pushlevel) (0);
+  expand_function_start (ctor, 0);
+  cfun->arc_profile = 0;
+
+  /* Actually generate the code to call __gcov_init.  */
+  gcov_info_address = force_reg (Pmode,
+				 gen_rtx_SYMBOL_REF (
+					Pmode,
+					IDENTIFIER_POINTER (
+						DECL_NAME (gcov_info))));
+  emit_library_call (gen_rtx_SYMBOL_REF (Pmode, "__gcov_init"),
+		     LCT_NORMAL, VOIDmode, 1,
+		     gcov_info_address, Pmode);
+
+  expand_function_end (input_filename, lineno, 0);
+  (*lang_hooks.decls.poplevel) (1, 0, 1);
+
+  /* Since ctor isn't in the list of globals, it would never be emitted
+     when it's considered to be 'safe' for inlining, so turn off
+     flag_inline_functions.  */
+  flag_inline_functions = 0;
+
+  rest_of_compilation (ctor);
+
+  /* Reset flag_inline_functions to its original value.  */
+  flag_inline_functions = save_flag_inline_functions;
+
+  if (! quiet_flag)
+    fflush (asm_out_file);
+  current_function_decl = NULL_TREE;
+
+  if (targetm.have_ctors_dtors)
+    (* targetm.asm_out.constructor) (XEXP (DECL_RTL (ctor), 0),
+				     DEFAULT_INIT_PRIORITY);
+}
+
 /* Output instructions as RTL to increment the edge execution count.  */
 
 static rtx
@@ -1337,90 +2596,324 @@ gen_edge_profiler (edgeno)
   return sequence;
 }
 
-/* Output code for a constructor that will invoke __bb_init_func, if
-   this has not already been done.  */
+/* Output instructions as RTL to increment the interval histogram counter.
+   VALUE is the expression whose value is profiled.  BASE_LABEL is the base
+   of histogram counters, BASE is offset from this position.  */
 
-void
-output_func_start_profiler ()
+static rtx
+gen_interval_profiler (value, base_label, base)
+     struct histogram_value *value;
+     rtx base_label;
+     int base;
 {
-  tree fnname, fndecl;
-  char *name;
-  char buf[20];
-  const char *cfnname;
-  rtx table_address;
   enum machine_mode mode = mode_for_size (GCOV_TYPE_SIZE, MODE_INT, 0);
-  int save_flag_inline_functions = flag_inline_functions;
+  rtx mem_ref, tmp, tmp1, mr, val;
+  rtx sequence;
+  rtx more_label = gen_label_rtx ();
+  rtx less_label = gen_label_rtx ();
+  rtx end_of_code_label = gen_label_rtx ();
+  int per_counter = GCOV_TYPE_SIZE / BITS_PER_UNIT;
 
-  /* It's either already been output, or we don't need it because we're
-     not doing profile-edges.  */
-  if (! need_func_profiler)
-    return;
+  start_sequence ();
 
-  need_func_profiler = 0;
+  if (value->seq)
+    emit_insn (value->seq);
 
-  /* Synthesize a constructor function to invoke __bb_init_func with a
-     pointer to this object file's profile block.  */
+  mr = gen_reg_rtx (Pmode);
 
-  /* Try and make a unique name given the "file function name".
+  tmp = force_reg (Pmode, base_label);
+  tmp = plus_constant (tmp, per_counter * base);
 
-     And no, I don't like this either.  */
+  val = expand_simple_binop (value->mode, MINUS,
+			     copy_rtx (value->value),
+			     GEN_INT (value->hdata.intvl.int_start),
+			     NULL_RTX, 0, OPTAB_WIDEN);
 
-  fnname = get_file_function_name ('I');
-  cfnname = IDENTIFIER_POINTER (fnname);
-  name = concat (cfnname, "GCOV", NULL);
-  fnname = get_identifier (name);
-  free (name);
+  if (value->hdata.intvl.may_be_more)
+    do_compare_rtx_and_jump (copy_rtx (val), GEN_INT (value->hdata.intvl.steps),
+			     GE, 0, value->mode, NULL_RTX, NULL_RTX, more_label);
+  if (value->hdata.intvl.may_be_less)
+    do_compare_rtx_and_jump (copy_rtx (val), const0_rtx, LT, 0, value->mode,
+			     NULL_RTX, NULL_RTX, less_label);
 
-  fndecl = build_decl (FUNCTION_DECL, fnname,
-		       build_function_type (void_type_node, NULL_TREE));
-  DECL_EXTERNAL (fndecl) = 0;
+  /* We are in range.  */
+  tmp1 = expand_simple_binop (value->mode, MULT, copy_rtx (val), GEN_INT (per_counter),
+			      NULL_RTX, 0, OPTAB_WIDEN);
+  tmp1 = expand_simple_binop (Pmode, PLUS, copy_rtx (tmp), tmp1, mr, 0, OPTAB_WIDEN);
+  if (tmp1 != mr)
+    emit_move_insn (copy_rtx (mr), tmp1);
 
-  /* It can be a static function as long as collect2 does not have
-     to scan the object file to find its ctor/dtor routine.  */
-  TREE_PUBLIC (fndecl) = ! targetm.have_ctors_dtors;
+  if (value->hdata.intvl.may_be_more
+      || value->hdata.intvl.may_be_less)
+    {
+      emit_jump_insn (gen_jump (end_of_code_label));
+      emit_barrier ();
+    }
 
-  TREE_USED (fndecl) = 1;
+  /* Above the interval.  */
+  if (value->hdata.intvl.may_be_more)
+    {
+      emit_label (more_label);
+      tmp1 = expand_simple_binop (Pmode, PLUS, copy_rtx (tmp),
+				  GEN_INT (per_counter * value->hdata.intvl.steps),
+    				  mr, 0, OPTAB_WIDEN);
+      if (tmp1 != mr)
+	emit_move_insn (copy_rtx (mr), tmp1);
+      if (value->hdata.intvl.may_be_less)
+	{
+	  emit_jump_insn (gen_jump (end_of_code_label));
+	  emit_barrier ();
+	}
+    }
 
-  DECL_RESULT (fndecl) = build_decl (RESULT_DECL, NULL_TREE, void_type_node);
+  /* Below the interval.  */
+  if (value->hdata.intvl.may_be_less)
+    {
+      emit_label (less_label);
+      tmp1 = expand_simple_binop (Pmode, PLUS, copy_rtx (tmp),
+		GEN_INT (per_counter * (value->hdata.intvl.steps
+					+ (value->hdata.intvl.may_be_more ? 1 : 0))),
+		mr, 0, OPTAB_WIDEN);
+      if (tmp1 != mr)
+	emit_move_insn (copy_rtx (mr), tmp1);
+    }
 
-  fndecl = (*lang_hooks.decls.pushdecl) (fndecl);
-  rest_of_decl_compilation (fndecl, 0, 1, 0);
-  announce_function (fndecl);
-  current_function_decl = fndecl;
-  DECL_INITIAL (fndecl) = error_mark_node;
-  make_decl_rtl (fndecl, NULL);
-  init_function_start (fndecl, input_filename, lineno);
-  (*lang_hooks.decls.pushlevel) (0);
-  expand_function_start (fndecl, 0);
-  cfun->arc_profile = 0;
+  if (value->hdata.intvl.may_be_more
+      || value->hdata.intvl.may_be_less)
+    emit_label (end_of_code_label);
 
-  /* Actually generate the code to call __bb_init_func.  */
-  ASM_GENERATE_INTERNAL_LABEL (buf, "LPBX", 0);
-  table_address = force_reg (Pmode,
-			     gen_rtx_SYMBOL_REF (Pmode, ggc_strdup (buf)));
-  emit_library_call (gen_rtx_SYMBOL_REF (Pmode, "__bb_init_func"), LCT_NORMAL,
-		     mode, 1, table_address, Pmode);
+  mem_ref = validize_mem (gen_rtx_MEM (mode, mr));
 
-  expand_function_end (input_filename, lineno, 0);
-  (*lang_hooks.decls.poplevel) (1, 0, 1);
+  tmp = expand_simple_binop (mode, PLUS, copy_rtx (mem_ref), const1_rtx,
+			     mem_ref, 0, OPTAB_WIDEN);
 
-  /* Since fndecl isn't in the list of globals, it would never be emitted
-     when it's considered to be 'safe' for inlining, so turn off
-     flag_inline_functions.  */
-  flag_inline_functions = 0;
+  if (tmp != mem_ref)
+    emit_move_insn (copy_rtx (mem_ref), tmp);
 
-  rest_of_compilation (fndecl);
+  sequence = get_insns ();
+  end_sequence ();
+  rebuild_jump_labels (sequence);
+  return sequence;
+}
 
-  /* Reset flag_inline_functions to its original value.  */
-  flag_inline_functions = save_flag_inline_functions;
+/* Output instructions as RTL to increment the range histogram counter.
+   VALUE is the expression whose value is profiled.  BASE_LABEL is the base
+   of histogram counters, BASE is offset from this position.  */
 
-  if (! quiet_flag)
-    fflush (asm_out_file);
-  current_function_decl = NULL_TREE;
+static rtx
+gen_range_profiler (value, base_label, base)
+     struct histogram_value *value;
+     rtx base_label;
+     int base;
+{
+  enum machine_mode mode = mode_for_size (GCOV_TYPE_SIZE, MODE_INT, 0);
+  rtx mem_ref, tmp, mr, uval;
+  rtx sequence;
+  rtx end_of_code_label = gen_label_rtx ();
+  int per_counter = GCOV_TYPE_SIZE / BITS_PER_UNIT, i;
 
-  if (targetm.have_ctors_dtors)
-    (* targetm.asm_out.constructor) (XEXP (DECL_RTL (fndecl), 0),
-				     DEFAULT_INIT_PRIORITY);
+  start_sequence ();
+
+  if (value->seq)
+    emit_insn (value->seq);
+
+  mr = gen_reg_rtx (Pmode);
+
+  tmp = force_reg (Pmode, base_label);
+  tmp = plus_constant (tmp, per_counter * base);
+  emit_move_insn (mr, tmp);
+
+  if (REG_P (value->value))
+    {
+      uval = value->value;
+    }
+  else
+    {
+      uval = gen_reg_rtx (value->mode);
+      emit_move_insn (uval, copy_rtx (value->value));
+    }
+
+  for (i = 0; i < value->hdata.range.n_ranges; i++)
+    {
+      do_compare_rtx_and_jump (copy_rtx (uval), GEN_INT (value->hdata.range.ranges[i]),
+			       LT, 0, value->mode, NULL_RTX,
+    			       NULL_RTX, end_of_code_label);
+      tmp = expand_simple_binop (Pmode, PLUS, copy_rtx (mr),
+				 GEN_INT (per_counter), mr, 0, OPTAB_WIDEN);
+      if (tmp != mr)
+	emit_move_insn (copy_rtx (mr), tmp);
+    }
+
+  emit_label (end_of_code_label);
+
+  mem_ref = validize_mem (gen_rtx_MEM (mode, mr));
+
+  tmp = expand_simple_binop (mode, PLUS, mem_ref, const1_rtx,
+			     mem_ref, 0, OPTAB_WIDEN);
+
+  if (tmp != mem_ref)
+    emit_move_insn (copy_rtx (mem_ref), tmp);
+
+  sequence = get_insns ();
+  end_sequence ();
+  rebuild_jump_labels (sequence);
+  return sequence;
 }
 
+/* Output instructions as RTL to increment the power of two histogram counter.
+   VALUE is the expression whose value is profiled.  BASE_LABEL is the base
+   of histogram counters, BASE is offset from this position.  */
+
+static rtx
+gen_pow2_profiler (value, base_label, base)
+     struct histogram_value *value;
+     rtx base_label;
+     int base;
+{
+  enum machine_mode mode = mode_for_size (GCOV_TYPE_SIZE, MODE_INT, 0);
+  rtx mem_ref, tmp, mr, uval;
+  rtx sequence;
+  rtx end_of_code_label = gen_label_rtx ();
+  rtx loop_label = gen_label_rtx ();
+  int per_counter = GCOV_TYPE_SIZE / BITS_PER_UNIT;
+
+  start_sequence ();
+
+  if (value->seq)
+    emit_insn (value->seq);
+
+  mr = gen_reg_rtx (Pmode);
+  tmp = force_reg (Pmode, base_label);
+  tmp = plus_constant (tmp, per_counter * base);
+  emit_move_insn (mr, tmp);
+
+  uval = gen_reg_rtx (value->mode);
+  emit_move_insn (uval, copy_rtx (value->value));
+
+  /* Check for non-power of 2.  */
+  if (value->hdata.pow2.may_be_other)
+    {
+      do_compare_rtx_and_jump (copy_rtx (uval), const0_rtx, LE, 0, value->mode,
+			       NULL_RTX, NULL_RTX, end_of_code_label);
+      tmp = expand_simple_binop (value->mode, PLUS, copy_rtx (uval),
+				 constm1_rtx, NULL_RTX, 0, OPTAB_WIDEN);
+      tmp = expand_simple_binop (value->mode, AND, copy_rtx (uval), tmp,
+				 NULL_RTX, 0, OPTAB_WIDEN);
+      do_compare_rtx_and_jump (tmp, const0_rtx, NE, 0, value->mode, NULL_RTX,
+    			       NULL_RTX, end_of_code_label);
+    }
+
+  /* Count log_2(value).  */
+  emit_label (loop_label);
+
+  tmp = expand_simple_binop (Pmode, PLUS, copy_rtx (mr), GEN_INT (per_counter), mr, 0, OPTAB_WIDEN);
+  if (tmp != mr)
+    emit_move_insn (copy_rtx (mr), tmp);
+
+  tmp = expand_simple_binop (value->mode, ASHIFTRT, copy_rtx (uval), const1_rtx,
+			     uval, 0, OPTAB_WIDEN);
+  if (tmp != uval)
+    emit_move_insn (copy_rtx (uval), tmp);
+
+  do_compare_rtx_and_jump (copy_rtx (uval), const0_rtx, NE, 0, value->mode,
+			   NULL_RTX, NULL_RTX, loop_label);
+
+  /* Increase the counter.  */
+  emit_label (end_of_code_label);
+
+  mem_ref = validize_mem (gen_rtx_MEM (mode, mr));
+
+  tmp = expand_simple_binop (mode, PLUS, copy_rtx (mem_ref), const1_rtx,
+			     mem_ref, 0, OPTAB_WIDEN);
+
+  if (tmp != mem_ref)
+    emit_move_insn (copy_rtx (mem_ref), tmp);
+
+  sequence = get_insns ();
+  end_sequence ();
+  rebuild_jump_labels (sequence);
+  return sequence;
+}
+
+/* Output instructions as RTL for code to find the most common value.
+   VALUE is the expression whose value is profiled.  BASE_LABEL is the base
+   of histogram counters, BASE is offset from this position.  */
+
+static rtx
+gen_one_value_profiler (value, base_label, base)
+     struct histogram_value *value;
+     rtx base_label;
+     int base;
+{
+  enum machine_mode mode = mode_for_size (GCOV_TYPE_SIZE, MODE_INT, 0);
+  rtx stored_value_ref, counter_ref, all_ref, stored_value, counter, all, tmp, uval;
+  rtx sequence;
+  rtx same_label = gen_label_rtx ();
+  rtx zero_label = gen_label_rtx ();
+  rtx end_of_code_label = gen_label_rtx ();
+  int per_counter = GCOV_TYPE_SIZE / BITS_PER_UNIT;
+
+  start_sequence ();
+
+  if (value->seq)
+    emit_insn (value->seq);
+
+  tmp = force_reg (Pmode, base_label);
+  stored_value = plus_constant (tmp, per_counter * base);
+  counter = plus_constant (stored_value, per_counter);
+  all = plus_constant (counter, per_counter);
+  stored_value_ref = validize_mem (gen_rtx_MEM (mode, stored_value));
+  counter_ref = validize_mem (gen_rtx_MEM (mode, counter));
+  all_ref = validize_mem (gen_rtx_MEM (mode, all));
+
+  uval = gen_reg_rtx (mode);
+  convert_move (uval, copy_rtx (value->value), 0);
+
+  /* Check if the stored value matches.  */
+  do_compare_rtx_and_jump (copy_rtx (uval), copy_rtx (stored_value_ref), EQ,
+			   0, mode, NULL_RTX, NULL_RTX, same_label);
+  
+  /* Does not match; check whether the counter is zero.  */
+  do_compare_rtx_and_jump (copy_rtx (counter_ref), const0_rtx, EQ, 0, mode,
+			   NULL_RTX, NULL_RTX, zero_label);
+
+  /* The counter is not zero yet.  */
+  tmp = expand_simple_binop (mode, PLUS, copy_rtx (counter_ref), constm1_rtx,
+			     counter_ref, 0, OPTAB_WIDEN);
+
+  if (tmp != counter_ref)
+    emit_move_insn (copy_rtx (counter_ref), tmp);
+
+  emit_jump_insn (gen_jump (end_of_code_label));
+  emit_barrier ();
+ 
+  emit_label (zero_label);
+  /* Set new value.  */
+  emit_move_insn (copy_rtx (stored_value_ref), copy_rtx (uval));
+
+  emit_label (same_label);
+  /* Increase the counter.  */
+  tmp = expand_simple_binop (mode, PLUS, copy_rtx (counter_ref), const1_rtx,
+			     counter_ref, 0, OPTAB_WIDEN);
+
+  if (tmp != counter_ref)
+    emit_move_insn (copy_rtx (counter_ref), tmp);
+  
+  emit_label (end_of_code_label);
+
+  /* Increase the counter of all executions; this seems redundant given
+     that ve have counts for edges in cfg, but it may happen that some
+     optimization will change the counts for the block (either because
+     it is unable to update them correctly, or because it will duplicate
+     the block or its part).  */
+  tmp = expand_simple_binop (mode, PLUS, copy_rtx (all_ref), const1_rtx,
+			     all_ref, 0, OPTAB_WIDEN);
+
+  if (tmp != all_ref)
+    emit_move_insn (copy_rtx (all_ref), tmp);
+  sequence = get_insns ();
+  end_sequence ();
+  rebuild_jump_labels (sequence);
+  return sequence;
+}
 #include "gt-profile.h"
--- gcc-3.3.1/gcc/profile.h.hammer-branch	2002-05-12 19:03:35.000000000 +0200
+++ gcc-3.3.1/gcc/profile.h	2003-08-05 18:22:47.000000000 +0200
@@ -21,22 +21,29 @@ Software Foundation, 59 Temple Place - S
 #ifndef GCC_PROFILE_H
 #define GCC_PROFILE_H
 
-struct profile_info
-  {
-    /* Used by final, for allocating the proper amount of storage for the
-       instrumented arc execution counts.  */
-
-    int count_instrumented_edges;
-
-    /* Used by final, for writing correct # of instrumented edges
-       in this function.  */
+/* The number of different counter sections.  */
+#define MAX_COUNTER_SECTIONS	4
 
-    int count_edges_instrumented_now;
+/* Info about number of counters in the section.  */
+struct section_info
+{
+  unsigned tag;		/* Section tag.  */
+  int present;		/* Are the data from this section read into gcc?  */
+  int n_counters;	/* Total number of counters.  */
+  int n_counters_now;	/* Number of counters in the current function.  */
+};
 
+struct profile_info
+  {
+    /* Information about numbers of counters in counter sections, for
+       allocating the storage and storing the sizes.  */
+    unsigned n_sections;
+    struct section_info section_info[MAX_COUNTER_SECTIONS];
+    
     /* Checksum of the cfg. Used for 'identification' of code.
        Used by final.  */
 
-    long current_function_cfg_checksum;
+    unsigned current_function_cfg_checksum;
 
     /* Max. value of counter in program corresponding to the profile data
        for the current function.  */
@@ -46,9 +53,10 @@ struct profile_info
     /* The number of profiles merged to form the profile data for the current
        function.  */
     int count_profiles_merged;
-
   };
 
 extern struct profile_info profile_info;
 
+struct section_info *find_counters_section	PARAMS ((unsigned));
+
 #endif
--- gcc-3.3.1/gcc/rtl.c.hammer-branch	2003-02-20 09:54:52.000000000 +0100
+++ gcc-3.3.1/gcc/rtl.c	2003-08-05 18:22:47.000000000 +0200
@@ -226,7 +226,8 @@ const char * const reg_note_name[] =
   "REG_WAS_0", "REG_RETVAL", "REG_LIBCALL", "REG_NONNEG",
   "REG_NO_CONFLICT", "REG_UNUSED", "REG_CC_SETTER", "REG_CC_USER",
   "REG_LABEL", "REG_DEP_ANTI", "REG_DEP_OUTPUT", "REG_BR_PROB",
-  "REG_NOALIAS", "REG_SAVE_AREA", "REG_BR_PRED",
+  "REG_VALUE_HISTOGRAM", "REG_NOALIAS",
+  "REG_SAVE_AREA", "REG_BR_PRED",
   "REG_FRAME_RELATED_EXPR", "REG_EH_CONTEXT", "REG_EH_REGION",
   "REG_SAVE_NOTE", "REG_MAYBE_DEAD", "REG_NORETURN",
   "REG_NON_LOCAL_GOTO", "REG_SETJMP", "REG_ALWAYS_RETURN",
--- gcc-3.3.1/gcc/rtl.h.hammer-branch	2003-05-02 03:21:07.000000000 +0200
+++ gcc-3.3.1/gcc/rtl.h	2003-08-05 18:22:47.000000000 +0200
@@ -684,6 +684,10 @@ enum reg_note
      return.  */
   REG_BR_PROB,
 
+  /* REG_VALUE_HISTOGRAM is attached to an insn before that the contained
+     histogram of a value is measured.  */
+  REG_VALUE_HISTOGRAM,
+
   /* Attached to a call insn; indicates that the call is malloc-like and
      that the pointer returned cannot alias anything else.  */
   REG_NOALIAS,
@@ -2091,6 +2095,8 @@ extern rtx expand_mult_highpart		PARAMS 
 #ifdef BUFSIZ
 extern int gcse_main			PARAMS ((rtx, FILE *));
 #endif
+/* In web.c */
+extern void web_main			PARAMS ((void));
 
 /* In global.c */
 extern void mark_elimination		PARAMS ((int, int));
@@ -2150,7 +2156,7 @@ extern int function_invariant_p		PARAMS 
 extern void init_branch_prob		PARAMS ((const char *));
 extern void branch_prob			PARAMS ((void));
 extern void end_branch_prob		PARAMS ((void));
-extern void output_func_start_profiler	PARAMS ((void));
+extern void create_profiler		PARAMS ((void));
 
 /* In reg-stack.c */
 #ifdef BUFSIZ
@@ -2255,6 +2261,7 @@ extern void init_alias_once		PARAMS ((vo
 extern void init_alias_analysis		PARAMS ((void));
 extern void end_alias_analysis		PARAMS ((void));
 extern rtx addr_side_effect_eval	PARAMS ((rtx, int, int));
+extern bool memory_modified_in_insn_p	PARAMS ((rtx, rtx));
 
 /* In sibcall.c */
 typedef enum {
--- gcc-3.3.1/gcc/rtlanal.c.hammer-branch	2003-06-27 02:50:56.000000000 +0200
+++ gcc-3.3.1/gcc/rtlanal.c	2003-08-05 18:22:47.000000000 +0200
@@ -959,6 +959,10 @@ modified_between_p (x, start, end)
   enum rtx_code code = GET_CODE (x);
   const char *fmt;
   int i, j;
+  rtx insn;
+
+  if (start == end)
+    return 0;
 
   switch (code)
     {
@@ -975,10 +979,14 @@ modified_between_p (x, start, end)
       return 1;
 
     case MEM:
-      /* If the memory is not constant, assume it is modified.  If it is
-	 constant, we still have to check the address.  */
-      if (! RTX_UNCHANGING_P (x))
+      if (RTX_UNCHANGING_P (x))
+	return 0;
+      if (modified_between_p (XEXP (x, 0), start, end))
 	return 1;
+      for (insn = NEXT_INSN (start); insn != end; insn = NEXT_INSN (insn))
+	if (memory_modified_in_insn_p (x, insn))
+	  return 1;
+      return 0;
       break;
 
     case REG:
@@ -1031,10 +1039,13 @@ modified_in_p (x, insn)
       return 1;
 
     case MEM:
-      /* If the memory is not constant, assume it is modified.  If it is
-	 constant, we still have to check the address.  */
-      if (! RTX_UNCHANGING_P (x))
+      if (RTX_UNCHANGING_P (x))
+	return 0;
+      if (modified_in_p (XEXP (x, 0), insn))
 	return 1;
+      if (memory_modified_in_insn_p (x, insn))
+	return 1;
+      return 0;
       break;
 
     case REG:
--- gcc-3.3.1/gcc/stor-layout.c.hammer-branch	2003-05-05 16:36:06.000000000 +0200
+++ gcc-3.3.1/gcc/stor-layout.c	2003-08-05 18:22:47.000000000 +0200
@@ -1537,6 +1537,46 @@ finish_record_layout (rli, free_p)
     free (rli);
 }
 
+
+/* Finish processing a builtin RECORD_TYPE type TYPE.  It's name is
+   NAME, its fields are chained in reverse on FIELDS.
+
+   If ALIGN_TYPE is non-null, it is given the same alignment as
+   ALIGN_TYPE.  */
+
+void
+finish_builtin_struct (type, name, fields, align_type)
+     tree type;
+     const char *name;
+     tree fields;
+     tree align_type;
+{
+  tree tail, next;
+
+  for (tail = NULL_TREE; fields; tail = fields, fields = next)
+    {
+      DECL_FIELD_CONTEXT (fields) = type;
+      next = TREE_CHAIN (fields);
+      TREE_CHAIN (fields) = tail;
+    }
+  TYPE_FIELDS (type) = tail;
+
+  if (align_type)
+    {
+      TYPE_ALIGN (type) = TYPE_ALIGN (align_type);
+      TYPE_USER_ALIGN (type) = TYPE_USER_ALIGN (align_type);
+    }
+
+  layout_type (type);
+#if 0 /* not yet, should get fixed properly later */
+  TYPE_NAME (type) = make_type_decl (get_identifier (name), type);
+#else
+  TYPE_NAME (type) = build_decl (TYPE_DECL, get_identifier (name), type);
+#endif
+  TYPE_STUB_DECL (type) = TYPE_NAME (type);
+  layout_decl (TYPE_NAME (type), 0);
+}
+
 /* Calculate the mode, size, and alignment for TYPE.
    For an array type, calculate the element separation as well.
    Record TYPE on the chain of permanent or temporary types
--- gcc-3.3.1/gcc/target-def.h.hammer-branch	2002-12-10 00:53:59.000000000 +0100
+++ gcc-3.3.1/gcc/target-def.h	2003-08-05 18:22:47.000000000 +0200
@@ -217,6 +217,10 @@ Foundation, 59 Temple Place - Suite 330,
    TARGET_SCHED_INIT_DFA_BUBBLES,				\
    TARGET_SCHED_DFA_BUBBLE}
 
+#define TARGET_CGRAPH_OPTIMIZE_LOCAL_FUNCTION 0
+#define TARGET_CGRAPH						\
+   {TARGET_CGRAPH_OPTIMIZE_LOCAL_FUNCTION}
+
 /* All in tree.c.  */
 #define TARGET_MERGE_DECL_ATTRIBUTES merge_decl_attributes
 #define TARGET_MERGE_TYPE_ATTRIBUTES merge_type_attributes
@@ -247,6 +251,7 @@ Foundation, 59 Temple Place - Suite 330,
 #define TARGET_INSERT_ATTRIBUTES hook_void_tree_treeptr
 #define TARGET_FUNCTION_ATTRIBUTE_INLINABLE_P hook_bool_tree_false
 #define TARGET_MS_BITFIELD_LAYOUT_P hook_bool_tree_false
+#define TARGET_CANNOT_COPY_INSN_P NULL
 
 #ifndef TARGET_IN_SMALL_DATA_P
 #define TARGET_IN_SMALL_DATA_P hook_bool_tree_false
@@ -261,6 +266,7 @@ Foundation, 59 Temple Place - Suite 330,
 {						\
   TARGET_ASM_OUT,				\
   TARGET_SCHED,					\
+  TARGET_CGRAPH,				\
   TARGET_MERGE_DECL_ATTRIBUTES,			\
   TARGET_MERGE_TYPE_ATTRIBUTES,			\
   TARGET_ATTRIBUTE_TABLE,			\
@@ -269,6 +275,7 @@ Foundation, 59 Temple Place - Suite 330,
   TARGET_INSERT_ATTRIBUTES,			\
   TARGET_FUNCTION_ATTRIBUTE_INLINABLE_P,	\
   TARGET_MS_BITFIELD_LAYOUT_P,			\
+  TARGET_CANNOT_COPY_INSN_P,			\
   TARGET_INIT_BUILTINS,				\
   TARGET_EXPAND_BUILTIN,			\
   TARGET_SECTION_TYPE_FLAGS,			\
--- gcc-3.3.1/gcc/target.h.hammer-branch	2002-12-10 00:53:59.000000000 +0100
+++ gcc-3.3.1/gcc/target.h	2003-08-05 18:22:47.000000000 +0200
@@ -217,6 +217,14 @@ struct gcc_target
     rtx (* dfa_bubble) PARAMS ((int));
   } sched;
 
+  /* Functions relating to callgraph optimizations.  */
+  struct cgraph
+  {
+    /* Called for static functions whose address is not taken,  so for instance
+       calling convention can be customized.  */
+    void (* optimize_local_function) PARAMS ((tree));
+  } cgraph;
+
   /* Given two decls, merge their attributes and return the result.  */
   tree (* merge_decl_attributes) PARAMS ((tree, tree));
 
@@ -246,6 +254,9 @@ struct gcc_target
      Microsoft Visual C++ bitfield layout rules.  */
   bool (* ms_bitfield_layout_p) PARAMS ((tree record_type));
 
+  /* True if the insn X cannot be duplicated.  */
+  bool (* cannot_copy_insn_p) PARAMS ((rtx));
+
   /* Set up target-specific built-in functions.  */
   void (* init_builtins) PARAMS ((void));
 
--- gcc-3.3.1/gcc/timevar.def.hammer-branch	2003-01-24 17:05:31.000000000 +0100
+++ gcc-3.3.1/gcc/timevar.def	2003-08-05 18:22:47.000000000 +0200
@@ -62,6 +62,7 @@ DEFTIMEVAR (TV_CSE                   , "
 DEFTIMEVAR (TV_GCSE                  , "global CSE")
 DEFTIMEVAR (TV_LOOP                  , "loop analysis")
 DEFTIMEVAR (TV_TRACER                , "tracer")
+DEFTIMEVAR (TV_WEB                   , "web")
 DEFTIMEVAR (TV_CSE2                  , "CSE 2")
 DEFTIMEVAR (TV_BRANCH_PROB           , "branch prediction")
 DEFTIMEVAR (TV_FLOW                  , "flow analysis")
--- gcc-3.3.1/gcc/toplev.c.hammer-branch	2003-07-19 11:32:44.000000000 +0200
+++ gcc-3.3.1/gcc/toplev.c	2003-08-05 18:22:47.000000000 +0200
@@ -71,6 +71,8 @@ Software Foundation, 59 Temple Place - S
 #include "target.h"
 #include "langhooks.h"
 #include "cfglayout.h"
+#include "cfgloop.h"
+#include "vpt.h"
 
 #if defined (DWARF2_UNWIND_INFO) || defined (DWARF2_DEBUGGING_INFO)
 #include "dwarf2out.h"
@@ -232,8 +234,11 @@ enum dump_file_index
   DFI_loop,
   DFI_cfg,
   DFI_bp,
+  DFI_vpt,
+  DFI_loop2,
   DFI_ce1,
   DFI_tracer,
+  DFI_web,
   DFI_cse2,
   DFI_life,
   DFI_combine,
@@ -281,8 +286,11 @@ static struct dump_file_info dump_file[D
   { "loop",	'L', 1, 0, 0 },
   { "cfg",	'f', 1, 0, 0 },
   { "bp",	'b', 1, 0, 0 },
+  { "vpt",	'V', 1, 0, 0 },
+  { "loop2",	'L', 1, 0, 0 },
   { "ce1",	'C', 1, 0, 0 },
   { "tracer",	'T', 1, 0, 0 },
+  { "web",      'Z', 0, 0, 0 },
   { "cse2",	't', 1, 0, 0 },
   { "life",	'f', 1, 0, 0 },	/* Yes, duplicate enable switch.  */
   { "combine",	'c', 1, 0, 0 },
@@ -385,6 +393,15 @@ int flag_test_coverage = 0;
 
 int flag_branch_probabilities = 0;
 
+/* Nonzero if generating or using histograms for loop iterations.  */
+int flag_loop_histograms = 0;
+
+/* Nonzero if generating or using value histograms.  */
+int flag_value_histograms = 0;
+
+/* Nonzero if value histograms should be used to optimize code.  */
+int flag_value_profile_transformations = 0;
+
 /* Nonzero if basic blocks should be reordered.  */
 
 int flag_reorder_blocks = 0;
@@ -506,13 +523,25 @@ int flag_strength_reduce = 0;
    UNROLL_MODULO) or at run-time (preconditioned to be UNROLL_MODULO) are
    unrolled.  */
 
-int flag_unroll_loops;
+int flag_old_unroll_loops;
 
 /* Nonzero enables loop unrolling in unroll.c.  All loops are unrolled.
    This is generally not a win.  */
 
+int flag_old_unroll_all_loops;
+
+/* Enables unrolling of simple loops in loop-unroll.c.  */
+int flag_unroll_loops;
+
+/* Enables unrolling of all loops in loop-unroll.c.  */
 int flag_unroll_all_loops;
 
+/* Nonzero enables loop peeling.  */
+int flag_peel_loops;
+
+/* Nonzero enables loop unswitching.  */
+int flag_unswitch_loops;
+
 /* Nonzero enables prefetch optimizations for arrays in loops.  */
 
 int flag_prefetch_loop_arrays;
@@ -621,6 +650,10 @@ int flag_syntax_only = 0;
 
 static int flag_gcse;
 
+/* Nonzero means performs web construction pass.  */
+
+static int flag_web;
+
 /* Nonzero means perform loop optimizer.  */
 
 static int flag_loop_optimize;
@@ -883,6 +916,10 @@ int flag_new_regalloc = 0;
 
 int flag_tracer = 0;
 
+/* Nonzero if we porform compilation unit at time compilation.  */
+
+int flag_unit_at_time = 0;
+
 /* Values of the -falign-* flags: how much to align labels in code.
    0 means `use default', 1 means `don't align'.
    For each variable, there is an _log variant which is the power
@@ -995,6 +1032,8 @@ static const lang_independent_options f_
    N_("Optimize sibling and tail recursive calls") },
   {"tracer", &flag_tracer, 1,
    N_("Perform superblock formation via tail duplication") },
+  {"unit-at-a-time", &flag_unit_at_time, 1,
+   N_("Compile whole compilation unit at time") },
   {"cse-follow-jumps", &flag_cse_follow_jumps, 1,
    N_("When running CSE, follow jumps to their targets") },
   {"cse-skip-blocks", &flag_cse_skip_blocks, 1,
@@ -1009,6 +1048,14 @@ static const lang_independent_options f_
    N_("Perform loop unrolling when iteration count is known") },
   {"unroll-all-loops", &flag_unroll_all_loops, 1,
    N_("Perform loop unrolling for all loops") },
+  {"old-unroll-loops", &flag_old_unroll_loops, 1,
+   N_("Perform loop unrolling when iteration count is known") },
+  {"old-unroll-all-loops", &flag_old_unroll_all_loops, 1,
+   N_("Perform loop unrolling for all loops") },
+  {"peel-loops", &flag_peel_loops, 1,
+   N_("Perform loop peeling") },
+  {"unswitch-loops", &flag_unswitch_loops, 1,
+   N_("Perform loop unswitching") },
   {"prefetch-loop-arrays", &flag_prefetch_loop_arrays, 1,
    N_("Generate prefetch instructions, if available, for arrays in loops") },
   {"move-all-movables", &flag_move_all_movables, 1,
@@ -1045,6 +1092,8 @@ static const lang_independent_options f_
    N_("Return 'short' aggregates in registers") },
   {"delayed-branch", &flag_delayed_branch, 1,
    N_("Attempt to fill delay slots of branch instructions") },
+  {"web", &flag_web, 1,
+   N_("Construct webs and split unrelated uses of single variable") },
   {"gcse", &flag_gcse, 1,
    N_("Perform the global common subexpression elimination") },
   {"gcse-lm", &flag_gcse_lm, 1,
@@ -1096,6 +1145,12 @@ static const lang_independent_options f_
    N_("Create data files needed by gcov") },
   {"branch-probabilities", &flag_branch_probabilities, 1,
    N_("Use profiling information for branch probabilities") },
+  {"loop-histograms", &flag_loop_histograms, 1,
+   N_("Insert code to measure loop histograms and/or use them") },
+  {"value-histograms", &flag_value_histograms, 1,
+   N_("Insert code to measure value histograms and/or use them") },
+  {"value-profile-transformations", &flag_value_profile_transformations, 1,
+   N_("Use value histograms to optimize code") },
   {"profile", &profile_flag, 1,
    N_("Enable basic program profiling code") },
   {"reorder-blocks", &flag_reorder_blocks, 1,
@@ -2142,14 +2197,11 @@ compile_file ()
 
   (*lang_hooks.decls.final_write_globals)();
 
-    /* This must occur after the loop to output deferred functions.  Else
-       the profiler initializer would not be emitted if all the functions
-       in this compilation unit were deferred.
-
-       output_func_start_profiler can not cause any additional functions or
-       data to need to be output, so it need not be in the deferred function
-       loop above.  */
-    output_func_start_profiler ();
+    if (profile_arc_flag)
+      /* This must occur after the loop to output deferred functions.
+         Else the profiler initializer would not be emitted if all the
+         functions in this compilation unit were deferred.  */
+      create_profiler ();
 
   /* Write out any pending weak symbol declarations.  */
 
@@ -2170,8 +2222,6 @@ compile_file ()
 
   dw2_output_indirect_constants ();
 
-  end_final (aux_base_name);
-
   if (profile_arc_flag || flag_test_coverage || flag_branch_probabilities)
     {
       timevar_push (TV_DUMP);
@@ -2931,7 +2981,7 @@ rest_of_compilation (decl)
       /* CFG is no longer maintained up-to-date.  */
       free_bb_for_insn ();
 
-      do_unroll = flag_unroll_loops ? LOOP_UNROLL : LOOP_AUTO_UNROLL;
+      do_unroll = flag_old_unroll_loops ? LOOP_UNROLL : LOOP_AUTO_UNROLL;
       do_prefetch = flag_prefetch_loop_arrays ? LOOP_PREFETCH : 0;
       if (flag_rerun_loop_opt)
 	{
@@ -2985,7 +3035,6 @@ rest_of_compilation (decl)
     mark_constant_function ();
 
   close_dump_file (DFI_cfg, print_rtl_with_bb, insns);
-
   /* Do branch profiling and static profile estimation passes.  */
   if (optimize > 0 || cfun->arc_profile || flag_branch_probabilities)
     {
@@ -2994,7 +3043,15 @@ rest_of_compilation (decl)
       timevar_push (TV_BRANCH_PROB);
       open_dump_file (DFI_bp, decl);
       if (cfun->arc_profile || flag_branch_probabilities)
-	branch_prob ();
+	{
+	  if (flag_value_histograms)
+	    {
+	      /* Mark unused registers.  This is needed to turn divmods back into
+		 corresponding divs/mods.  */
+	      life_analysis (get_insns (), NULL, PROP_DEATH_NOTES);
+	    }
+	  branch_prob ();
+	}
 
       /* Discover and record the loop depth at the head of each basic
 	 block.  The loop infrastructure does the real job for us.  */
@@ -3008,9 +3065,57 @@ rest_of_compilation (decl)
 	estimate_probability (&loops);
 
       flow_loops_free (&loops);
-      close_dump_file (DFI_bp, print_rtl_with_bb, insns);
+      close_dump_file (DFI_bp, print_rtl_with_bb, get_insns ());
       timevar_pop (TV_BRANCH_PROB);
     }
+
+  if (optimize > 0
+      && flag_branch_probabilities
+      && flag_value_histograms
+      && flag_value_profile_transformations)
+    {
+      open_dump_file (DFI_vpt, decl);
+
+      if (value_profile_transformations ())
+	cleanup_cfg (CLEANUP_EXPENSIVE);
+
+      close_dump_file (DFI_vpt, print_rtl_with_bb, get_insns ());
+    }
+  if ((cfun->arc_profile || flag_branch_probabilities)
+      && flag_value_histograms)
+    {
+      reg_scan (get_insns (), max_reg_num (), 1);
+      count_or_remove_death_notes (NULL, 1);
+    }
+
+  /* Perform loop optimalizations.  */
+  if (flag_unswitch_loops)
+    {
+      struct loops *loops;
+      timevar_push (TV_LOOP);
+      open_dump_file (DFI_loop2, decl);
+      if (rtl_dump_file)
+	dump_flow_info (rtl_dump_file);
+
+      loops = loop_optimizer_init (rtl_dump_file);
+
+      if (loops)
+	{
+	  unswitch_loops (loops);
+          loop_optimizer_finalize (loops, rtl_dump_file);
+	}
+
+      
+      cleanup_cfg (CLEANUP_EXPENSIVE);
+      delete_trivially_dead_insns (insns, max_reg_num ());
+      reg_scan (insns, max_reg_num (), 0);
+      if (rtl_dump_file)
+	dump_flow_info (rtl_dump_file);
+      close_dump_file (DFI_loop2, print_rtl_with_bb, get_insns ());
+      timevar_pop (TV_LOOP);
+      ggc_collect ();
+    }
+
   if (optimize > 0)
     {
       open_dump_file (DFI_ce1, decl);
@@ -3026,6 +3131,7 @@ rest_of_compilation (decl)
 	}
       timevar_push (TV_JUMP);
       cleanup_cfg (CLEANUP_EXPENSIVE);
+      delete_trivially_dead_insns (insns, max_reg_num ());
       reg_scan (insns, max_reg_num (), 0);
       timevar_pop (TV_JUMP);
       close_dump_file (DFI_ce1, print_rtl_with_bb, get_insns ());
@@ -3043,6 +3149,54 @@ rest_of_compilation (decl)
       timevar_pop (TV_TRACER);
     }
 
+  /* Perform loop optimalizations.  */
+  if (flag_peel_loops || flag_unroll_loops)
+    {
+      struct loops *loops;
+      timevar_push (TV_LOOP);
+      open_dump_file (DFI_loop2, decl);
+      if (rtl_dump_file)
+	dump_flow_info (rtl_dump_file);
+
+      loops = loop_optimizer_init (rtl_dump_file);
+
+      if (loops)
+	{
+	  unroll_and_peel_loops (loops,
+	      (flag_peel_loops ? UAP_PEEL : 0) |
+	      (flag_unroll_loops ? UAP_UNROLL : 0) |
+	      (flag_unroll_all_loops ? UAP_UNROLL_ALL : 0));
+
+	  loop_optimizer_finalize (loops, rtl_dump_file);
+	}
+
+      
+      cleanup_cfg (CLEANUP_EXPENSIVE);
+      delete_trivially_dead_insns (insns, max_reg_num ());
+      reg_scan (insns, max_reg_num (), 0);
+      if (rtl_dump_file)
+	dump_flow_info (rtl_dump_file);
+      close_dump_file (DFI_loop2, print_rtl_with_bb, get_insns ());
+      timevar_pop (TV_LOOP);
+      ggc_collect ();
+    }
+
+  if (flag_web)
+    {
+      open_dump_file (DFI_web, decl);
+      timevar_push (TV_WEB);
+      cleanup_cfg (CLEANUP_EXPENSIVE);
+      web_main ();
+      delete_trivially_dead_insns (get_insns (), max_reg_num ());
+      cleanup_cfg (CLEANUP_EXPENSIVE);
+      reg_scan (insns, max_reg_num (), 0);
+
+      timevar_pop (TV_WEB);
+      close_dump_file (DFI_web, print_rtl_with_bb, get_insns ());
+      reg_scan (get_insns (), max_reg_num (), 0);
+    }
+
+
   if (flag_rerun_cse_after_loop)
     {
       timevar_push (TV_CSE2);
@@ -3435,6 +3589,19 @@ rest_of_compilation (decl)
       timevar_pop (TV_IFCVT2);
     }
 
+  /* Copy propagation and if conversion possibly introduced new splittable
+     instructions.  When scheduling it is profitable to split then now.
+     When doing reg-stack it is a must as we can't split after the stack
+     registers are converted.  */
+#if defined (INSN_SCHEDULING) || defined (STACK_REGS)
+  if ((flag_if_conversion2 || flag_rename_registers || flag_cprop_registers)
+#ifndef STACK_REGS
+      && optimize > 0 && flag_schedule_insns_after_reload
+#endif
+      )
+      split_all_insns (1);
+#endif
+
 #ifdef INSN_SCHEDULING
   if (optimize > 0 && flag_schedule_insns_after_reload)
     {
@@ -3444,8 +3611,6 @@ rest_of_compilation (decl)
       /* Do control and data sched analysis again,
 	 and write some more of the results to dump file.  */
 
-      split_all_insns (1);
-
       schedule_insns (rtl_dump_file);
 
       close_dump_file (DFI_sched2, print_rtl_with_bb, insns);
@@ -4884,12 +5049,18 @@ parse_options_and_default_flags (argc, a
       flag_delete_null_pointer_checks = 1;
       flag_reorder_blocks = 1;
       flag_reorder_functions = 1;
+      flag_value_histograms = 1;
+      flag_value_profile_transformations = 1;
     }
 
   if (optimize >= 3)
     {
       flag_inline_functions = 1;
       flag_rename_registers = 1;
+      flag_tracer = 1;
+      flag_unit_at_time = 1;
+      flag_web = 1;
+      flag_unswitch_loops = 1;
     }
 
   if (optimize < 2 || optimize_size)
@@ -5055,21 +5226,38 @@ process_options ()
      be done.  */
   if (flag_unroll_all_loops)
     flag_unroll_loops = 1;
-  /* Loop unrolling requires that strength_reduction be on also.  Silently
+
+  if (flag_unroll_loops)
+    {
+      flag_old_unroll_loops = 0;
+      flag_old_unroll_all_loops = 0;
+    }
+
+  if (flag_old_unroll_all_loops)
+    flag_old_unroll_loops = 1;
+
+  /* Old loop unrolling requires that strength_reduction be on also.  Silently
      turn on strength reduction here if it isn't already on.  Also, the loop
      unrolling code assumes that cse will be run after loop, so that must
      be turned on also.  */
-  if (flag_unroll_loops)
+  if (flag_old_unroll_loops)
     {
       flag_strength_reduce = 1;
       flag_rerun_cse_after_loop = 1;
     }
+  if (flag_unroll_loops || flag_peel_loops)
+    flag_rerun_cse_after_loop = 1;
 
   if (flag_non_call_exceptions)
     flag_asynchronous_unwind_tables = 1;
   if (flag_asynchronous_unwind_tables)
     flag_unwind_tables = 1;
 
+  /* Disable unit-at-a-time mode for frontends not supporting callgraph
+     interface.  */
+  if (flag_unit_at_time && ! lang_hooks.callgraph.expand_function)
+    flag_unit_at_time = 0;
+
   /* Warn about options that are not supported on this machine.  */
 #ifndef INSN_SCHEDULING
   if (flag_schedule_insns || flag_schedule_insns_after_reload)
--- gcc-3.3.1/gcc/tracer.c.hammer-branch	2002-09-08 14:47:27.000000000 +0200
+++ gcc-3.3.1/gcc/tracer.c	2003-08-05 18:22:47.000000000 +0200
@@ -364,7 +364,7 @@ tracer ()
 {
   if (n_basic_blocks <= 1)
     return;
-  cfg_layout_initialize ();
+  cfg_layout_initialize (NULL);
   mark_dfs_back_edges ();
   if (rtl_dump_file)
     dump_flow_info (rtl_dump_file);
--- gcc-3.3.1/gcc/tree-inline.c.hammer-branch	2003-07-23 18:14:05.000000000 +0200
+++ gcc-3.3.1/gcc/tree-inline.c	2003-08-05 18:22:47.000000000 +0200
@@ -103,6 +103,8 @@ typedef struct inline_data
   /* Hash table used to prevent walk_tree from visiting the same node
      umpteen million times.  */
   htab_t tree_pruner;
+  /* Decl of function we are inlining into.  */
+  tree decl;
 } inline_data;
 
 /* Prototypes.  */
@@ -112,7 +114,7 @@ static tree copy_body_r PARAMS ((tree *,
 static tree copy_body PARAMS ((inline_data *));
 static tree expand_call_inline PARAMS ((tree *, int *, void *));
 static void expand_calls_inline PARAMS ((tree *, inline_data *));
-static int inlinable_function_p PARAMS ((tree, inline_data *));
+static int inlinable_function_p PARAMS ((tree, inline_data *, int));
 static tree remap_decl PARAMS ((tree, inline_data *));
 #ifndef INLINER_FOR_JAVA
 static tree initialize_inlined_parameters PARAMS ((inline_data *, tree, tree));
@@ -871,10 +873,11 @@ declare_return_variable (id, var)
 /* Returns nonzero if a function can be inlined as a tree.  */
 
 int
-tree_inlinable_function_p (fn)
+tree_inlinable_function_p (fn, nolimit)
      tree fn;
+     int nolimit;
 {
-  return inlinable_function_p (fn, NULL);
+  return inlinable_function_p (fn, NULL, nolimit);
 }
 
 /* If *TP is possibly call to alloca, return nonzero.  */
@@ -928,9 +931,10 @@ find_builtin_longjmp_call (exp)
    can be inlined at all.  */
 
 static int
-inlinable_function_p (fn, id)
+inlinable_function_p (fn, id, nolimit)
      tree fn;
      inline_data *id;
+     int nolimit;
 {
   int inlinable;
   int currfn_insns;
@@ -962,7 +966,7 @@ inlinable_function_p (fn, id)
      front-end that must set DECL_INLINE in this case, because
      dwarf2out loses if a function is inlined that doesn't have
      DECL_INLINE set.  */
-  else if (! DECL_INLINE (fn))
+  else if (! DECL_INLINE (fn) && !nolimit)
     ;
 #ifdef INLINER_FOR_JAVA
   /* Synchronized methods can't be inlined.  This is a bug.  */
@@ -973,6 +977,7 @@ inlinable_function_p (fn, id)
      function to be of MAX_INLINE_INSNS_SINGLE size.  Make special
      allowance for extern inline functions, though.  */
   else if (! (*lang_hooks.tree_inlining.disregard_inline_limits) (fn)
+	   && !nolimit
 	   && currfn_insns > max_inline_insns_single)
     ;
   /* We can't inline functions that call __builtin_longjmp at all.
@@ -1003,7 +1008,7 @@ inlinable_function_p (fn, id)
   /* In case we don't disregard the inlining limits and we basically
      can inline this function, investigate further.  */
   if (! (*lang_hooks.tree_inlining.disregard_inline_limits) (fn)
-      && inlinable
+      && inlinable && !nolimit
       && currfn_insns > MIN_INLINE_INSNS)
     {
       int sum_insns = (id ? id->inlined_stmts : 0) * INSNS_PER_STMT
@@ -1169,7 +1174,7 @@ expand_call_inline (tp, walk_subtrees, d
 
   /* Don't try to inline functions that are not well-suited to
      inlining.  */
-  if (!inlinable_function_p (fn, id))
+  if (!inlinable_function_p (fn, id, 0))
     {
       if (warn_inline && DECL_INLINE (fn) && !DID_INLINE_FUNC (fn)
 	  && !DECL_IN_SYSTEM_HEADER (fn))
@@ -1399,6 +1404,13 @@ expand_call_inline (tp, walk_subtrees, d
   /* For accounting, subtract one for the saved call/ret.  */
   id->inlined_stmts += DECL_NUM_STMTS (fn) - 1;
 
+  /* Update callgraph if needed.  */
+  if (id->decl && flag_unit_at_time)
+    {
+      cgraph_remove_call (id->decl, fn);
+      cgraph_create_edges (id->decl, *inlined_body);
+    }
+
   /* Recurse into the body of the just inlined function.  */
   expand_calls_inline (inlined_body, id);
   VARRAY_POP (id->fns);
@@ -1412,7 +1424,6 @@ expand_call_inline (tp, walk_subtrees, d
   *walk_subtrees = 0;
 
   (*lang_hooks.tree_inlining.end_inlining) (fn);
-
   /* Keep iterating.  */
   return NULL_TREE;
 }
@@ -1445,6 +1456,7 @@ optimize_inline_calls (fn)
   /* Clear out ID.  */
   memset (&id, 0, sizeof (id));
 
+  id.decl = fn;
   /* Don't allow recursion into FN.  */
   VARRAY_TREE_INIT (id.fns, 32, "fns");
   VARRAY_PUSH_TREE (id.fns, fn);
--- gcc-3.3.1/gcc/tree-inline.h.hammer-branch	2002-06-01 00:15:38.000000000 +0200
+++ gcc-3.3.1/gcc/tree-inline.h	2003-08-05 18:22:47.000000000 +0200
@@ -25,7 +25,7 @@ Boston, MA 02111-1307, USA.  */
 /* Function prototypes.  */
 
 void optimize_inline_calls PARAMS ((tree));
-int tree_inlinable_function_p PARAMS ((tree));
+int tree_inlinable_function_p PARAMS ((tree, int));
 tree walk_tree PARAMS ((tree*, walk_tree_fn, void*, void*));
 tree walk_tree_without_duplicates PARAMS ((tree*, walk_tree_fn, void*));
 tree copy_tree_r PARAMS ((tree*, int*, void*));
--- gcc-3.3.1/gcc/tree.h.hammer-branch	2003-06-20 23:18:42.000000000 +0200
+++ gcc-3.3.1/gcc/tree.h	2003-08-05 18:22:47.000000000 +0200
@@ -84,7 +84,7 @@ extern const char *const built_in_class_
 /* Codes that identify the various built in functions
    so that expand_call can identify them quickly.  */
 
-#define DEF_BUILTIN(ENUM, N, C, T, LT, B, F, NA, AT) ENUM,
+#define DEF_BUILTIN(ENUM, N, C, T, LT, B, F, NA, AT, IM) ENUM,
 enum built_in_function
 {
 #include "builtins.def"
@@ -99,6 +99,7 @@ extern const char *const built_in_names[
 
 /* An array of _DECL trees for the above.  */
 extern tree built_in_decls[(int) END_BUILTINS];
+extern tree implicit_built_in_decls[(int) END_BUILTINS];
 
 /* The definition of tree nodes fills the next several pages.  */
 
@@ -2435,6 +2436,12 @@ extern tree build_qualified_type        
 
 extern tree build_type_copy		PARAMS ((tree));
 
+/* Finish up a builtin RECORD_TYPE. Give it a name and provide its
+   fields. Optionally specify an alignment, and then lsy it out.  */
+
+extern void finish_builtin_struct		PARAMS ((tree, const char *,
+							 tree, tree));
+
 /* Given a ..._TYPE node, calculate the TYPE_SIZE, TYPE_SIZE_UNIT,
    TYPE_ALIGN and TYPE_MODE fields.  If called more than once on one
    node, does nothing except for the first time.  */
@@ -2919,7 +2926,12 @@ extern void rrotate_double	PARAMS ((unsi
 extern int operand_equal_p	PARAMS ((tree, tree, int));
 extern tree invert_truthvalue	PARAMS ((tree));
 
-extern tree fold_builtin		PARAMS ((tree));
+/* In builtins.c */
+extern tree fold_builtin				PARAMS ((tree));
+extern enum built_in_function builtin_mathfn_code	PARAMS ((tree));
+extern tree build_function_call_expr			PARAMS ((tree, tree));
+extern tree mathfn_built_in				PARAMS ((tree, enum built_in_function fn));
+extern tree strip_float_extensions			PARAMS ((tree));
 
 extern tree build_range_type PARAMS ((tree, tree, tree));
 
@@ -3170,6 +3182,15 @@ extern void dump_node			PARAMS ((tree, i
 extern int dump_switch_p                PARAMS ((const char *));
 extern const char *dump_flag_name	PARAMS ((enum tree_dump_index));
 
+/* In callgraph.c  */
+void cgraph_finalize_function		PARAMS ((tree, tree));
+void cgraph_finalize_compilation_unit	PARAMS ((void));
+void cgraph_create_edges		PARAMS ((tree, tree));
+void dump_cgraph			PARAMS ((FILE *));
+void cgraph_optimize			PARAMS ((void));
+void cgraph_remove_call			PARAMS ((tree, tree));
+bool cgraph_calls_p			PARAMS ((tree, tree));
+
 
 /* Redefine abort to report an internal error w/o coredump, and
    reporting the location of the error in the source file.  This logic
--- gcc-3.3.1/gcc/unroll.c.hammer-branch	2003-07-23 18:14:06.000000000 +0200
+++ gcc-3.3.1/gcc/unroll.c	2003-08-05 18:22:47.000000000 +0200
@@ -148,6 +148,7 @@ Software Foundation, 59 Temple Place - S
 #include "basic-block.h"
 #include "predict.h"
 #include "params.h"
+#include "cfgloop.h"
 
 /* The prime factors looked for when trying to unroll a loop by some
    number which is modulo the total number of iterations.  Just checking
--- gcc-3.3.1/gcc/vpt.c.hammer-branch	2003-08-05 18:22:47.000000000 +0200
+++ gcc-3.3.1/gcc/vpt.c	2003-08-05 18:22:47.000000000 +0200
@@ -0,0 +1,696 @@
+/* Transformations based on profile information for values.
+   Copyright (C) 1987, 1988, 1989, 1991, 1992, 1993, 1994, 1995, 1996, 1997,
+   1998, 1999, 2000, 2001, 2002 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+#include "config.h"
+#include "system.h"
+#include "rtl.h"
+#include "expr.h"
+#include "hard-reg-set.h"
+#include "basic-block.h"
+#include "profile.h"
+#include "vpt.h"
+#include "output.h"
+
+static void insn_values_to_profile	PARAMS ((rtx, unsigned *,
+					    	 struct histogram_value *));
+static rtx gen_divmod_fixed_value	PARAMS ((enum machine_mode,
+						 enum rtx_code,
+						 rtx, rtx, rtx, gcov_type));
+static rtx gen_mod_pow2			PARAMS ((enum machine_mode,
+						 enum rtx_code,
+						 rtx, rtx, rtx));
+static rtx gen_mod_subtract	PARAMS ((enum machine_mode,
+						 enum rtx_code,
+						 rtx, rtx, rtx, int));
+static int divmod_fixed_value_transform	PARAMS ((rtx));
+static int mod_pow2_value_transform	PARAMS ((rtx));
+static int mod_subtract_transform	PARAMS ((rtx));
+
+
+/* Release the list of values for that we want to measure histograms.  */
+void
+free_profiled_values (n_values, values)
+     unsigned n_values;
+     struct histogram_value *values;
+{
+  unsigned i;
+
+  for (i = 0; i < n_values; i++)
+    {
+      switch (values[i].type)
+	{
+	case HIST_TYPE_RANGE:
+	  free (values[i].hdata.range.ranges);
+	  break;
+
+	default:
+	  break;
+	}
+    }
+  free (values);
+}
+
+/* Find values inside INSN for that we want to measure histograms.  */
+static void
+insn_values_to_profile (insn, n_values, values)
+     rtx insn;
+     unsigned *n_values;
+     struct histogram_value *values;
+{
+  rtx set, set_src, op1, op2;
+  enum machine_mode mode;
+
+  if (!INSN_P (insn))
+    return;
+
+  set = single_set_1 (insn);
+  if (!set)
+    return;
+
+  mode = GET_MODE (SET_DEST (set));
+  if (!INTEGRAL_MODE_P (mode))
+    return;
+
+  set_src = SET_SRC (set);
+  switch (GET_CODE (set_src))
+    {
+    case DIV:
+    case MOD:
+    case UDIV:
+    case UMOD:
+      op1 = XEXP (set_src, 0);
+      op2 = XEXP (set_src, 1);
+      if (side_effects_p (op2))
+	return;
+
+      /* Check for a special case where the divisor is power of 2.  */
+      if ((GET_CODE (set_src) == UMOD) && !CONSTANT_P (op2))
+	{
+	  if (values)
+	    {
+	      values[*n_values].value = op2;
+	      values[*n_values].seq = NULL_RTX;
+	      values[*n_values].mode = mode;
+	      values[*n_values].insn = insn;
+	      values[*n_values].type = HIST_TYPE_POW2;
+	      values[*n_values].hdata.pow2.may_be_other = 1;
+	    }
+	  (*n_values)++;
+	}
+
+      /* Check whether the divisor is not in fact a constant.  */
+      if (!CONSTANT_P (op2))
+	{
+	  if (values)
+	    {
+	      values[*n_values].value = op2;
+	      values[*n_values].mode = mode;
+	      values[*n_values].seq = NULL_RTX;
+	      values[*n_values].insn = insn;
+	      values[*n_values].type = HIST_TYPE_ONE_VALUE;
+	    }
+	  (*n_values)++;
+	}
+
+      /* For mod, check whether it is not often a noop (or replacable by
+	 a few subtractions).  */
+      if (GET_CODE (set_src) == UMOD && !side_effects_p (op1))
+	{
+	  if (values)
+	    {
+	      rtx tmp;
+
+	      start_sequence ();
+	      tmp = simplify_gen_binary (DIV, mode, copy_rtx (op1), copy_rtx (op2));
+	      values[*n_values].value = force_operand (tmp, NULL_RTX);
+	      values[*n_values].seq = get_insns ();
+	      end_sequence ();
+	      values[*n_values].mode = mode;
+	      values[*n_values].insn = insn;
+	      values[*n_values].type = HIST_TYPE_INTERVAL;
+	      values[*n_values].hdata.intvl.int_start = 0;
+	      values[*n_values].hdata.intvl.steps = 2;
+	      values[*n_values].hdata.intvl.may_be_less = 1;
+	      values[*n_values].hdata.intvl.may_be_more = 1;
+	    }
+	  (*n_values)++;
+	}
+      return;
+
+    default:
+      return;
+    }
+}
+
+/* Find list of values for that we want to measure histograms.  */
+void
+find_values_to_profile (n_values, values)
+     unsigned *n_values;
+     struct histogram_value **values;
+{
+  rtx insn;
+  unsigned i;
+
+  *n_values = 0;
+  for (insn = get_insns (); insn; insn = NEXT_INSN (insn))
+    insn_values_to_profile (insn, n_values, NULL);
+  *values = xmalloc (*n_values * sizeof (struct histogram_value));
+
+  (*n_values) = 0;
+  for (insn = get_insns (); insn; insn = NEXT_INSN (insn))
+    insn_values_to_profile (insn, n_values, *values);
+
+  for (i = 0; i < *n_values; i++)
+    {
+      switch ((*values)[i].type)
+	{
+	case HIST_TYPE_INTERVAL:
+	  (*values)[i].n_counters = (*values)[i].hdata.intvl.steps +
+		  ((*values)[i].hdata.intvl.may_be_less ? 1 : 0) +
+		  ((*values)[i].hdata.intvl.may_be_more ? 1 : 0);
+	  break;
+
+	case HIST_TYPE_RANGE:
+	  (*values)[i].n_counters = (*values)[i].hdata.range.n_ranges + 1;
+	  break;
+
+	case HIST_TYPE_POW2:
+	  (*values)[i].n_counters = GET_MODE_BITSIZE ((*values)[i].mode) +
+		  ((*values)[i].hdata.pow2.may_be_other ? 1 : 0);
+	  break;
+
+	case HIST_TYPE_ONE_VALUE:
+	  (*values)[i].n_counters = 3;
+	  break;
+
+	default:
+	  abort ();
+	}
+    }
+}
+
+/* Main entry point.  Finds REG_VALUE_HISTOGRAM notes from profiler and uses
+   them to identify and exploit properties of values that are hard to analyze
+   statically.
+
+   We do following transformations:
+
+   1)
+
+   x = a / b;
+
+   where b is almost always a constant N is transformed to
+
+   if (b == N)
+     x = a / N;
+   else
+     x = a / b;
+
+   analogically with %
+
+   2)
+
+   x = a % b
+
+   where b is almost always a power of 2 and the division is unsigned
+   TODO -- handle signed case as well
+
+   if ((b & (b - 1)) == 0)
+     x = a & (b - 1);
+   else
+     x = x % b;
+
+   note that when b = 0, no error will occur and x = a; this is correct,
+   as result of such operation is undefined.
+
+   3)
+
+   x = a % b
+
+   where a is almost always less then b and the division is unsigned
+   TODO -- handle signed case as well
+
+   x = a;
+   if (x >= b)
+     x %= b;
+
+   4)
+
+   x = a % b
+
+   where a is almost always less then 2 * b and the division is unsigned
+   TODO -- handle signed case as well
+
+   x = a;
+   if (x >= b)
+     x -= b;
+   if (x >= b)
+     x %= b;
+
+   it would be possible to continue analogically for K * b for other small
+   K's, but I am not sure whether it is worth that.
+   
+   TODO:
+   
+   there are other useful cases that could be handled by a simmilar mechanism;
+   for example:
+   
+   for (i = 0; i < n; i++)
+     ...
+   
+   transform to (for constant N):
+   
+   if (n == N)
+     for (i = 0; i < N; i++)
+       ...
+   else
+     for (i = 0; i < n; i++)
+       ...
+   making unroller happy.
+
+   */
+
+int
+value_profile_transformations ()
+{
+  rtx insn, next;
+  int changed = 0;
+
+  for (insn = get_insns (); insn; insn = next)
+    {
+      next = NEXT_INSN (insn);
+
+      /* Analyze the insn.  */
+      if (!INSN_P (insn))
+	continue;
+
+      /* Scan for insn carrying a histogram.  */
+      if (!find_reg_note (insn, REG_VALUE_HISTOGRAM, 0))
+	continue;
+
+      /* Ignore cold areas -- we are growing a code.  */
+      if (!maybe_hot_bb_p (BLOCK_FOR_INSN (insn)))
+	continue;
+
+      if (rtl_dump_file)
+	{
+	  fprintf (rtl_dump_file, "Trying transformations on insn %d\n",
+		   INSN_UID (insn));
+	  print_rtl_single (rtl_dump_file, insn);
+	}
+
+      /* Transformations:  */
+      if (mod_subtract_transform (insn)
+	  || divmod_fixed_value_transform (insn)
+	  || mod_pow2_value_transform (insn))
+	changed = 1;
+    }
+
+  if (changed)
+    commit_edge_insertions ();
+
+  return changed;
+}
+
+/* Generate code for transformation 1 (with MODE and OPERATION, operands OP1
+   and OP2 whose value is expected to be VALUE and result TARGET).  */
+static rtx
+gen_divmod_fixed_value (mode, operation, target, op1, op2, value)
+     enum machine_mode mode;
+     enum rtx_code operation;
+     rtx target;
+     rtx op1;
+     rtx op2;
+     gcov_type value;
+{
+  rtx tmp, tmp1;
+  rtx neq_label = gen_label_rtx ();
+  rtx end_label = gen_label_rtx ();
+  rtx sequence;
+
+  start_sequence ();
+  
+  if (!REG_P (op2))
+    {
+      tmp = gen_reg_rtx (mode);
+      emit_move_insn (tmp, copy_rtx (op2));
+    }
+  else
+    tmp = op2;
+
+  do_compare_rtx_and_jump (tmp, GEN_INT (value), NE, 0, mode, NULL_RTX,
+			   NULL_RTX, neq_label);
+  tmp1 = simplify_gen_binary (operation, mode, copy_rtx (op1), GEN_INT (value));
+  tmp1 = force_operand (tmp1, target);
+  if (tmp1 != target)
+    emit_move_insn (copy_rtx (target), copy_rtx (tmp1));
+
+  emit_jump_insn (gen_jump (end_label));
+  emit_barrier ();
+
+  emit_label (neq_label);
+  tmp1 = simplify_gen_binary (operation, mode, copy_rtx (op1), copy_rtx (tmp));
+  tmp1 = force_operand (tmp1, target);
+  if (tmp1 != target)
+    emit_move_insn (copy_rtx (target), copy_rtx (tmp1));
+  
+  emit_label (end_label);
+
+  sequence = get_insns ();
+  end_sequence ();
+  rebuild_jump_labels (sequence);
+  return sequence;
+}
+
+/* Do transform 1) on INSN if applicable.  */
+static int
+divmod_fixed_value_transform (insn)
+     rtx insn;
+{
+  rtx set, set_src, set_dest, op1, op2, value, histogram;
+  enum rtx_code code;
+  enum machine_mode mode;
+  gcov_type val, count, all;
+  edge e;
+
+  set = single_set_1 (insn);
+  if (!set)
+    return 0;
+
+  set_src = SET_SRC (set);
+  set_dest = SET_DEST (set);
+  code = GET_CODE (set_src);
+  mode = GET_MODE (set_dest);
+  
+  if (code != DIV && code != MOD && code != UDIV && code != UMOD)
+    return 0;
+  op1 = XEXP (set_src, 0);
+  op2 = XEXP (set_src, 1);
+
+  for (histogram = REG_NOTES (insn);
+       histogram;
+       histogram = XEXP (histogram, 1))
+    if (REG_NOTE_KIND (histogram) == REG_VALUE_HISTOGRAM
+	&& XEXP (XEXP (histogram, 0), 0) == GEN_INT (HIST_TYPE_ONE_VALUE))
+      break;
+
+  if (!histogram)
+    return 0;
+
+  histogram = XEXP (XEXP (histogram, 0), 1);
+  value = XEXP (histogram, 0);
+  histogram = XEXP (histogram, 1);
+  val = INTVAL (XEXP (histogram, 0));
+  histogram = XEXP (histogram, 1);
+  count = INTVAL (XEXP (histogram, 0));
+  histogram = XEXP (histogram, 1);
+  all = INTVAL (XEXP (histogram, 0));
+
+  /* We requiere that count is at least half of all; this means
+     that for the transformation to fire the value must be constant
+     at least 50% of time (and 75% gives the garantee of usage).  */
+  if (!rtx_equal_p (op2, value) || 2 * count < all)
+    return 0;
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, "Div/mod by constant transformation on insn %d\n",
+	     INSN_UID (insn));
+
+  e = split_block (BLOCK_FOR_INSN (insn), PREV_INSN (insn));
+  delete_insn (insn);
+  
+  insert_insn_on_edge (
+	gen_divmod_fixed_value (mode, code, set_dest, op1, op2, val), e);
+
+  return 1;
+}
+
+/* Generate code for transformation 2 (with MODE and OPERATION, operands OP1
+   and OP2 and result TARGET).  */
+static rtx
+gen_mod_pow2 (mode, operation, target, op1, op2)
+     enum machine_mode mode;
+     enum rtx_code operation;
+     rtx target;
+     rtx op1;
+     rtx op2;
+{
+  rtx tmp, tmp1, tmp2, tmp3;
+  rtx neq_label = gen_label_rtx ();
+  rtx end_label = gen_label_rtx ();
+  rtx sequence;
+
+  start_sequence ();
+  
+  if (!REG_P (op2))
+    {
+      tmp = gen_reg_rtx (mode);
+      emit_move_insn (tmp, copy_rtx (op2));
+    }
+  else
+    tmp = op2;
+
+  tmp1 = expand_simple_binop (mode, PLUS, tmp, constm1_rtx, NULL_RTX,
+			      0, OPTAB_WIDEN);
+  tmp2 = expand_simple_binop (mode, AND, tmp, tmp1, NULL_RTX,
+			      0, OPTAB_WIDEN);
+  do_compare_rtx_and_jump (tmp2, const0_rtx, NE, 0, mode, NULL_RTX,
+			   NULL_RTX, neq_label);
+  tmp3 = expand_simple_binop (mode, AND, op1, tmp1, target,
+			      0, OPTAB_WIDEN);
+  if (tmp3 != target)
+    emit_move_insn (copy_rtx (target), tmp3);
+  emit_jump_insn (gen_jump (end_label));
+  emit_barrier ();
+
+  emit_label (neq_label);
+  tmp1 = simplify_gen_binary (operation, mode, copy_rtx (op1), copy_rtx (tmp));
+  tmp1 = force_operand (tmp1, target);
+  if (tmp1 != target)
+    emit_move_insn (target, tmp1);
+  
+  emit_label (end_label);
+
+  sequence = get_insns ();
+  end_sequence ();
+  rebuild_jump_labels (sequence);
+  return sequence;
+}
+
+/* Do transform 2) on INSN if applicable.  */
+static int
+mod_pow2_value_transform (insn)
+     rtx insn;
+{
+  rtx set, set_src, set_dest, op1, op2, value, histogram;
+  enum rtx_code code;
+  enum machine_mode mode;
+  gcov_type wrong_values, count;
+  edge e;
+  int i;
+
+  set = single_set_1 (insn);
+  if (!set)
+    return 0;
+
+  set_src = SET_SRC (set);
+  set_dest = SET_DEST (set);
+  code = GET_CODE (set_src);
+  mode = GET_MODE (set_dest);
+  
+  if (code != UMOD)
+    return 0;
+  op1 = XEXP (set_src, 0);
+  op2 = XEXP (set_src, 1);
+
+  for (histogram = REG_NOTES (insn);
+       histogram;
+       histogram = XEXP (histogram, 1))
+    if (REG_NOTE_KIND (histogram) == REG_VALUE_HISTOGRAM
+	&& XEXP (XEXP (histogram, 0), 0) == GEN_INT (HIST_TYPE_POW2))
+      break;
+
+  if (!histogram)
+    return 0;
+
+  histogram = XEXP (XEXP (histogram, 0), 1);
+  value = XEXP (histogram, 0);
+  histogram = XEXP (histogram, 1);
+  wrong_values =INTVAL (XEXP (histogram, 0));
+  histogram = XEXP (histogram, 1);
+
+  count = 0;
+  for (i = 0; i < GET_MODE_BITSIZE (mode); i++)
+    {
+      count += INTVAL (XEXP (histogram, 0));
+      histogram = XEXP (histogram, 1);
+    }
+
+  if (!rtx_equal_p (op2, value))
+    return 0;
+
+  /* We require that we hit a power of two at least half of all evaluations.  */
+  if (count < wrong_values)
+    return 0;
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, "Mod power of 2 transformation on insn %d\n",
+	     INSN_UID (insn));
+
+  e = split_block (BLOCK_FOR_INSN (insn), PREV_INSN (insn));
+  delete_insn (insn);
+  
+  insert_insn_on_edge (
+	gen_mod_pow2 (mode, code, set_dest, op1, op2), e);
+
+  return 1;
+}
+
+/* Generate code for transformations 3 and 4 (with MODE and OPERATION,
+   operands OP1 and OP2, result TARGET and at most SUB subtractions).  */
+static rtx
+gen_mod_subtract (mode, operation, target, op1, op2, sub)
+     enum machine_mode mode;
+     enum rtx_code operation;
+     rtx target;
+     rtx op1;
+     rtx op2;
+     int sub;
+{
+  rtx tmp, tmp1;
+  rtx end_label = gen_label_rtx ();
+  rtx sequence;
+  int i;
+
+  start_sequence ();
+  
+  if (!REG_P (op2))
+    {
+      tmp = gen_reg_rtx (mode);
+      emit_move_insn (tmp, copy_rtx (op2));
+    }
+  else
+    tmp = op2;
+
+  emit_move_insn (target, copy_rtx (op1));
+  do_compare_rtx_and_jump (target, tmp, LTU, 0, mode, NULL_RTX,
+			   NULL_RTX, end_label);
+  
+
+  for (i = 0; i < sub; i++)
+    {
+      tmp1 = expand_simple_binop (mode, MINUS, target, tmp, target,
+	    			  0, OPTAB_WIDEN);
+      if (tmp1 != target)
+	emit_move_insn (target, tmp1);
+      do_compare_rtx_and_jump (target, tmp, LTU, 0, mode, NULL_RTX,
+    			       NULL_RTX, end_label);
+    }
+
+  tmp1 = simplify_gen_binary (operation, mode, copy_rtx (target), copy_rtx (tmp));
+  tmp1 = force_operand (tmp1, target);
+  if (tmp1 != target)
+    emit_move_insn (target, tmp1);
+  
+  emit_label (end_label);
+
+  sequence = get_insns ();
+  end_sequence ();
+  rebuild_jump_labels (sequence);
+  return sequence;
+}
+
+/* Do transforms 3) and 4) on INSN if applicable.  */
+static int
+mod_subtract_transform (insn)
+     rtx insn;
+{
+  rtx set, set_src, set_dest, op1, op2, value, histogram;
+  enum rtx_code code;
+  enum machine_mode mode;
+  gcov_type wrong_values, counts[2], count, all;
+  edge e;
+  int i;
+
+  set = single_set_1 (insn);
+  if (!set)
+    return 0;
+
+  set_src = SET_SRC (set);
+  set_dest = SET_DEST (set);
+  code = GET_CODE (set_src);
+  mode = GET_MODE (set_dest);
+  
+  if (code != UMOD)
+    return 0;
+  op1 = XEXP (set_src, 0);
+  op2 = XEXP (set_src, 1);
+
+  for (histogram = REG_NOTES (insn);
+       histogram;
+       histogram = XEXP (histogram, 1))
+    if (REG_NOTE_KIND (histogram) == REG_VALUE_HISTOGRAM
+	&& XEXP (XEXP (histogram, 0), 0) == GEN_INT (HIST_TYPE_INTERVAL))
+      break;
+
+  if (!histogram)
+    return 0;
+
+  histogram = XEXP (XEXP (histogram, 0), 1);
+  value = XEXP (histogram, 0);
+  histogram = XEXP (histogram, 1);
+
+  all = 0;
+  for (i = 0; i < 2; i++)
+    {
+      counts[i] = INTVAL (XEXP (histogram, 0));
+      all += counts[i];
+      histogram = XEXP (histogram, 1);
+    }
+  wrong_values = INTVAL (XEXP (histogram, 0));
+  histogram = XEXP (histogram, 1);
+  wrong_values += INTVAL (XEXP (histogram, 0));
+  all += wrong_values;
+
+  /* We require that we use just subtractions in at least 50% of all evaluations.  */
+  count = 0;
+  for (i = 0; i < 2; i++)
+    {
+      count += counts[i];
+      if (count * 2 >= all)
+	break;
+    }
+  
+  if (i == 2)
+    return 0;
+
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, "Mod subtract transformation on insn %d\n",
+	     INSN_UID (insn));
+
+  e = split_block (BLOCK_FOR_INSN (insn), PREV_INSN (insn));
+  delete_insn (insn);
+  
+  insert_insn_on_edge (
+	gen_mod_subtract (mode, code, set_dest, op1, op2, i), e);
+
+  return 1;
+}
--- gcc-3.3.1/gcc/vpt.h.hammer-branch	2003-08-05 18:22:47.000000000 +0200
+++ gcc-3.3.1/gcc/vpt.h	2003-08-05 18:22:47.000000000 +0200
@@ -0,0 +1,68 @@
+/* Definitions for transformations based on profile information for values.
+   Copyright (C) 1987, 1988, 1989, 1991, 1992, 1993, 1994, 1995, 1996, 1997,
+   1998, 1999, 2000, 2001, 2002 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+/* Supported histogram types.  */
+enum hist_type
+{
+  HIST_TYPE_INTERVAL,	/* Measures histogram of values inside a specified
+			   interval.  */
+  HIST_TYPE_RANGE,	/* Histogram of membership into one of specified
+			   ranges.  */
+  HIST_TYPE_POW2,	/* Histogram of power of 2 values.  */
+  HIST_TYPE_ONE_VALUE	/* Tries to identify the value that is (almost)
+			   always constant.  */
+};
+
+/* The value to measure.  */
+struct histogram_value
+{
+  rtx value;		/* The value.  */
+  enum machine_mode mode; /* And its mode.  */
+  rtx seq;		/* Insns requiered to count the value.  */
+  rtx insn;		/* Insn before that to measure.  */
+  enum hist_type type;	/* Type of histogram to measure.  */
+  unsigned n_counters;	/* Number of requiered counters.  */
+  union
+    {
+      struct
+	{
+	  int int_start;	/* First value in interval.  */
+	  int steps;		/* Number of values in it.  */
+	  int may_be_less;	/* May the value be below?  */
+	  int may_be_more;	/* Or above.  */
+	} intvl;	/* Interval histogram data.  */
+      struct
+	{
+	  int n_ranges;		/* Number of separating points.  */
+	  int *ranges;		/* Separating points.  */
+	} range;	/* Range histogram data.  */
+      struct
+	{
+	  int may_be_other;	/* If the value may be non-positive or not 2^k.  */
+	} pow2;		/* Power of 2 histogram data.  */
+    } hdata;		/* Histogram data.  */
+};
+
+extern int value_profile_transformations PARAMS ((void));
+extern void find_values_to_profile 	PARAMS ((unsigned *,
+						 struct histogram_value **));
+extern void free_profiled_values	PARAMS ((unsigned,
+						 struct histogram_value *));
--- gcc-3.3.1/gcc/web.c.hammer-branch	2003-08-05 18:22:47.000000000 +0200
+++ gcc-3.3.1/gcc/web.c	2003-08-05 18:22:47.000000000 +0200
@@ -0,0 +1,320 @@
+/* Web construction code for GNU compiler.
+   Contributed by Jan Hubicka
+   Copyright (C) 2001, 2002 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+/* Simple optimization pass that splits indepdendent uses of each pseudo
+   increasing effectivity of other optimizations.  The optimization can
+   serve as an example of the use of dataflow module.
+
+   We don't split registers with REG_USERVAR set unless -fmessy-debugging is
+   used, because debug information about such split variables is almost
+   useless.  
+
+   TODO
+    - Add code to keep debugging up-to-date after splitting of user variable
+      pseudos.  This can be done by remembering all the pseudos used for the
+      variable and use life analysis information before reload to determing
+      wich one of the possible choices is alive and in case more are live,
+      choose one with latest definition.
+
+      Some other optimization passes will benefit from the infrastructure
+      too.
+
+    - We may use profile information and ignore infrequent use for purposes
+      of web unifying inserting the compensation code later to implement full
+      induction variable expansion for loops (currently we expand only if
+      induction is dead afterwards, that is often the case anyway).  */
+
+#include "config.h"
+#include "system.h"
+#include "toplev.h"
+
+#include "rtl.h"
+#include "hard-reg-set.h"
+#include "flags.h"
+#include "basic-block.h"
+#include "output.h"
+#include "df.h"
+#include "function.h"
+
+
+/* This entry is allocated for each reference in the insn stream.  */
+struct web_entry
+{
+    /* pointer to the parent in the union/find tree.  */
+  struct web_entry *pred;
+    /* Newly assigned register to the entry.  Set only for roots.  */
+  rtx reg;
+};
+
+static struct web_entry *unionfind_root PARAMS ((struct web_entry *));
+static void unionfind_union		PARAMS ((struct web_entry *,
+						 struct web_entry *));
+static void union_defs			PARAMS ((struct df *, struct ref *,
+						 struct web_entry *,
+						 struct web_entry *));
+static rtx entry_register		PARAMS ((struct web_entry *,
+						 struct ref *, char *, char *));
+static void replace_ref			PARAMS ((struct ref *, rtx));
+static int mark_addressof		PARAMS ((rtx *, void *));
+
+/* Find the root of unionfind tree (the representatnt of set).  */
+
+static struct web_entry *
+unionfind_root (element)
+     struct web_entry *element;
+{
+  struct web_entry *element1 = element, *element2;
+
+  while (element->pred)
+    element = element->pred;
+  while (element1->pred)
+    {
+      element2 = element1->pred;
+      element1->pred = element;
+      element1 = element2;
+    }
+  return element;
+}
+
+/* Union sets.  */
+
+static void
+unionfind_union (first, second)
+     struct web_entry *first, *second;
+{
+  first = unionfind_root (first);
+  second = unionfind_root (second);
+  if (first == second)
+    return;
+  second->pred = first;
+}
+
+/* For each use, all possible defs reaching it must come in same register,
+   union them.  */
+
+static void
+union_defs (df, use, def_entry, use_entry)
+     struct df *df;
+     struct ref *use;
+     struct web_entry *def_entry;
+     struct web_entry *use_entry;
+{
+  rtx insn = DF_REF_INSN (use);
+  struct df_link *link = DF_REF_CHAIN (use);
+  struct df_link *use_link = DF_INSN_USES (df, insn);
+  struct df_link *def_link = DF_INSN_DEFS (df, insn);
+  rtx set = single_set (insn);
+
+  /* Some instructions may use match_dup for it's operands.  In case the
+     operands are dead, we will assign them different pseudos creating
+     invalid instruction, so union all uses of the same operands for each
+     insn.  */
+
+  while (use_link)
+    {
+      if (use != use_link->ref
+	  && DF_REF_REAL_REG (use) == DF_REF_REAL_REG (use_link->ref))
+	unionfind_union (use_entry + DF_REF_ID (use),
+		         use_entry + DF_REF_ID (use_link->ref));
+      use_link = use_link->next;
+    }
+
+  /* Recognize trivial noop moves and attempt to keep them noop.
+     While most of noop moves should be removed we still keep some at
+     libcall boundaries and such.  */
+
+  if (set
+      && SET_SRC (set) == DF_REF_REG (use)
+      && SET_SRC (set) == SET_DEST (set))
+    {
+      while (def_link)
+	{
+	  if (DF_REF_REAL_REG (use) == DF_REF_REAL_REG (def_link->ref))
+	    unionfind_union (use_entry + DF_REF_ID (use),
+			     def_entry + DF_REF_ID (def_link->ref));
+	  def_link = def_link->next;
+	}
+    }
+  while (link)
+    {
+      unionfind_union (use_entry + DF_REF_ID (use),
+		       def_entry + DF_REF_ID (link->ref));
+      link = link->next;
+    }
+
+  /* An READ_WRITE use require the corresponding def to be in the same
+     register.  Find it and union.  */
+  if (use->flags & DF_REF_READ_WRITE)
+    {
+      struct df_link *link = DF_INSN_DEFS (df, DF_REF_INSN (use));
+
+      while (DF_REF_REAL_REG (link->ref) != DF_REF_REAL_REG (use))
+	link = link->next;
+
+      unionfind_union (use_entry + DF_REF_ID (use),
+		       def_entry + DF_REF_ID (link->ref));
+    }
+}
+
+/* Find corresponding register for given entry.  */
+
+static rtx
+entry_register (entry, ref, used, use_addressof)
+     struct web_entry *entry;
+     struct ref *ref;
+     char *used;
+     char *use_addressof;
+{
+  struct web_entry *root;
+  rtx reg, newreg;
+
+  /* Find corresponding web and see if it has been visited.  */
+
+  root = unionfind_root (entry);
+  if (root->reg)
+    return root->reg;
+
+  /* We are seeing this web first time, do the assignment.  */
+
+  reg = DF_REF_REAL_REG (ref);
+
+  /* In case the original register is already assigned, generate new one.  */
+  if (!used[REGNO (reg)])
+    newreg = reg, used[REGNO (reg)] = 1;
+  else if (REG_USERVAR_P (reg) && 0/*&& !flag_messy_debugging*/)
+    {
+      newreg = reg;
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file,
+		 "New web forced to keep reg=%i (user variable)\n",
+		 REGNO (reg));
+    }
+  else if (use_addressof [REGNO (reg)])
+    {
+      newreg = reg;
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file,
+		 "New web forced to keep reg=%i (address taken)\n",
+		 REGNO (reg));
+    }
+  else
+    {
+      newreg = gen_reg_rtx (GET_MODE (reg));
+      REG_USERVAR_P (newreg) = REG_USERVAR_P (reg);
+      REG_POINTER (newreg) = REG_POINTER (reg);
+      REG_LOOP_TEST_P (newreg) = REG_LOOP_TEST_P (reg);
+      RTX_UNCHANGING_P (newreg) = RTX_UNCHANGING_P (reg);
+      if (rtl_dump_file)
+	fprintf (rtl_dump_file, "Web oldreg=%i newreg=%i\n", REGNO (reg),
+		 REGNO (newreg));
+    }
+
+  root->reg = newreg;
+  return newreg;
+}
+
+/* Replace the reference by REG.  */
+
+static void
+replace_ref (ref, reg)
+   struct ref *ref;
+   rtx reg;
+{
+  rtx oldreg = DF_REF_REAL_REG (ref);
+  rtx *loc = DF_REF_REAL_LOC (ref);
+
+  if (oldreg == reg)
+    return;
+  if (rtl_dump_file)
+    fprintf (rtl_dump_file, "Updating insn %i (%i->%i)\n",
+	     INSN_UID (DF_REF_INSN (ref)), REGNO (oldreg), REGNO (reg)); 
+  *loc = reg;
+}
+
+/* Mark each pseudo, whose address is taken.  */
+
+static int
+mark_addressof (rtl, data)
+     rtx *rtl;
+     void *data;
+{
+  if (!*rtl)
+    return 0;
+  if (GET_CODE (*rtl) == ADDRESSOF
+      && REG_P (XEXP (*rtl, 0)))
+    ((char *)data)[REGNO (XEXP (*rtl, 0))] = 1;
+  return 0;
+}
+
+/* Main entry point.  */
+
+void
+web_main ()
+{
+  struct df *df;
+  struct web_entry *def_entry;
+  struct web_entry *use_entry;
+  unsigned int i;
+  int max = max_reg_num ();
+  char *used;
+  char *use_addressof;
+  rtx insn;
+
+  df = df_init ();
+  df_analyse (df, 0, DF_UD_CHAIN | DF_EQUIV_NOTES);
+
+  def_entry =
+    (struct web_entry *) xcalloc (df->n_defs, sizeof (struct web_entry));
+  use_entry =
+    (struct web_entry *) xcalloc (df->n_uses, sizeof (struct web_entry));
+  used = (char *) xcalloc (max, sizeof (char));
+  use_addressof = (char *) xcalloc (max, sizeof (char));
+
+  if (rtl_dump_file)
+    df_dump (df, DF_UD_CHAIN | DF_DU_CHAIN, rtl_dump_file);
+
+  /* Produce the web.  */
+  for (i = 0; i < df->n_uses; i++)
+    union_defs (df, df->uses[i], def_entry, use_entry);
+
+  /* We can not safely rename registers whose address is taken.  */
+  for (insn = get_insns (); insn; insn = NEXT_INSN (insn))
+    if (INSN_P (insn))
+      for_each_rtx (&PATTERN (insn), mark_addressof, use_addressof);
+
+  /* Update the instruction stream, allocating new registers for split pseudos
+     in progress.  */
+  for (i = 0; i < df->n_uses; i++)
+    replace_ref (df->uses[i], entry_register (use_entry + i, df->uses[i],
+					      used, use_addressof));
+  for (i = 0; i < df->n_defs; i++)
+    replace_ref (df->defs[i], entry_register (def_entry + i, df->defs[i],
+					      used, use_addressof));
+
+  /* Dataflow information is corrupt here, but it can be easy to update it
+     by creating new entries for new registers and update or calilng
+     df_insns_modify.  */
+  free (def_entry);
+  free (use_entry);
+  free (used);
+  free (use_addressof);
+  df_finish (df);
+}
--- gcc-3.3.1/libf2c/libI77/config.h.in.hammer-branch	2003-07-16 10:27:05.000000000 +0200
+++ gcc-3.3.1/libf2c/libI77/config.h.in	2003-08-05 18:22:47.000000000 +0200
@@ -1,62 +1,103 @@
-/* config.h.in.  Generated automatically from configure.in by autoheader.  */
+/* config.h.in.  Generated from configure.in by autoheader.  */
 
-/* Define to empty if the keyword does not work.  */
-#undef const
-
-/* Define to `long' if <sys/types.h> doesn't define.  */
-#undef off_t
-
-/* Define to `unsigned' if <sys/types.h> doesn't define.  */
-#undef size_t
-
-/* Define if you have the ANSI C header files.  */
-#undef STDC_HEADERS
-
-/* Define if you have the fseeko function.  */
+/* Define to 1 if you have the `fseeko' function. */
 #undef HAVE_FSEEKO
 
-/* Define if you have the ftello function.  */
+/* Define to 1 if you have the `ftello' function. */
 #undef HAVE_FTELLO
 
-/* Define if you have the ftruncate function.  */
+/* Define to 1 if you have the `ftruncate' function. */
 #undef HAVE_FTRUNCATE
 
-/* Define if you have the mkstemp function.  */
+/* Define to 1 if you have the <inttypes.h> header file. */
+#undef HAVE_INTTYPES_H
+
+/* Define to 1 if you have the <memory.h> header file. */
+#undef HAVE_MEMORY_H
+
+/* Define to 1 if you have the `mkstemp' function. */
 #undef HAVE_MKSTEMP
 
-/* Define if you have the tempnam function.  */
-#undef HAVE_TEMPNAM
+/* Define to 1 if you have the <stdint.h> header file. */
+#undef HAVE_STDINT_H
 
-/* Define if you have the tmpnam function.  */
-#undef HAVE_TMPNAM
+/* Define to 1 if you have the <stdlib.h> header file. */
+#undef HAVE_STDLIB_H
 
-/* Get Single Unix Specification semantics */
-#undef _XOPEN_SOURCE
+/* Define to 1 if you have the <strings.h> header file. */
+#undef HAVE_STRINGS_H
 
-/* Get Single Unix Specification semantics */
-#undef _XOPEN_SOURCE_EXTENDED
+/* Define to 1 if you have the <string.h> header file. */
+#undef HAVE_STRING_H
 
-/* Solaris extensions */
-#undef __EXTENSIONS__
+/* Define to 1 if you have the <sys/stat.h> header file. */
+#undef HAVE_SYS_STAT_H
 
-/* Get 64-bit file size support */
-#undef _FILE_OFFSET_BITS
+/* Define to 1 if you have the <sys/types.h> header file. */
+#undef HAVE_SYS_TYPES_H
 
-/* Define for HP-UX ftello and fseeko extension. */
-#undef _LARGEFILE_SOURCE
+/* Define to 1 if you have the `tempnam' function. */
+#undef HAVE_TEMPNAM
 
-/* Define if we do not have Unix Stdio. */
-#undef NON_UNIX_STDIO
+/* Define to 1 if you have the `tmpnam' function. */
+#undef HAVE_TMPNAM
 
-/* Define if we use strlen. */
-#undef USE_STRLEN
+/* Define to 1 if you have the <unistd.h> header file. */
+#undef HAVE_UNISTD_H
 
 /* Define if we have non ANSI RW modes. */
 #undef NON_ANSI_RW_MODES
 
+/* Define if we do not have Unix Stdio. */
+#undef NON_UNIX_STDIO
+
 /* Always defined. */
 #undef NO_EOF_CHAR_CHECK
 
+/* Define to the address where bug reports for this package should be sent. */
+#undef PACKAGE_BUGREPORT
+
+/* Define to the full name of this package. */
+#undef PACKAGE_NAME
+
+/* Define to the full name and version of this package. */
+#undef PACKAGE_STRING
+
+/* Define to the one symbol short name of this package. */
+#undef PACKAGE_TARNAME
+
+/* Define to the version of this package. */
+#undef PACKAGE_VERSION
+
+/* Define to 1 if you have the ANSI C header files. */
+#undef STDC_HEADERS
+
 /* Define to skip f2c undefs. */
 #undef Skip_f2c_Undefs
 
+/* Define if we use strlen. */
+#undef USE_STRLEN
+
+/* Get 64-bit file size support */
+#undef _FILE_OFFSET_BITS
+
+/* Define for HP-UX ftello and fseeko extension. */
+#undef _LARGEFILE_SOURCE
+
+/* Get Single Unix Specification semantics */
+#undef _XOPEN_SOURCE
+
+/* Get Single Unix Specification semantics */
+#undef _XOPEN_SOURCE_EXTENDED
+
+/* Solaris extensions */
+#undef __EXTENSIONS__
+
+/* Define to empty if `const' does not conform to ANSI C. */
+#undef const
+
+/* Define to `long' if <sys/types.h> does not define. */
+#undef off_t
+
+/* Define to `unsigned' if <sys/types.h> does not define. */
+#undef size_t
--- gcc-3.3.1/libf2c/libU77/config.hin.hammer-branch	2002-06-01 03:53:53.000000000 +0200
+++ gcc-3.3.1/libf2c/libU77/config.hin	2003-08-05 18:22:47.000000000 +0200
@@ -1,4 +1,4 @@
-/* config.hin.  Generated automatically from configure.in by autoheader.  */
+/* config.hin.  Generated automatically from configure.in by autoheader 2.13.  */
 
 /* Define to empty if the keyword does not work.  */
 #undef const
--- gcc-3.3.1/libf2c/libU77/configure.hammer-branch	2003-04-24 03:58:04.000000000 +0200
+++ gcc-3.3.1/libf2c/libU77/configure	2003-08-05 18:22:47.000000000 +0200
@@ -28,7 +28,6 @@ program_suffix=NONE
 program_transform_name=s,x,x,
 silent=
 site=
-sitefile=
 srcdir=
 target=NONE
 verbose=
@@ -143,7 +142,6 @@ Configuration:
   --help                  print this message
   --no-create             do not create output files
   --quiet, --silent       do not print \`checking...' messages
-  --site-file=FILE        use FILE as the site file
   --version               print the version of autoconf that created configure
 Directory and file names:
   --prefix=PREFIX         install architecture-independent files in PREFIX
@@ -314,11 +312,6 @@ EOF
   -site=* | --site=* | --sit=*)
     site="$ac_optarg" ;;
 
-  -site-file | --site-file | --site-fil | --site-fi | --site-f)
-    ac_prev=sitefile ;;
-  -site-file=* | --site-file=* | --site-fil=* | --site-fi=* | --site-f=*)
-    sitefile="$ac_optarg" ;;
-
   -srcdir | --srcdir | --srcdi | --srcd | --src | --sr)
     ac_prev=srcdir ;;
   -srcdir=* | --srcdir=* | --srcdi=* | --srcd=* | --src=* | --sr=*)
@@ -484,16 +477,12 @@ fi
 srcdir=`echo "${srcdir}" | sed 's%\([^/]\)/*$%\1%'`
 
 # Prefer explicitly selected file to automatically selected ones.
-if test -z "$sitefile"; then
-  if test -z "$CONFIG_SITE"; then
-    if test "x$prefix" != xNONE; then
-      CONFIG_SITE="$prefix/share/config.site $prefix/etc/config.site"
-    else
-      CONFIG_SITE="$ac_default_prefix/share/config.site $ac_default_prefix/etc/config.site"
-    fi
+if test -z "$CONFIG_SITE"; then
+  if test "x$prefix" != xNONE; then
+    CONFIG_SITE="$prefix/share/config.site $prefix/etc/config.site"
+  else
+    CONFIG_SITE="$ac_default_prefix/share/config.site $ac_default_prefix/etc/config.site"
   fi
-else
-  CONFIG_SITE="$sitefile"
 fi
 for ac_site_file in $CONFIG_SITE; do
   if test -r "$ac_site_file"; then
@@ -541,7 +530,7 @@ fi
 # Extract the first word of "gcc", so it can be a program name with args.
 set dummy gcc; ac_word=$2
 echo $ac_n "checking for $ac_word""... $ac_c" 1>&6
-echo "configure:545: checking for $ac_word" >&5
+echo "configure:534: checking for $ac_word" >&5
 if eval "test \"`echo '$''{'ac_cv_prog_CC'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
@@ -571,7 +560,7 @@ if test -z "$CC"; then
   # Extract the first word of "cc", so it can be a program name with args.
 set dummy cc; ac_word=$2
 echo $ac_n "checking for $ac_word""... $ac_c" 1>&6
-echo "configure:575: checking for $ac_word" >&5
+echo "configure:564: checking for $ac_word" >&5
 if eval "test \"`echo '$''{'ac_cv_prog_CC'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
@@ -622,7 +611,7 @@ fi
       # Extract the first word of "cl", so it can be a program name with args.
 set dummy cl; ac_word=$2
 echo $ac_n "checking for $ac_word""... $ac_c" 1>&6
-echo "configure:626: checking for $ac_word" >&5
+echo "configure:615: checking for $ac_word" >&5
 if eval "test \"`echo '$''{'ac_cv_prog_CC'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
@@ -655,7 +644,7 @@ fi
 
 
 echo $ac_n "checking whether we are using GNU C""... $ac_c" 1>&6
-echo "configure:659: checking whether we are using GNU C" >&5
+echo "configure:648: checking whether we are using GNU C" >&5
 if eval "test \"`echo '$''{'ac_cv_prog_gcc'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
@@ -664,7 +653,7 @@ else
   yes;
 #endif
 EOF
-if { ac_try='${CC-cc} -E conftest.c'; { (eval echo configure:668: \"$ac_try\") 1>&5; (eval $ac_try) 2>&5; }; } | egrep yes >/dev/null 2>&1; then
+if { ac_try='${CC-cc} -E conftest.c'; { (eval echo configure:657: \"$ac_try\") 1>&5; (eval $ac_try) 2>&5; }; } | egrep yes >/dev/null 2>&1; then
   ac_cv_prog_gcc=yes
 else
   ac_cv_prog_gcc=no
@@ -683,7 +672,7 @@ ac_test_CFLAGS="${CFLAGS+set}"
 ac_save_CFLAGS="$CFLAGS"
 CFLAGS=
 echo $ac_n "checking whether ${CC-cc} accepts -g""... $ac_c" 1>&6
-echo "configure:687: checking whether ${CC-cc} accepts -g" >&5
+echo "configure:676: checking whether ${CC-cc} accepts -g" >&5
 if eval "test \"`echo '$''{'ac_cv_prog_cc_g'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
@@ -719,9 +708,9 @@ fi
 # NetBSD 1.4 header files does not support XOPEN_SOURCE == 600, but it
 # handles 64-bit file sizes without needing these defines.
 echo $ac_n "checking whether _XOPEN_SOURCE may be defined""... $ac_c" 1>&6
-echo "configure:723: checking whether _XOPEN_SOURCE may be defined" >&5
+echo "configure:712: checking whether _XOPEN_SOURCE may be defined" >&5
 cat > conftest.$ac_ext <<EOF
-#line 725 "configure"
+#line 714 "configure"
 #include "confdefs.h"
 #define _XOPEN_SOURCE 600L
 #include <unistd.h>
@@ -729,7 +718,7 @@ int main() {
 
 ; return 0; }
 EOF
-if { (eval echo configure:733: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
+if { (eval echo configure:722: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
   rm -rf conftest*
   may_use_xopen_source=yes
 else
@@ -774,7 +763,7 @@ fi
 test "$AR" || AR=ar
 
 echo $ac_n "checking whether ${MAKE-make} sets \${MAKE}""... $ac_c" 1>&6
-echo "configure:778: checking whether ${MAKE-make} sets \${MAKE}" >&5
+echo "configure:767: checking whether ${MAKE-make} sets \${MAKE}" >&5
 set dummy ${MAKE-make}; ac_make=`echo "$2" | sed 'y%./+-%__p_%'`
 if eval "test \"`echo '$''{'ac_cv_prog_make_${ac_make}_set'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
@@ -804,7 +793,7 @@ fi
 # Extract the first word of "chmod", so it can be a program name with args.
 set dummy chmod; ac_word=$2
 echo $ac_n "checking for $ac_word""... $ac_c" 1>&6
-echo "configure:808: checking for $ac_word" >&5
+echo "configure:797: checking for $ac_word" >&5
 if eval "test \"`echo '$''{'ac_cv_path_ac_cv_prog_chmod'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
@@ -848,7 +837,7 @@ else
 fi
 
 echo $ac_n "checking how to run the C preprocessor""... $ac_c" 1>&6
-echo "configure:852: checking how to run the C preprocessor" >&5
+echo "configure:841: checking how to run the C preprocessor" >&5
 # On Suns, sometimes $CPP names a directory.
 if test -n "$CPP" && test -d "$CPP"; then
   CPP=
@@ -863,13 +852,13 @@ else
   # On the NeXT, cc -E runs the code through the compiler's parser,
   # not just through cpp.
   cat > conftest.$ac_ext <<EOF
-#line 867 "configure"
+#line 856 "configure"
 #include "confdefs.h"
 #include <assert.h>
 Syntax Error
 EOF
 ac_try="$ac_cpp conftest.$ac_ext >/dev/null 2>conftest.out"
-{ (eval echo configure:873: \"$ac_try\") 1>&5; (eval $ac_try) 2>&5; }
+{ (eval echo configure:862: \"$ac_try\") 1>&5; (eval $ac_try) 2>&5; }
 ac_err=`grep -v '^ *+' conftest.out | grep -v "^conftest.${ac_ext}\$"`
 if test -z "$ac_err"; then
   :
@@ -880,13 +869,13 @@ else
   rm -rf conftest*
   CPP="${CC-cc} -E -traditional-cpp"
   cat > conftest.$ac_ext <<EOF
-#line 884 "configure"
+#line 873 "configure"
 #include "confdefs.h"
 #include <assert.h>
 Syntax Error
 EOF
 ac_try="$ac_cpp conftest.$ac_ext >/dev/null 2>conftest.out"
-{ (eval echo configure:890: \"$ac_try\") 1>&5; (eval $ac_try) 2>&5; }
+{ (eval echo configure:879: \"$ac_try\") 1>&5; (eval $ac_try) 2>&5; }
 ac_err=`grep -v '^ *+' conftest.out | grep -v "^conftest.${ac_ext}\$"`
 if test -z "$ac_err"; then
   :
@@ -897,13 +886,13 @@ else
   rm -rf conftest*
   CPP="${CC-cc} -nologo -E"
   cat > conftest.$ac_ext <<EOF
-#line 901 "configure"
+#line 890 "configure"
 #include "confdefs.h"
 #include <assert.h>
 Syntax Error
 EOF
 ac_try="$ac_cpp conftest.$ac_ext >/dev/null 2>conftest.out"
-{ (eval echo configure:907: \"$ac_try\") 1>&5; (eval $ac_try) 2>&5; }
+{ (eval echo configure:896: \"$ac_try\") 1>&5; (eval $ac_try) 2>&5; }
 ac_err=`grep -v '^ *+' conftest.out | grep -v "^conftest.${ac_ext}\$"`
 if test -z "$ac_err"; then
   :
@@ -928,12 +917,12 @@ fi
 echo "$ac_t""$CPP" 1>&6
 
 echo $ac_n "checking for ANSI C header files""... $ac_c" 1>&6
-echo "configure:932: checking for ANSI C header files" >&5
+echo "configure:921: checking for ANSI C header files" >&5
 if eval "test \"`echo '$''{'ac_cv_header_stdc'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 937 "configure"
+#line 926 "configure"
 #include "confdefs.h"
 #include <stdlib.h>
 #include <stdarg.h>
@@ -941,7 +930,7 @@ else
 #include <float.h>
 EOF
 ac_try="$ac_cpp conftest.$ac_ext >/dev/null 2>conftest.out"
-{ (eval echo configure:945: \"$ac_try\") 1>&5; (eval $ac_try) 2>&5; }
+{ (eval echo configure:934: \"$ac_try\") 1>&5; (eval $ac_try) 2>&5; }
 ac_err=`grep -v '^ *+' conftest.out | grep -v "^conftest.${ac_ext}\$"`
 if test -z "$ac_err"; then
   rm -rf conftest*
@@ -958,7 +947,7 @@ rm -f conftest*
 if test $ac_cv_header_stdc = yes; then
   # SunOS 4.x string.h does not declare mem*, contrary to ANSI.
 cat > conftest.$ac_ext <<EOF
-#line 962 "configure"
+#line 951 "configure"
 #include "confdefs.h"
 #include <string.h>
 EOF
@@ -976,7 +965,7 @@ fi
 if test $ac_cv_header_stdc = yes; then
   # ISC 2.0.2 stdlib.h does not declare free, contrary to ANSI.
 cat > conftest.$ac_ext <<EOF
-#line 980 "configure"
+#line 969 "configure"
 #include "confdefs.h"
 #include <stdlib.h>
 EOF
@@ -997,7 +986,7 @@ if test "$cross_compiling" = yes; then
   :
 else
   cat > conftest.$ac_ext <<EOF
-#line 1001 "configure"
+#line 990 "configure"
 #include "confdefs.h"
 #include <ctype.h>
 #define ISLOWER(c) ('a' <= (c) && (c) <= 'z')
@@ -1008,7 +997,7 @@ if (XOR (islower (i), ISLOWER (i)) || to
 exit (0); }
 
 EOF
-if { (eval echo configure:1012: \"$ac_link\") 1>&5; (eval $ac_link) 2>&5; } && test -s conftest${ac_exeext} && (./conftest; exit) 2>/dev/null
+if { (eval echo configure:1001: \"$ac_link\") 1>&5; (eval $ac_link) 2>&5; } && test -s conftest${ac_exeext} && (./conftest; exit) 2>/dev/null
 then
   :
 else
@@ -1032,12 +1021,12 @@ EOF
 fi
 
 echo $ac_n "checking whether time.h and sys/time.h may both be included""... $ac_c" 1>&6
-echo "configure:1036: checking whether time.h and sys/time.h may both be included" >&5
+echo "configure:1025: checking whether time.h and sys/time.h may both be included" >&5
 if eval "test \"`echo '$''{'ac_cv_header_time'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1041 "configure"
+#line 1030 "configure"
 #include "confdefs.h"
 #include <sys/types.h>
 #include <sys/time.h>
@@ -1046,7 +1035,7 @@ int main() {
 struct tm *tp;
 ; return 0; }
 EOF
-if { (eval echo configure:1050: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
+if { (eval echo configure:1039: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
   rm -rf conftest*
   ac_cv_header_time=yes
 else
@@ -1071,17 +1060,17 @@ for ac_hdr in limits.h unistd.h sys/time
 do
 ac_safe=`echo "$ac_hdr" | sed 'y%./+-%__p_%'`
 echo $ac_n "checking for $ac_hdr""... $ac_c" 1>&6
-echo "configure:1075: checking for $ac_hdr" >&5
+echo "configure:1064: checking for $ac_hdr" >&5
 if eval "test \"`echo '$''{'ac_cv_header_$ac_safe'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1080 "configure"
+#line 1069 "configure"
 #include "confdefs.h"
 #include <$ac_hdr>
 EOF
 ac_try="$ac_cpp conftest.$ac_ext >/dev/null 2>conftest.out"
-{ (eval echo configure:1085: \"$ac_try\") 1>&5; (eval $ac_try) 2>&5; }
+{ (eval echo configure:1074: \"$ac_try\") 1>&5; (eval $ac_try) 2>&5; }
 ac_err=`grep -v '^ *+' conftest.out | grep -v "^conftest.${ac_ext}\$"`
 if test -z "$ac_err"; then
   rm -rf conftest*
@@ -1109,12 +1098,12 @@ done
 
 
 echo $ac_n "checking for working const""... $ac_c" 1>&6
-echo "configure:1113: checking for working const" >&5
+echo "configure:1102: checking for working const" >&5
 if eval "test \"`echo '$''{'ac_cv_c_const'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1118 "configure"
+#line 1107 "configure"
 #include "confdefs.h"
 
 int main() {
@@ -1163,7 +1152,7 @@ ccp = (char const *const *) p;
 
 ; return 0; }
 EOF
-if { (eval echo configure:1167: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
+if { (eval echo configure:1156: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
   rm -rf conftest*
   ac_cv_c_const=yes
 else
@@ -1184,12 +1173,12 @@ EOF
 fi
 
 echo $ac_n "checking for size_t""... $ac_c" 1>&6
-echo "configure:1188: checking for size_t" >&5
+echo "configure:1177: checking for size_t" >&5
 if eval "test \"`echo '$''{'ac_cv_type_size_t'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1193 "configure"
+#line 1182 "configure"
 #include "confdefs.h"
 #include <sys/types.h>
 #if STDC_HEADERS
@@ -1217,12 +1206,12 @@ EOF
 fi
 
 echo $ac_n "checking for mode_t""... $ac_c" 1>&6
-echo "configure:1221: checking for mode_t" >&5
+echo "configure:1210: checking for mode_t" >&5
 if eval "test \"`echo '$''{'ac_cv_type_mode_t'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1226 "configure"
+#line 1215 "configure"
 #include "confdefs.h"
 #include <sys/types.h>
 #if STDC_HEADERS
@@ -1251,12 +1240,12 @@ fi
 
 
 echo $ac_n "checking for pid_t""... $ac_c" 1>&6
-echo "configure:1255: checking for pid_t" >&5
+echo "configure:1244: checking for pid_t" >&5
 if eval "test \"`echo '$''{'ac_cv_type_pid_t'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1260 "configure"
+#line 1249 "configure"
 #include "confdefs.h"
 #include <sys/types.h>
 #if STDC_HEADERS
@@ -1284,12 +1273,12 @@ EOF
 fi
 
 echo $ac_n "checking for st_blksize in struct stat""... $ac_c" 1>&6
-echo "configure:1288: checking for st_blksize in struct stat" >&5
+echo "configure:1277: checking for st_blksize in struct stat" >&5
 if eval "test \"`echo '$''{'ac_cv_struct_st_blksize'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1293 "configure"
+#line 1282 "configure"
 #include "confdefs.h"
 #include <sys/types.h>
 #include <sys/stat.h>
@@ -1297,7 +1286,7 @@ int main() {
 struct stat s; s.st_blksize;
 ; return 0; }
 EOF
-if { (eval echo configure:1301: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
+if { (eval echo configure:1290: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
   rm -rf conftest*
   ac_cv_struct_st_blksize=yes
 else
@@ -1318,12 +1307,12 @@ EOF
 fi
 
 echo $ac_n "checking for st_blocks in struct stat""... $ac_c" 1>&6
-echo "configure:1322: checking for st_blocks in struct stat" >&5
+echo "configure:1311: checking for st_blocks in struct stat" >&5
 if eval "test \"`echo '$''{'ac_cv_struct_st_blocks'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1327 "configure"
+#line 1316 "configure"
 #include "confdefs.h"
 #include <sys/types.h>
 #include <sys/stat.h>
@@ -1331,7 +1320,7 @@ int main() {
 struct stat s; s.st_blocks;
 ; return 0; }
 EOF
-if { (eval echo configure:1335: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
+if { (eval echo configure:1324: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
   rm -rf conftest*
   ac_cv_struct_st_blocks=yes
 else
@@ -1354,12 +1343,12 @@ else
 fi
 
 echo $ac_n "checking for st_rdev in struct stat""... $ac_c" 1>&6
-echo "configure:1358: checking for st_rdev in struct stat" >&5
+echo "configure:1347: checking for st_rdev in struct stat" >&5
 if eval "test \"`echo '$''{'ac_cv_struct_st_rdev'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1363 "configure"
+#line 1352 "configure"
 #include "confdefs.h"
 #include <sys/types.h>
 #include <sys/stat.h>
@@ -1367,7 +1356,7 @@ int main() {
 struct stat s; s.st_rdev;
 ; return 0; }
 EOF
-if { (eval echo configure:1371: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
+if { (eval echo configure:1360: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
   rm -rf conftest*
   ac_cv_struct_st_rdev=yes
 else
@@ -1388,12 +1377,12 @@ EOF
 fi
 
 echo $ac_n "checking whether struct tm is in sys/time.h or time.h""... $ac_c" 1>&6
-echo "configure:1392: checking whether struct tm is in sys/time.h or time.h" >&5
+echo "configure:1381: checking whether struct tm is in sys/time.h or time.h" >&5
 if eval "test \"`echo '$''{'ac_cv_struct_tm'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1397 "configure"
+#line 1386 "configure"
 #include "confdefs.h"
 #include <sys/types.h>
 #include <time.h>
@@ -1401,7 +1390,7 @@ int main() {
 struct tm *tp; tp->tm_sec;
 ; return 0; }
 EOF
-if { (eval echo configure:1405: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
+if { (eval echo configure:1394: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
   rm -rf conftest*
   ac_cv_struct_tm=time.h
 else
@@ -1423,7 +1412,7 @@ fi
 
 
 echo $ac_n "checking for gethostname in -lsocket""... $ac_c" 1>&6
-echo "configure:1427: checking for gethostname in -lsocket" >&5
+echo "configure:1416: checking for gethostname in -lsocket" >&5
 ac_lib_var=`echo socket'_'gethostname | sed 'y%./+-%__p_%'`
 if eval "test \"`echo '$''{'ac_cv_lib_$ac_lib_var'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
@@ -1431,7 +1420,7 @@ else
   ac_save_LIBS="$LIBS"
 LIBS="-lsocket  $LIBS"
 cat > conftest.$ac_ext <<EOF
-#line 1435 "configure"
+#line 1424 "configure"
 #include "confdefs.h"
 /* Override any gcc2 internal prototype to avoid an error.  */
 /* We use char because int might match the return type of a gcc2
@@ -1442,7 +1431,7 @@ int main() {
 gethostname()
 ; return 0; }
 EOF
-if { (eval echo configure:1446: \"$ac_link\") 1>&5; (eval $ac_link) 2>&5; } && test -s conftest${ac_exeext}; then
+if { (eval echo configure:1435: \"$ac_link\") 1>&5; (eval $ac_link) 2>&5; } && test -s conftest${ac_exeext}; then
   rm -rf conftest*
   eval "ac_cv_lib_$ac_lib_var=yes"
 else
@@ -1468,12 +1457,12 @@ for ac_func in symlink getcwd getwd lsta
   getrusage times alarm getlogin getgid getuid kill link ttyname
 do
 echo $ac_n "checking for $ac_func""... $ac_c" 1>&6
-echo "configure:1472: checking for $ac_func" >&5
+echo "configure:1461: checking for $ac_func" >&5
 if eval "test \"`echo '$''{'ac_cv_func_$ac_func'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1477 "configure"
+#line 1466 "configure"
 #include "confdefs.h"
 /* System header to define __stub macros and hopefully few prototypes,
     which can conflict with char $ac_func(); below.  */
@@ -1496,7 +1485,7 @@ $ac_func();
 
 ; return 0; }
 EOF
-if { (eval echo configure:1500: \"$ac_link\") 1>&5; (eval $ac_link) 2>&5; } && test -s conftest${ac_exeext}; then
+if { (eval echo configure:1489: \"$ac_link\") 1>&5; (eval $ac_link) 2>&5; } && test -s conftest${ac_exeext}; then
   rm -rf conftest*
   eval "ac_cv_func_$ac_func=yes"
 else
@@ -1525,12 +1514,12 @@ done
   for ac_func in gettimeofday
 do
 echo $ac_n "checking for $ac_func""... $ac_c" 1>&6
-echo "configure:1529: checking for $ac_func" >&5
+echo "configure:1518: checking for $ac_func" >&5
 if eval "test \"`echo '$''{'ac_cv_func_$ac_func'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1534 "configure"
+#line 1523 "configure"
 #include "confdefs.h"
 /* System header to define __stub macros and hopefully few prototypes,
     which can conflict with char $ac_func(); below.  */
@@ -1553,7 +1542,7 @@ $ac_func();
 
 ; return 0; }
 EOF
-if { (eval echo configure:1557: \"$ac_link\") 1>&5; (eval $ac_link) 2>&5; } && test -s conftest${ac_exeext}; then
+if { (eval echo configure:1546: \"$ac_link\") 1>&5; (eval $ac_link) 2>&5; } && test -s conftest${ac_exeext}; then
   rm -rf conftest*
   eval "ac_cv_func_$ac_func=yes"
 else
@@ -1579,19 +1568,19 @@ done
 
   if test "$ac_cv_func_gettimeofday" = yes; then
     echo $ac_n "checking for struct timezone""... $ac_c" 1>&6
-echo "configure:1583: checking for struct timezone" >&5
+echo "configure:1572: checking for struct timezone" >&5
 if eval "test \"`echo '$''{'g77_cv_struct_timezone'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1588 "configure"
+#line 1577 "configure"
 #include "confdefs.h"
 #include <sys/time.h>
 int main() {
 struct timezone tz;
 ; return 0; }
 EOF
-if { (eval echo configure:1595: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
+if { (eval echo configure:1584: \"$ac_compile\") 1>&5; (eval $ac_compile) 2>&5; }; then
   rm -rf conftest*
   g77_cv_struct_timezone=yes
 else
@@ -1612,7 +1601,7 @@ EOF
 
 else
   cat > conftest.$ac_ext <<EOF
-#line 1616 "configure"
+#line 1605 "configure"
 #include "confdefs.h"
 
 #ifdef TIME_WITH_SYS_TIME
@@ -1635,7 +1624,7 @@ main ()
     exit (0);
 }
 EOF
-if { (eval echo configure:1639: \"$ac_link\") 1>&5; (eval $ac_link) 2>&5; } && test -s conftest${ac_exeext} && (./conftest; exit) 2>/dev/null
+if { (eval echo configure:1628: \"$ac_link\") 1>&5; (eval $ac_link) 2>&5; } && test -s conftest${ac_exeext} && (./conftest; exit) 2>/dev/null
 then
   cat >> confdefs.h <<\EOF
 #define HAVE_TIMEZONE 1
@@ -1651,12 +1640,12 @@ fi
     fi
     
     echo $ac_n "checking whether gettimeofday can accept two arguments""... $ac_c" 1>&6
-echo "configure:1655: checking whether gettimeofday can accept two arguments" >&5
+echo "configure:1644: checking whether gettimeofday can accept two arguments" >&5
 if eval "test \"`echo '$''{'emacs_cv_gettimeofday_two_arguments'+set}'`\" = set"; then
   echo $ac_n "(cached) $ac_c" 1>&6
 else
   cat > conftest.$ac_ext <<EOF
-#line 1660 "configure"
+#line 1649 "configure"
 #include "confdefs.h"
 
 #ifdef TIME_WITH_SYS_TIME
@@ -1682,7 +1671,7 @@ int main() {
       gettimeofday (&time, DUMMY);
 ; return 0; }
 EOF
-if { (eval echo configure:1686: \"$ac_link\") 1>&5; (eval $ac_link) 2>&5; } && test -s conftest${ac_exeext}; then
+if { (eval echo configure:1675: \"$ac_link\") 1>&5; (eval $ac_link) 2>&5; } && test -s conftest${ac_exeext}; then
   rm -rf conftest*
   emacs_cv_gettimeofday_two_arguments=yes
 else
--- gcc-3.3.1/libjava/org/xml/sax/helpers/NewInstance.java.hammer-branch	2002-12-20 04:50:17.000000000 +0100
+++ gcc-3.3.1/libjava/org/xml/sax/helpers/NewInstance.java	2003-08-05 18:22:47.000000000 +0200
@@ -4,7 +4,7 @@
 // and by David Brownell, dbrownell@users.sourceforge.net
 // NO WARRANTY!  This class is in the Public Domain.
 
-// $Id: NewInstance.java,v 1.1.2.1 2002/12/20 03:50:17 tromey Exp $
+// $Id: NewInstance.java,v 1.1.8.1 2002/12/31 20:09:52 hubicka Exp $
 
 package org.xml.sax.helpers;
 
--- gcc-3.3.1/libjava/org/xml/sax/helpers/package.html.hammer-branch	2002-12-20 04:50:17.000000000 +0100
+++ gcc-3.3.1/libjava/org/xml/sax/helpers/package.html	2003-08-05 18:22:47.000000000 +0200
@@ -1,6 +1,6 @@
 <HTML><HEAD>
 
-<!-- $Id: package.html,v 1.1.2.1 2002/12/20 03:50:17 tromey Exp $ -->
+<!-- $Id: package.html,v 1.1.8.1 2002/12/31 20:09:52 hubicka Exp $ -->
 
 </HEAD><BODY>
 
--- gcc-3.3.1/libjava/org/xml/sax/package.html.hammer-branch	2002-12-20 04:50:17.000000000 +0100
+++ gcc-3.3.1/libjava/org/xml/sax/package.html	2003-08-05 18:22:47.000000000 +0200
@@ -1,6 +1,6 @@
 <html><head>
 
-<!-- $Id: package.html,v 1.1.2.1 2002/12/20 03:50:17 tromey Exp $ -->
+<!-- $Id: package.html,v 1.1.8.1 2002/12/31 20:09:52 hubicka Exp $ -->
 
 </head><body>
 
--- gcc-3.3.1/Makefile.in.hammer-branch	2003-07-05 04:37:08.000000000 +0200
+++ gcc-3.3.1/Makefile.in	2003-08-05 18:22:47.000000000 +0200
@@ -1566,6 +1566,23 @@ bootstrap bootstrap-lean bootstrap2 boot
 	echo "Building runtime libraries"; \
 	$(MAKE) $(BASE_FLAGS_TO_PASS) $(RECURSE_FLAGS) all
 
+profiledbootstrap: all-bootstrap
+	@r=`${PWD}`; export r; \
+	s=`cd $(srcdir); ${PWD}`; export s; \
+	$(SET_LIB_PATH) \
+	echo "Bootstrapping the compiler"; \
+	cd gcc && $(MAKE) $(GCC_FLAGS_TO_PASS) stageprofile_build
+	@r=`${PWD}`; export r; \
+	s=`cd $(srcdir); ${PWD}` ; export s; \
+	$(SET_LIB_PATH) \
+	echo "Building runtime libraries and training compiler"; \
+	$(MAKE) $(BASE_FLAGS_TO_PASS) $(RECURSE_FLAGS) all
+	@r=`${PWD}`; export r; \
+	s=`cd $(srcdir); ${PWD}`; export s; \
+	$(SET_LIB_PATH) \
+	echo "Building feedback based compiler"; \
+	cd gcc && $(MAKE) $(GCC_FLAGS_TO_PASS) stagefeedback_build
+
 .PHONY: cross
 cross: all-texinfo all-bison all-byacc all-binutils all-gas all-ld
 	@r=`${PWD_COMMAND}`; export r; \
